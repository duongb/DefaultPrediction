{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bQ6RIwSL2fI",
        "outputId": "37c54b82-9fb2-40f5-b841-4831e7cfb2d9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rH4hCTwiWphF"
      },
      "outputs": [],
      "source": [
        "!pip install GPUtil -q\n",
        "!pip install pytorch-tabnet -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Activation, Conv1D, MaxPooling1D, Flatten, Dropout, Embedding, LSTM\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "from pytorch_tabnet.metrics import Metric\n",
        "\n",
        "import gc\n",
        "import os\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "import torch\n",
        "from GPUtil import showUtilization as gpu_usage\n",
        "from numba import cuda"
      ],
      "metadata": {
        "id": "54qXxTn_W1tc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def free_gpu_cache():\n",
        "    print(\"Initial GPU Usage\")\n",
        "    gpu_usage() \n",
        "    torch.cuda.empty_cache()\n",
        "    cuda.select_device(0)\n",
        "    cuda.close()\n",
        "    cuda.select_device(0)\n",
        "\n",
        "    print(\"GPU Usage after emptying the cache\")\n",
        "    gpu_usage()"
      ],
      "metadata": {
        "id": "3mdclUhsW_DG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pd.read_pickle('./drive/MyDrive/save/taiwan/X_train.pkl')\n",
        "X_test = pd.read_pickle('./drive/MyDrive/save/taiwan/X_test.pkl')\n",
        "y_train = pd.read_pickle('./drive/MyDrive/save/taiwan/y_train.pkl')\n",
        "y_test = pd.read_pickle('./drive/MyDrive/save/taiwan/y_test.pkl')"
      ],
      "metadata": {
        "id": "1NUS7aB1YLBC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_fold = 5"
      ],
      "metadata": {
        "id": "s-dxsYxqXTKY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###########################################################################"
      ],
      "metadata": {
        "id": "j5KgNRf2XMA9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EarlyStop(patience):\n",
        "    return EarlyStopping(\n",
        "    monitor=\"val_accuracy\",\n",
        "    min_delta=0,\n",
        "    patience=patience,\n",
        "    verbose=0,\n",
        "    mode=\"auto\",\n",
        ")\n",
        "\n",
        "def ModelCheckpointFull(model_name):\n",
        "    return ModelCheckpoint(\n",
        "                     filepath=model_name, \n",
        "                     save_freq='epoch', verbose=1, monitor='val_accuracy', \n",
        "                     save_weights_only=True, save_best_only=True\n",
        "                 )   "
      ],
      "metadata": {
        "id": "Hf0QHyZ8XLL7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########### 1D-CNN model #############"
      ],
      "metadata": {
        "id": "_gta-DrwXJb7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define 1D - CNN model\n",
        "def create_model(input_shape):\n",
        "    # Model Building\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=512, kernel_size=3, activation='relu', input_shape=(input_shape,1)))\n",
        "    model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
        "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train_cnn = np.array(X_train).reshape(-1, X_train.shape[1], 1)\n",
        "X_test_cnn = np.array(X_test).reshape(-1, X_test.shape[1], 1)\n",
        "\n",
        "# 1D-CNN model\n",
        "cnn1d = create_model(input_shape = X_train_cnn.shape[1])   "
      ],
      "metadata": {
        "id": "ERHokefhXJxw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=n_fold, shuffle = False)\n",
        "acc_score_cnn1d = []\n",
        "auc_score_cnn1d = []\n",
        "f1_cnn1d = []\n",
        "meta_train_cnn1d = []\n",
        "meta_test_cnn1d = []\n",
        "i = 1\n",
        "for train_index, valid_index in kf.split(X_train_cnn, y_train):\n",
        "    print('KFold {} of {}'.format(i,kf.n_splits))\n",
        "    train_X, val_X = X_train_cnn[train_index], X_train_cnn[valid_index]\n",
        "    train_y, val_y = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
        "    cnn1d.fit(train_X, train_y, validation_data=(val_X, val_y), epochs = 500, \n",
        "            batch_size = 256, \n",
        "            callbacks = [EarlyStop(30), ModelCheckpointFull('./cnn1d.h5')],\n",
        "            verbose = 1)\n",
        "    ####meta\n",
        "    meta_train_cnn1d = np.append(meta_train_cnn1d, cnn1d.predict(val_X))\n",
        "    \n",
        "    if len(meta_test_cnn1d) == 0:\n",
        "        meta_test_cnn1d = cnn1d.predict(X_test_cnn)\n",
        "    else:\n",
        "        meta_test_cnn1d = np.add(meta_test_cnn1d, cnn1d.predict(X_test_cnn))\n",
        "    #####\n",
        "    yhat = cnn1d.predict(X_test_cnn).round()\n",
        "    acc_score_cnn1d.append(accuracy_score(yhat,y_test))\n",
        "    auc_score_cnn1d.append(roc_auc_score(yhat,y_test))\n",
        "    f1_cnn1d.append(f1_score(yhat,y_test))\n",
        "    i += 1\n",
        "meta_test_cnn1d = np.divide(meta_test_cnn1d, n_fold)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFJNlCaoXPES",
        "outputId": "aaaf25de-89e9-4f4f-dc9f-bbea0b53af91"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KFold 1 of 5\n",
            "Epoch 1/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.5086 - accuracy: 0.7214\n",
            "Epoch 1: val_accuracy improved from -inf to 0.75740, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 5s 31ms/step - loss: 0.5086 - accuracy: 0.7214 - val_loss: 0.4672 - val_accuracy: 0.7574\n",
            "Epoch 2/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.4766 - accuracy: 0.7436\n",
            "Epoch 2: val_accuracy improved from 0.75740 to 0.76357, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 2s 25ms/step - loss: 0.4764 - accuracy: 0.7437 - val_loss: 0.4492 - val_accuracy: 0.7636\n",
            "Epoch 3/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.4669 - accuracy: 0.7522\n",
            "Epoch 3: val_accuracy did not improve from 0.76357\n",
            "64/64 [==============================] - 2s 26ms/step - loss: 0.4669 - accuracy: 0.7521 - val_loss: 0.4484 - val_accuracy: 0.7614\n",
            "Epoch 4/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.4565 - accuracy: 0.7612\n",
            "Epoch 4: val_accuracy did not improve from 0.76357\n",
            "64/64 [==============================] - 2s 27ms/step - loss: 0.4563 - accuracy: 0.7615 - val_loss: 0.4462 - val_accuracy: 0.7628\n",
            "Epoch 5/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.4442 - accuracy: 0.7727\n",
            "Epoch 5: val_accuracy improved from 0.76357 to 0.78973, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 2s 27ms/step - loss: 0.4440 - accuracy: 0.7724 - val_loss: 0.4221 - val_accuracy: 0.7897\n",
            "Epoch 6/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.4384 - accuracy: 0.7720\n",
            "Epoch 6: val_accuracy did not improve from 0.78973\n",
            "64/64 [==============================] - 2s 24ms/step - loss: 0.4376 - accuracy: 0.7721 - val_loss: 0.4213 - val_accuracy: 0.7841\n",
            "Epoch 7/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.4231 - accuracy: 0.7817\n",
            "Epoch 7: val_accuracy did not improve from 0.78973\n",
            "64/64 [==============================] - 2s 25ms/step - loss: 0.4215 - accuracy: 0.7829 - val_loss: 0.4147 - val_accuracy: 0.7841\n",
            "Epoch 8/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.4148 - accuracy: 0.7897\n",
            "Epoch 8: val_accuracy improved from 0.78973 to 0.79664, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 2s 27ms/step - loss: 0.4148 - accuracy: 0.7897 - val_loss: 0.3968 - val_accuracy: 0.7966\n",
            "Epoch 9/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.4033 - accuracy: 0.7974\n",
            "Epoch 9: val_accuracy improved from 0.79664 to 0.79738, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 2s 28ms/step - loss: 0.4033 - accuracy: 0.7971 - val_loss: 0.3952 - val_accuracy: 0.7974\n",
            "Epoch 10/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.3907 - accuracy: 0.8049\n",
            "Epoch 10: val_accuracy improved from 0.79738 to 0.80652, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 2s 30ms/step - loss: 0.3909 - accuracy: 0.8050 - val_loss: 0.3805 - val_accuracy: 0.8065\n",
            "Epoch 11/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.3783 - accuracy: 0.8148\n",
            "Epoch 11: val_accuracy improved from 0.80652 to 0.80997, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 2s 29ms/step - loss: 0.3776 - accuracy: 0.8155 - val_loss: 0.3833 - val_accuracy: 0.8100\n",
            "Epoch 12/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.3691 - accuracy: 0.8175\n",
            "Epoch 12: val_accuracy improved from 0.80997 to 0.81392, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 2s 27ms/step - loss: 0.3693 - accuracy: 0.8176 - val_loss: 0.3763 - val_accuracy: 0.8139\n",
            "Epoch 13/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.3627 - accuracy: 0.8259\n",
            "Epoch 13: val_accuracy improved from 0.81392 to 0.83070, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 2s 28ms/step - loss: 0.3627 - accuracy: 0.8259 - val_loss: 0.3570 - val_accuracy: 0.8307\n",
            "Epoch 14/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.3534 - accuracy: 0.8305\n",
            "Epoch 14: val_accuracy did not improve from 0.83070\n",
            "64/64 [==============================] - 2s 29ms/step - loss: 0.3531 - accuracy: 0.8307 - val_loss: 0.3682 - val_accuracy: 0.8129\n",
            "Epoch 15/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.3426 - accuracy: 0.8364\n",
            "Epoch 15: val_accuracy improved from 0.83070 to 0.83514, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 21ms/step - loss: 0.3426 - accuracy: 0.8356 - val_loss: 0.3444 - val_accuracy: 0.8351\n",
            "Epoch 16/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.3483 - accuracy: 0.8355\n",
            "Epoch 16: val_accuracy did not improve from 0.83514\n",
            "64/64 [==============================] - 1s 21ms/step - loss: 0.3508 - accuracy: 0.8329 - val_loss: 0.3791 - val_accuracy: 0.8176\n",
            "Epoch 17/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.3612 - accuracy: 0.8274\n",
            "Epoch 17: val_accuracy did not improve from 0.83514\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.3612 - accuracy: 0.8274 - val_loss: 0.3495 - val_accuracy: 0.8260\n",
            "Epoch 18/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.3359 - accuracy: 0.8403\n",
            "Epoch 18: val_accuracy improved from 0.83514 to 0.84156, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.3349 - accuracy: 0.8409 - val_loss: 0.3348 - val_accuracy: 0.8416\n",
            "Epoch 19/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.3233 - accuracy: 0.8468\n",
            "Epoch 19: val_accuracy improved from 0.84156 to 0.84773, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.3236 - accuracy: 0.8469 - val_loss: 0.3295 - val_accuracy: 0.8477\n",
            "Epoch 20/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.3139 - accuracy: 0.8526\n",
            "Epoch 20: val_accuracy did not improve from 0.84773\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.3123 - accuracy: 0.8537 - val_loss: 0.3305 - val_accuracy: 0.8398\n",
            "Epoch 21/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.3041 - accuracy: 0.8616\n",
            "Epoch 21: val_accuracy improved from 0.84773 to 0.85464, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.3047 - accuracy: 0.8618 - val_loss: 0.3203 - val_accuracy: 0.8546\n",
            "Epoch 22/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2995 - accuracy: 0.8614\n",
            "Epoch 22: val_accuracy did not improve from 0.85464\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.2996 - accuracy: 0.8609 - val_loss: 0.3415 - val_accuracy: 0.8374\n",
            "Epoch 23/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2923 - accuracy: 0.8647\n",
            "Epoch 23: val_accuracy improved from 0.85464 to 0.85686, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.2926 - accuracy: 0.8649 - val_loss: 0.3174 - val_accuracy: 0.8569\n",
            "Epoch 24/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.2838 - accuracy: 0.8709\n",
            "Epoch 24: val_accuracy improved from 0.85686 to 0.85760, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.2838 - accuracy: 0.8709 - val_loss: 0.3186 - val_accuracy: 0.8576\n",
            "Epoch 25/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2843 - accuracy: 0.8695\n",
            "Epoch 25: val_accuracy did not improve from 0.85760\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.2841 - accuracy: 0.8696 - val_loss: 0.3151 - val_accuracy: 0.8514\n",
            "Epoch 26/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2681 - accuracy: 0.8808\n",
            "Epoch 26: val_accuracy improved from 0.85760 to 0.86895, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.2684 - accuracy: 0.8809 - val_loss: 0.3010 - val_accuracy: 0.8690\n",
            "Epoch 27/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.2675 - accuracy: 0.8789\n",
            "Epoch 27: val_accuracy did not improve from 0.86895\n",
            "64/64 [==============================] - 1s 21ms/step - loss: 0.2675 - accuracy: 0.8789 - val_loss: 0.2999 - val_accuracy: 0.8675\n",
            "Epoch 28/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2621 - accuracy: 0.8806\n",
            "Epoch 28: val_accuracy did not improve from 0.86895\n",
            "64/64 [==============================] - 1s 21ms/step - loss: 0.2616 - accuracy: 0.8812 - val_loss: 0.3145 - val_accuracy: 0.8566\n",
            "Epoch 29/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2554 - accuracy: 0.8877\n",
            "Epoch 29: val_accuracy did not improve from 0.86895\n",
            "64/64 [==============================] - 1s 21ms/step - loss: 0.2559 - accuracy: 0.8872 - val_loss: 0.3071 - val_accuracy: 0.8655\n",
            "Epoch 30/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.2529 - accuracy: 0.8889\n",
            "Epoch 30: val_accuracy did not improve from 0.86895\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.2529 - accuracy: 0.8889 - val_loss: 0.3039 - val_accuracy: 0.8623\n",
            "Epoch 31/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.2477 - accuracy: 0.8892\n",
            "Epoch 31: val_accuracy did not improve from 0.86895\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.2477 - accuracy: 0.8892 - val_loss: 0.3013 - val_accuracy: 0.8690\n",
            "Epoch 32/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.2402 - accuracy: 0.8960\n",
            "Epoch 32: val_accuracy improved from 0.86895 to 0.87192, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.2402 - accuracy: 0.8960 - val_loss: 0.2945 - val_accuracy: 0.8719\n",
            "Epoch 33/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2366 - accuracy: 0.8954\n",
            "Epoch 33: val_accuracy did not improve from 0.87192\n",
            "64/64 [==============================] - 1s 21ms/step - loss: 0.2353 - accuracy: 0.8961 - val_loss: 0.2978 - val_accuracy: 0.8707\n",
            "Epoch 34/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2323 - accuracy: 0.8975\n",
            "Epoch 34: val_accuracy did not improve from 0.87192\n",
            "64/64 [==============================] - 1s 21ms/step - loss: 0.2329 - accuracy: 0.8974 - val_loss: 0.3038 - val_accuracy: 0.8697\n",
            "Epoch 35/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2293 - accuracy: 0.8957\n",
            "Epoch 35: val_accuracy improved from 0.87192 to 0.87784, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 21ms/step - loss: 0.2286 - accuracy: 0.8968 - val_loss: 0.2940 - val_accuracy: 0.8778\n",
            "Epoch 36/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.2188 - accuracy: 0.9049\n",
            "Epoch 36: val_accuracy improved from 0.87784 to 0.87883, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 21ms/step - loss: 0.2188 - accuracy: 0.9049 - val_loss: 0.2845 - val_accuracy: 0.8788\n",
            "Epoch 37/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.2137 - accuracy: 0.9076\n",
            "Epoch 37: val_accuracy did not improve from 0.87883\n",
            "64/64 [==============================] - 1s 21ms/step - loss: 0.2137 - accuracy: 0.9076 - val_loss: 0.2916 - val_accuracy: 0.8736\n",
            "Epoch 38/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2142 - accuracy: 0.9043\n",
            "Epoch 38: val_accuracy improved from 0.87883 to 0.88129, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.2134 - accuracy: 0.9049 - val_loss: 0.2906 - val_accuracy: 0.8813\n",
            "Epoch 39/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2072 - accuracy: 0.9123\n",
            "Epoch 39: val_accuracy improved from 0.88129 to 0.88228, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.2073 - accuracy: 0.9121 - val_loss: 0.2832 - val_accuracy: 0.8823\n",
            "Epoch 40/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2068 - accuracy: 0.9100\n",
            "Epoch 40: val_accuracy improved from 0.88228 to 0.88351, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.2062 - accuracy: 0.9105 - val_loss: 0.2826 - val_accuracy: 0.8835\n",
            "Epoch 41/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1992 - accuracy: 0.9132\n",
            "Epoch 41: val_accuracy did not improve from 0.88351\n",
            "64/64 [==============================] - 1s 21ms/step - loss: 0.1990 - accuracy: 0.9137 - val_loss: 0.2826 - val_accuracy: 0.8823\n",
            "Epoch 42/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9193\n",
            "Epoch 42: val_accuracy improved from 0.88351 to 0.88820, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1926 - accuracy: 0.9193 - val_loss: 0.2841 - val_accuracy: 0.8882\n",
            "Epoch 43/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1961 - accuracy: 0.9152\n",
            "Epoch 43: val_accuracy did not improve from 0.88820\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1976 - accuracy: 0.9146 - val_loss: 0.2990 - val_accuracy: 0.8852\n",
            "Epoch 44/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1920 - accuracy: 0.9179\n",
            "Epoch 44: val_accuracy improved from 0.88820 to 0.88968, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1920 - accuracy: 0.9180 - val_loss: 0.2738 - val_accuracy: 0.8897\n",
            "Epoch 45/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1844 - accuracy: 0.9215\n",
            "Epoch 45: val_accuracy did not improve from 0.88968\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1844 - accuracy: 0.9215 - val_loss: 0.2916 - val_accuracy: 0.8828\n",
            "Epoch 46/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1943 - accuracy: 0.9167\n",
            "Epoch 46: val_accuracy did not improve from 0.88968\n",
            "64/64 [==============================] - 1s 21ms/step - loss: 0.1935 - accuracy: 0.9171 - val_loss: 0.2859 - val_accuracy: 0.8882\n",
            "Epoch 47/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1804 - accuracy: 0.9244\n",
            "Epoch 47: val_accuracy improved from 0.88968 to 0.89437, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1804 - accuracy: 0.9244 - val_loss: 0.2705 - val_accuracy: 0.8944\n",
            "Epoch 48/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1753 - accuracy: 0.9276\n",
            "Epoch 48: val_accuracy did not improve from 0.89437\n",
            "64/64 [==============================] - 1s 21ms/step - loss: 0.1773 - accuracy: 0.9266 - val_loss: 0.2929 - val_accuracy: 0.8885\n",
            "Epoch 49/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1721 - accuracy: 0.9266\n",
            "Epoch 49: val_accuracy improved from 0.89437 to 0.89635, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1721 - accuracy: 0.9266 - val_loss: 0.2875 - val_accuracy: 0.8963\n",
            "Epoch 50/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1789 - accuracy: 0.9239\n",
            "Epoch 50: val_accuracy did not improve from 0.89635\n",
            "64/64 [==============================] - 1s 21ms/step - loss: 0.1792 - accuracy: 0.9234 - val_loss: 0.2819 - val_accuracy: 0.8919\n",
            "Epoch 51/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1716 - accuracy: 0.9265\n",
            "Epoch 51: val_accuracy improved from 0.89635 to 0.90301, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.1716 - accuracy: 0.9265 - val_loss: 0.2678 - val_accuracy: 0.9030\n",
            "Epoch 52/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1767 - accuracy: 0.9286\n",
            "Epoch 52: val_accuracy did not improve from 0.90301\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1767 - accuracy: 0.9286 - val_loss: 0.2835 - val_accuracy: 0.8919\n",
            "Epoch 53/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1684 - accuracy: 0.9287\n",
            "Epoch 53: val_accuracy did not improve from 0.90301\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1684 - accuracy: 0.9287 - val_loss: 0.2804 - val_accuracy: 0.8914\n",
            "Epoch 54/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1639 - accuracy: 0.9302\n",
            "Epoch 54: val_accuracy did not improve from 0.90301\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1639 - accuracy: 0.9302 - val_loss: 0.2795 - val_accuracy: 0.8959\n",
            "Epoch 55/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1617 - accuracy: 0.9328\n",
            "Epoch 55: val_accuracy did not improve from 0.90301\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.1619 - accuracy: 0.9328 - val_loss: 0.2818 - val_accuracy: 0.8951\n",
            "Epoch 56/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1548 - accuracy: 0.9372\n",
            "Epoch 56: val_accuracy did not improve from 0.90301\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1554 - accuracy: 0.9367 - val_loss: 0.2839 - val_accuracy: 0.8963\n",
            "Epoch 57/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1577 - accuracy: 0.9353\n",
            "Epoch 57: val_accuracy did not improve from 0.90301\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1577 - accuracy: 0.9353 - val_loss: 0.2758 - val_accuracy: 0.9003\n",
            "Epoch 58/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1516 - accuracy: 0.9367\n",
            "Epoch 58: val_accuracy did not improve from 0.90301\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1516 - accuracy: 0.9367 - val_loss: 0.2758 - val_accuracy: 0.9008\n",
            "Epoch 59/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1427 - accuracy: 0.9412\n",
            "Epoch 59: val_accuracy did not improve from 0.90301\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1444 - accuracy: 0.9406 - val_loss: 0.2869 - val_accuracy: 0.9013\n",
            "Epoch 60/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1470 - accuracy: 0.9371\n",
            "Epoch 60: val_accuracy did not improve from 0.90301\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1470 - accuracy: 0.9371 - val_loss: 0.2848 - val_accuracy: 0.9030\n",
            "Epoch 61/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1465 - accuracy: 0.9403\n",
            "Epoch 61: val_accuracy did not improve from 0.90301\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1478 - accuracy: 0.9401 - val_loss: 0.2758 - val_accuracy: 0.9010\n",
            "Epoch 62/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1478 - accuracy: 0.9375\n",
            "Epoch 62: val_accuracy improved from 0.90301 to 0.90745, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1478 - accuracy: 0.9375 - val_loss: 0.2774 - val_accuracy: 0.9075\n",
            "Epoch 63/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1414 - accuracy: 0.9426\n",
            "Epoch 63: val_accuracy did not improve from 0.90745\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1414 - accuracy: 0.9426 - val_loss: 0.2921 - val_accuracy: 0.8949\n",
            "Epoch 64/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1477 - accuracy: 0.9389\n",
            "Epoch 64: val_accuracy did not improve from 0.90745\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1477 - accuracy: 0.9389 - val_loss: 0.2964 - val_accuracy: 0.8944\n",
            "Epoch 65/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1487 - accuracy: 0.9366\n",
            "Epoch 65: val_accuracy did not improve from 0.90745\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1485 - accuracy: 0.9366 - val_loss: 0.2759 - val_accuracy: 0.9018\n",
            "Epoch 66/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1377 - accuracy: 0.9423\n",
            "Epoch 66: val_accuracy did not improve from 0.90745\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1377 - accuracy: 0.9423 - val_loss: 0.2772 - val_accuracy: 0.9055\n",
            "Epoch 67/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.1332 - accuracy: 0.9468\n",
            "Epoch 67: val_accuracy did not improve from 0.90745\n",
            "64/64 [==============================] - 2s 28ms/step - loss: 0.1336 - accuracy: 0.9464 - val_loss: 0.2821 - val_accuracy: 0.9020\n",
            "Epoch 68/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1359 - accuracy: 0.9440\n",
            "Epoch 68: val_accuracy improved from 0.90745 to 0.90844, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 2s 30ms/step - loss: 0.1358 - accuracy: 0.9441 - val_loss: 0.2820 - val_accuracy: 0.9084\n",
            "Epoch 69/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1284 - accuracy: 0.9483\n",
            "Epoch 69: val_accuracy did not improve from 0.90844\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1293 - accuracy: 0.9481 - val_loss: 0.2903 - val_accuracy: 0.9050\n",
            "Epoch 70/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1290 - accuracy: 0.9447\n",
            "Epoch 70: val_accuracy did not improve from 0.90844\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1289 - accuracy: 0.9445 - val_loss: 0.2953 - val_accuracy: 0.9033\n",
            "Epoch 71/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1288 - accuracy: 0.9469\n",
            "Epoch 71: val_accuracy did not improve from 0.90844\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1288 - accuracy: 0.9469 - val_loss: 0.2813 - val_accuracy: 0.9067\n",
            "Epoch 72/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1238 - accuracy: 0.9494\n",
            "Epoch 72: val_accuracy improved from 0.90844 to 0.90869, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1244 - accuracy: 0.9492 - val_loss: 0.2899 - val_accuracy: 0.9087\n",
            "Epoch 73/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1230 - accuracy: 0.9505\n",
            "Epoch 73: val_accuracy improved from 0.90869 to 0.91412, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1230 - accuracy: 0.9505 - val_loss: 0.2864 - val_accuracy: 0.9141\n",
            "Epoch 74/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1271 - accuracy: 0.9497\n",
            "Epoch 74: val_accuracy did not improve from 0.91412\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1267 - accuracy: 0.9496 - val_loss: 0.2944 - val_accuracy: 0.9072\n",
            "Epoch 75/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1186 - accuracy: 0.9539\n",
            "Epoch 75: val_accuracy improved from 0.91412 to 0.91856, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1202 - accuracy: 0.9530 - val_loss: 0.2822 - val_accuracy: 0.9186\n",
            "Epoch 76/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1241 - accuracy: 0.9476\n",
            "Epoch 76: val_accuracy did not improve from 0.91856\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1241 - accuracy: 0.9476 - val_loss: 0.2876 - val_accuracy: 0.9099\n",
            "Epoch 77/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1201 - accuracy: 0.9512\n",
            "Epoch 77: val_accuracy did not improve from 0.91856\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1201 - accuracy: 0.9512 - val_loss: 0.3565 - val_accuracy: 0.8926\n",
            "Epoch 78/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1215 - accuracy: 0.9503\n",
            "Epoch 78: val_accuracy did not improve from 0.91856\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1215 - accuracy: 0.9503 - val_loss: 0.3196 - val_accuracy: 0.9121\n",
            "Epoch 79/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1163 - accuracy: 0.9539\n",
            "Epoch 79: val_accuracy did not improve from 0.91856\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1163 - accuracy: 0.9539 - val_loss: 0.3067 - val_accuracy: 0.9060\n",
            "Epoch 80/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1168 - accuracy: 0.9518\n",
            "Epoch 80: val_accuracy improved from 0.91856 to 0.91955, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1167 - accuracy: 0.9519 - val_loss: 0.2867 - val_accuracy: 0.9195\n",
            "Epoch 81/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1087 - accuracy: 0.9548\n",
            "Epoch 81: val_accuracy did not improve from 0.91955\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1087 - accuracy: 0.9548 - val_loss: 0.3097 - val_accuracy: 0.9124\n",
            "Epoch 82/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1151 - accuracy: 0.9532\n",
            "Epoch 82: val_accuracy did not improve from 0.91955\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1159 - accuracy: 0.9525 - val_loss: 0.3011 - val_accuracy: 0.9139\n",
            "Epoch 83/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1142 - accuracy: 0.9546\n",
            "Epoch 83: val_accuracy did not improve from 0.91955\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1142 - accuracy: 0.9546 - val_loss: 0.2997 - val_accuracy: 0.9144\n",
            "Epoch 84/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1048 - accuracy: 0.9582\n",
            "Epoch 84: val_accuracy did not improve from 0.91955\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.1051 - accuracy: 0.9580 - val_loss: 0.2900 - val_accuracy: 0.9188\n",
            "Epoch 85/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1024 - accuracy: 0.9603\n",
            "Epoch 85: val_accuracy did not improve from 0.91955\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.1024 - accuracy: 0.9603 - val_loss: 0.2981 - val_accuracy: 0.9156\n",
            "Epoch 86/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1075 - accuracy: 0.9572\n",
            "Epoch 86: val_accuracy did not improve from 0.91955\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1083 - accuracy: 0.9570 - val_loss: 0.2921 - val_accuracy: 0.9154\n",
            "Epoch 87/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1078 - accuracy: 0.9578\n",
            "Epoch 87: val_accuracy did not improve from 0.91955\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1078 - accuracy: 0.9578 - val_loss: 0.3053 - val_accuracy: 0.9144\n",
            "Epoch 88/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1047 - accuracy: 0.9584\n",
            "Epoch 88: val_accuracy did not improve from 0.91955\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.1047 - accuracy: 0.9584 - val_loss: 0.3084 - val_accuracy: 0.9168\n",
            "Epoch 89/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1110 - accuracy: 0.9551\n",
            "Epoch 89: val_accuracy improved from 0.91955 to 0.92251, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1110 - accuracy: 0.9551 - val_loss: 0.2997 - val_accuracy: 0.9225\n",
            "Epoch 90/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1015 - accuracy: 0.9602\n",
            "Epoch 90: val_accuracy did not improve from 0.92251\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1015 - accuracy: 0.9602 - val_loss: 0.3094 - val_accuracy: 0.9161\n",
            "Epoch 91/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0968 - accuracy: 0.9608\n",
            "Epoch 91: val_accuracy did not improve from 0.92251\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0977 - accuracy: 0.9607 - val_loss: 0.3018 - val_accuracy: 0.9191\n",
            "Epoch 92/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0985 - accuracy: 0.9601\n",
            "Epoch 92: val_accuracy did not improve from 0.92251\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0985 - accuracy: 0.9601 - val_loss: 0.3033 - val_accuracy: 0.9191\n",
            "Epoch 93/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0966 - accuracy: 0.9627\n",
            "Epoch 93: val_accuracy did not improve from 0.92251\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0966 - accuracy: 0.9627 - val_loss: 0.2920 - val_accuracy: 0.9198\n",
            "Epoch 94/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1004 - accuracy: 0.9601\n",
            "Epoch 94: val_accuracy did not improve from 0.92251\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.1004 - accuracy: 0.9601 - val_loss: 0.3037 - val_accuracy: 0.9161\n",
            "Epoch 95/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1078 - accuracy: 0.9595\n",
            "Epoch 95: val_accuracy did not improve from 0.92251\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.1076 - accuracy: 0.9592 - val_loss: 0.2939 - val_accuracy: 0.9178\n",
            "Epoch 96/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0941 - accuracy: 0.9637\n",
            "Epoch 96: val_accuracy did not improve from 0.92251\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0937 - accuracy: 0.9640 - val_loss: 0.3068 - val_accuracy: 0.9208\n",
            "Epoch 97/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1025 - accuracy: 0.9608\n",
            "Epoch 97: val_accuracy did not improve from 0.92251\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.1023 - accuracy: 0.9609 - val_loss: 0.3097 - val_accuracy: 0.9176\n",
            "Epoch 98/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0975 - accuracy: 0.9597\n",
            "Epoch 98: val_accuracy did not improve from 0.92251\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0975 - accuracy: 0.9597 - val_loss: 0.2996 - val_accuracy: 0.9183\n",
            "Epoch 99/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0887 - accuracy: 0.9655\n",
            "Epoch 99: val_accuracy did not improve from 0.92251\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0892 - accuracy: 0.9650 - val_loss: 0.2985 - val_accuracy: 0.9225\n",
            "Epoch 100/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0908 - accuracy: 0.9629\n",
            "Epoch 100: val_accuracy improved from 0.92251 to 0.92522, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0906 - accuracy: 0.9630 - val_loss: 0.2971 - val_accuracy: 0.9252\n",
            "Epoch 101/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0848 - accuracy: 0.9669\n",
            "Epoch 101: val_accuracy did not improve from 0.92522\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0848 - accuracy: 0.9669 - val_loss: 0.3156 - val_accuracy: 0.9171\n",
            "Epoch 102/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0871 - accuracy: 0.9656\n",
            "Epoch 102: val_accuracy did not improve from 0.92522\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0871 - accuracy: 0.9656 - val_loss: 0.3388 - val_accuracy: 0.9124\n",
            "Epoch 103/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0907 - accuracy: 0.9639\n",
            "Epoch 103: val_accuracy did not improve from 0.92522\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0907 - accuracy: 0.9639 - val_loss: 0.3242 - val_accuracy: 0.9225\n",
            "Epoch 104/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0885 - accuracy: 0.9652\n",
            "Epoch 104: val_accuracy did not improve from 0.92522\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0885 - accuracy: 0.9652 - val_loss: 0.3373 - val_accuracy: 0.9188\n",
            "Epoch 105/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0896 - accuracy: 0.9646\n",
            "Epoch 105: val_accuracy did not improve from 0.92522\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0896 - accuracy: 0.9646 - val_loss: 0.3541 - val_accuracy: 0.9158\n",
            "Epoch 106/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0879 - accuracy: 0.9650\n",
            "Epoch 106: val_accuracy did not improve from 0.92522\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0875 - accuracy: 0.9650 - val_loss: 0.3153 - val_accuracy: 0.9205\n",
            "Epoch 107/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0840 - accuracy: 0.9679\n",
            "Epoch 107: val_accuracy did not improve from 0.92522\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0840 - accuracy: 0.9679 - val_loss: 0.3286 - val_accuracy: 0.9124\n",
            "Epoch 108/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0890 - accuracy: 0.9631\n",
            "Epoch 108: val_accuracy did not improve from 0.92522\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0890 - accuracy: 0.9631 - val_loss: 0.3094 - val_accuracy: 0.9215\n",
            "Epoch 109/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0908 - accuracy: 0.9648\n",
            "Epoch 109: val_accuracy did not improve from 0.92522\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0902 - accuracy: 0.9652 - val_loss: 0.3111 - val_accuracy: 0.9232\n",
            "Epoch 110/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9704\n",
            "Epoch 110: val_accuracy did not improve from 0.92522\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0753 - accuracy: 0.9704 - val_loss: 0.3432 - val_accuracy: 0.9141\n",
            "Epoch 111/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0855 - accuracy: 0.9664\n",
            "Epoch 111: val_accuracy did not improve from 0.92522\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0855 - accuracy: 0.9664 - val_loss: 0.3183 - val_accuracy: 0.9225\n",
            "Epoch 112/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0810 - accuracy: 0.9675\n",
            "Epoch 112: val_accuracy did not improve from 0.92522\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0815 - accuracy: 0.9669 - val_loss: 0.3253 - val_accuracy: 0.9205\n",
            "Epoch 113/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0819 - accuracy: 0.9690\n",
            "Epoch 113: val_accuracy did not improve from 0.92522\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0819 - accuracy: 0.9690 - val_loss: 0.3286 - val_accuracy: 0.9220\n",
            "Epoch 114/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0799 - accuracy: 0.9696\n",
            "Epoch 114: val_accuracy improved from 0.92522 to 0.92720, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0797 - accuracy: 0.9697 - val_loss: 0.3190 - val_accuracy: 0.9272\n",
            "Epoch 115/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0736 - accuracy: 0.9719\n",
            "Epoch 115: val_accuracy did not improve from 0.92720\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0738 - accuracy: 0.9719 - val_loss: 0.3372 - val_accuracy: 0.9166\n",
            "Epoch 116/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9702\n",
            "Epoch 116: val_accuracy did not improve from 0.92720\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0747 - accuracy: 0.9702 - val_loss: 0.3263 - val_accuracy: 0.9267\n",
            "Epoch 117/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0803 - accuracy: 0.9696\n",
            "Epoch 117: val_accuracy did not improve from 0.92720\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0791 - accuracy: 0.9704 - val_loss: 0.3347 - val_accuracy: 0.9272\n",
            "Epoch 118/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0794 - accuracy: 0.9688\n",
            "Epoch 118: val_accuracy did not improve from 0.92720\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0788 - accuracy: 0.9690 - val_loss: 0.3377 - val_accuracy: 0.9247\n",
            "Epoch 119/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0835 - accuracy: 0.9679\n",
            "Epoch 119: val_accuracy did not improve from 0.92720\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0835 - accuracy: 0.9679 - val_loss: 0.3300 - val_accuracy: 0.9242\n",
            "Epoch 120/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0816 - accuracy: 0.9693\n",
            "Epoch 120: val_accuracy improved from 0.92720 to 0.92818, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0816 - accuracy: 0.9693 - val_loss: 0.3212 - val_accuracy: 0.9282\n",
            "Epoch 121/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9719\n",
            "Epoch 121: val_accuracy did not improve from 0.92818\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0742 - accuracy: 0.9719 - val_loss: 0.3297 - val_accuracy: 0.9257\n",
            "Epoch 122/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0748 - accuracy: 0.9712\n",
            "Epoch 122: val_accuracy did not improve from 0.92818\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0741 - accuracy: 0.9715 - val_loss: 0.3273 - val_accuracy: 0.9247\n",
            "Epoch 123/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9720\n",
            "Epoch 123: val_accuracy did not improve from 0.92818\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0748 - accuracy: 0.9720 - val_loss: 0.3345 - val_accuracy: 0.9232\n",
            "Epoch 124/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9715\n",
            "Epoch 124: val_accuracy did not improve from 0.92818\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0731 - accuracy: 0.9715 - val_loss: 0.3622 - val_accuracy: 0.9205\n",
            "Epoch 125/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0744 - accuracy: 0.9711\n",
            "Epoch 125: val_accuracy did not improve from 0.92818\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0748 - accuracy: 0.9708 - val_loss: 0.3387 - val_accuracy: 0.9232\n",
            "Epoch 126/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9701\n",
            "Epoch 126: val_accuracy improved from 0.92818 to 0.93460, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0736 - accuracy: 0.9701 - val_loss: 0.3168 - val_accuracy: 0.9346\n",
            "Epoch 127/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9709\n",
            "Epoch 127: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0737 - accuracy: 0.9709 - val_loss: 0.3262 - val_accuracy: 0.9255\n",
            "Epoch 128/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0772 - accuracy: 0.9703\n",
            "Epoch 128: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0772 - accuracy: 0.9703 - val_loss: 0.3347 - val_accuracy: 0.9309\n",
            "Epoch 129/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9709\n",
            "Epoch 129: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0740 - accuracy: 0.9709 - val_loss: 0.3502 - val_accuracy: 0.9267\n",
            "Epoch 130/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0707 - accuracy: 0.9723\n",
            "Epoch 130: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0708 - accuracy: 0.9724 - val_loss: 0.3579 - val_accuracy: 0.9235\n",
            "Epoch 131/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0646 - accuracy: 0.9765\n",
            "Epoch 131: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0646 - accuracy: 0.9765 - val_loss: 0.3347 - val_accuracy: 0.9299\n",
            "Epoch 132/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0636 - accuracy: 0.9753\n",
            "Epoch 132: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0642 - accuracy: 0.9748 - val_loss: 0.3590 - val_accuracy: 0.9230\n",
            "Epoch 133/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0660 - accuracy: 0.9749\n",
            "Epoch 133: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0660 - accuracy: 0.9749 - val_loss: 0.3563 - val_accuracy: 0.9210\n",
            "Epoch 134/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0645 - accuracy: 0.9746\n",
            "Epoch 134: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0646 - accuracy: 0.9746 - val_loss: 0.3484 - val_accuracy: 0.9267\n",
            "Epoch 135/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0641 - accuracy: 0.9754\n",
            "Epoch 135: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0641 - accuracy: 0.9754 - val_loss: 0.3480 - val_accuracy: 0.9257\n",
            "Epoch 136/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0692 - accuracy: 0.9744\n",
            "Epoch 136: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0694 - accuracy: 0.9742 - val_loss: 0.3454 - val_accuracy: 0.9250\n",
            "Epoch 137/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.0733 - accuracy: 0.9717\n",
            "Epoch 137: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0732 - accuracy: 0.9718 - val_loss: 0.3674 - val_accuracy: 0.9245\n",
            "Epoch 138/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0749 - accuracy: 0.9739\n",
            "Epoch 138: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0745 - accuracy: 0.9741 - val_loss: 0.3406 - val_accuracy: 0.9277\n",
            "Epoch 139/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0636 - accuracy: 0.9765\n",
            "Epoch 139: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0636 - accuracy: 0.9765 - val_loss: 0.3551 - val_accuracy: 0.9265\n",
            "Epoch 140/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0571 - accuracy: 0.9771\n",
            "Epoch 140: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0571 - accuracy: 0.9771 - val_loss: 0.3503 - val_accuracy: 0.9314\n",
            "Epoch 141/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0643 - accuracy: 0.9749\n",
            "Epoch 141: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0646 - accuracy: 0.9745 - val_loss: 0.3624 - val_accuracy: 0.9299\n",
            "Epoch 142/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0597 - accuracy: 0.9769\n",
            "Epoch 142: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0597 - accuracy: 0.9769 - val_loss: 0.3520 - val_accuracy: 0.9309\n",
            "Epoch 143/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0649 - accuracy: 0.9756\n",
            "Epoch 143: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0649 - accuracy: 0.9756 - val_loss: 0.3812 - val_accuracy: 0.9235\n",
            "Epoch 144/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0642 - accuracy: 0.9757\n",
            "Epoch 144: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0642 - accuracy: 0.9757 - val_loss: 0.3698 - val_accuracy: 0.9267\n",
            "Epoch 145/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0633 - accuracy: 0.9762\n",
            "Epoch 145: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0633 - accuracy: 0.9762 - val_loss: 0.3383 - val_accuracy: 0.9267\n",
            "Epoch 146/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0640 - accuracy: 0.9753\n",
            "Epoch 146: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0642 - accuracy: 0.9752 - val_loss: 0.3463 - val_accuracy: 0.9294\n",
            "Epoch 147/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0628 - accuracy: 0.9763\n",
            "Epoch 147: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0625 - accuracy: 0.9764 - val_loss: 0.3651 - val_accuracy: 0.9262\n",
            "Epoch 148/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.0678 - accuracy: 0.9747\n",
            "Epoch 148: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0677 - accuracy: 0.9748 - val_loss: 0.3587 - val_accuracy: 0.9257\n",
            "Epoch 149/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0610 - accuracy: 0.9775\n",
            "Epoch 149: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0610 - accuracy: 0.9775 - val_loss: 0.3571 - val_accuracy: 0.9302\n",
            "Epoch 150/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0611 - accuracy: 0.9777\n",
            "Epoch 150: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0611 - accuracy: 0.9777 - val_loss: 0.3497 - val_accuracy: 0.9260\n",
            "Epoch 151/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9738\n",
            "Epoch 151: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0701 - accuracy: 0.9738 - val_loss: 0.3390 - val_accuracy: 0.9265\n",
            "Epoch 152/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9742\n",
            "Epoch 152: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0693 - accuracy: 0.9742 - val_loss: 0.3242 - val_accuracy: 0.9324\n",
            "Epoch 153/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9746\n",
            "Epoch 153: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0664 - accuracy: 0.9746 - val_loss: 0.3381 - val_accuracy: 0.9307\n",
            "Epoch 154/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0577 - accuracy: 0.9795\n",
            "Epoch 154: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0577 - accuracy: 0.9795 - val_loss: 0.3694 - val_accuracy: 0.9302\n",
            "Epoch 155/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0562 - accuracy: 0.9786\n",
            "Epoch 155: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0563 - accuracy: 0.9786 - val_loss: 0.3811 - val_accuracy: 0.9247\n",
            "Epoch 156/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0644 - accuracy: 0.9746\n",
            "Epoch 156: val_accuracy did not improve from 0.93460\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0644 - accuracy: 0.9746 - val_loss: 0.3398 - val_accuracy: 0.9316\n",
            "127/127 [==============================] - 1s 3ms/step\n",
            "159/159 [==============================] - 0s 3ms/step\n",
            "159/159 [==============================] - 0s 2ms/step\n",
            "KFold 2 of 5\n",
            "Epoch 1/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1464 - accuracy: 0.9565\n",
            "Epoch 1: val_accuracy improved from -inf to 0.98322, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.1464 - accuracy: 0.9565 - val_loss: 0.0490 - val_accuracy: 0.9832\n",
            "Epoch 2/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1182 - accuracy: 0.9601\n",
            "Epoch 2: val_accuracy improved from 0.98322 to 0.98569, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.1182 - accuracy: 0.9601 - val_loss: 0.0518 - val_accuracy: 0.9857\n",
            "Epoch 3/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0938 - accuracy: 0.9680\n",
            "Epoch 3: val_accuracy improved from 0.98569 to 0.98766, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0948 - accuracy: 0.9673 - val_loss: 0.0468 - val_accuracy: 0.9877\n",
            "Epoch 4/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0972 - accuracy: 0.9653\n",
            "Epoch 4: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0972 - accuracy: 0.9653 - val_loss: 0.0565 - val_accuracy: 0.9812\n",
            "Epoch 5/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0921 - accuracy: 0.9671\n",
            "Epoch 5: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0921 - accuracy: 0.9671 - val_loss: 0.0462 - val_accuracy: 0.9845\n",
            "Epoch 6/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0901 - accuracy: 0.9665\n",
            "Epoch 6: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0900 - accuracy: 0.9667 - val_loss: 0.0499 - val_accuracy: 0.9832\n",
            "Epoch 7/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0832 - accuracy: 0.9708\n",
            "Epoch 7: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0832 - accuracy: 0.9708 - val_loss: 0.0493 - val_accuracy: 0.9849\n",
            "Epoch 8/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0830 - accuracy: 0.9691\n",
            "Epoch 8: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0830 - accuracy: 0.9691 - val_loss: 0.0495 - val_accuracy: 0.9835\n",
            "Epoch 9/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0845 - accuracy: 0.9683\n",
            "Epoch 9: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0845 - accuracy: 0.9683 - val_loss: 0.0554 - val_accuracy: 0.9820\n",
            "Epoch 10/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0787 - accuracy: 0.9700\n",
            "Epoch 10: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 2s 24ms/step - loss: 0.0787 - accuracy: 0.9700 - val_loss: 0.0519 - val_accuracy: 0.9827\n",
            "Epoch 11/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9722\n",
            "Epoch 11: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 2s 24ms/step - loss: 0.0730 - accuracy: 0.9722 - val_loss: 0.0505 - val_accuracy: 0.9810\n",
            "Epoch 12/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9760\n",
            "Epoch 12: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0671 - accuracy: 0.9760 - val_loss: 0.0555 - val_accuracy: 0.9835\n",
            "Epoch 13/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9758\n",
            "Epoch 13: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0676 - accuracy: 0.9758 - val_loss: 0.0534 - val_accuracy: 0.9832\n",
            "Epoch 14/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0739 - accuracy: 0.9710\n",
            "Epoch 14: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0732 - accuracy: 0.9713 - val_loss: 0.0575 - val_accuracy: 0.9805\n",
            "Epoch 15/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0754 - accuracy: 0.9726\n",
            "Epoch 15: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0746 - accuracy: 0.9726 - val_loss: 0.0517 - val_accuracy: 0.9835\n",
            "Epoch 16/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0699 - accuracy: 0.9723\n",
            "Epoch 16: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0698 - accuracy: 0.9725 - val_loss: 0.0544 - val_accuracy: 0.9832\n",
            "Epoch 17/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0676 - accuracy: 0.9752\n",
            "Epoch 17: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0673 - accuracy: 0.9751 - val_loss: 0.0618 - val_accuracy: 0.9790\n",
            "Epoch 18/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0618 - accuracy: 0.9773\n",
            "Epoch 18: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0620 - accuracy: 0.9770 - val_loss: 0.0598 - val_accuracy: 0.9778\n",
            "Epoch 19/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0767 - accuracy: 0.9699\n",
            "Epoch 19: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0761 - accuracy: 0.9701 - val_loss: 0.0620 - val_accuracy: 0.9768\n",
            "Epoch 20/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9731\n",
            "Epoch 20: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0707 - accuracy: 0.9731 - val_loss: 0.0640 - val_accuracy: 0.9790\n",
            "Epoch 21/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9715\n",
            "Epoch 21: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0775 - accuracy: 0.9715 - val_loss: 0.1082 - val_accuracy: 0.9598\n",
            "Epoch 22/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0874 - accuracy: 0.9664\n",
            "Epoch 22: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0868 - accuracy: 0.9666 - val_loss: 0.0676 - val_accuracy: 0.9775\n",
            "Epoch 23/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0705 - accuracy: 0.9725\n",
            "Epoch 23: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0702 - accuracy: 0.9725 - val_loss: 0.0677 - val_accuracy: 0.9770\n",
            "Epoch 24/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0647 - accuracy: 0.9760\n",
            "Epoch 24: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0647 - accuracy: 0.9760 - val_loss: 0.0676 - val_accuracy: 0.9753\n",
            "Epoch 25/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0642 - accuracy: 0.9758\n",
            "Epoch 25: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0642 - accuracy: 0.9758 - val_loss: 0.0666 - val_accuracy: 0.9778\n",
            "Epoch 26/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0640 - accuracy: 0.9763\n",
            "Epoch 26: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0640 - accuracy: 0.9763 - val_loss: 0.0620 - val_accuracy: 0.9775\n",
            "Epoch 27/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0638 - accuracy: 0.9775\n",
            "Epoch 27: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0645 - accuracy: 0.9772 - val_loss: 0.0742 - val_accuracy: 0.9733\n",
            "Epoch 28/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0624 - accuracy: 0.9766\n",
            "Epoch 28: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0626 - accuracy: 0.9765 - val_loss: 0.0702 - val_accuracy: 0.9768\n",
            "Epoch 29/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0599 - accuracy: 0.9767\n",
            "Epoch 29: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0599 - accuracy: 0.9767 - val_loss: 0.0762 - val_accuracy: 0.9736\n",
            "Epoch 30/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0626 - accuracy: 0.9779\n",
            "Epoch 30: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0626 - accuracy: 0.9779 - val_loss: 0.0736 - val_accuracy: 0.9741\n",
            "Epoch 31/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0553 - accuracy: 0.9795\n",
            "Epoch 31: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0559 - accuracy: 0.9793 - val_loss: 0.0628 - val_accuracy: 0.9793\n",
            "Epoch 32/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0584 - accuracy: 0.9785\n",
            "Epoch 32: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0584 - accuracy: 0.9785 - val_loss: 0.0688 - val_accuracy: 0.9785\n",
            "Epoch 33/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0580 - accuracy: 0.9785\n",
            "Epoch 33: val_accuracy did not improve from 0.98766\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0580 - accuracy: 0.9785 - val_loss: 0.0696 - val_accuracy: 0.9766\n",
            "127/127 [==============================] - 0s 3ms/step\n",
            "159/159 [==============================] - 0s 3ms/step\n",
            "159/159 [==============================] - 0s 2ms/step\n",
            "KFold 3 of 5\n",
            "Epoch 1/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.0732 - accuracy: 0.9743\n",
            "Epoch 1: val_accuracy improved from -inf to 0.99013, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 2s 24ms/step - loss: 0.0734 - accuracy: 0.9742 - val_loss: 0.0313 - val_accuracy: 0.9901\n",
            "Epoch 2/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9725\n",
            "Epoch 2: val_accuracy improved from 0.99013 to 0.99112, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0701 - accuracy: 0.9725 - val_loss: 0.0316 - val_accuracy: 0.9911\n",
            "Epoch 3/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9740\n",
            "Epoch 3: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0674 - accuracy: 0.9740 - val_loss: 0.0338 - val_accuracy: 0.9891\n",
            "Epoch 4/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0642 - accuracy: 0.9761\n",
            "Epoch 4: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0642 - accuracy: 0.9761 - val_loss: 0.0387 - val_accuracy: 0.9884\n",
            "Epoch 5/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9752\n",
            "Epoch 5: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0688 - accuracy: 0.9752 - val_loss: 0.0379 - val_accuracy: 0.9864\n",
            "Epoch 6/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0639 - accuracy: 0.9759\n",
            "Epoch 6: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0639 - accuracy: 0.9759 - val_loss: 0.0394 - val_accuracy: 0.9862\n",
            "Epoch 7/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0619 - accuracy: 0.9763\n",
            "Epoch 7: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0619 - accuracy: 0.9763 - val_loss: 0.0421 - val_accuracy: 0.9852\n",
            "Epoch 8/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0633 - accuracy: 0.9774\n",
            "Epoch 8: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0633 - accuracy: 0.9774 - val_loss: 0.0374 - val_accuracy: 0.9859\n",
            "Epoch 9/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0577 - accuracy: 0.9780\n",
            "Epoch 9: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0577 - accuracy: 0.9780 - val_loss: 0.0382 - val_accuracy: 0.9877\n",
            "Epoch 10/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0678 - accuracy: 0.9767\n",
            "Epoch 10: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0674 - accuracy: 0.9767 - val_loss: 0.0398 - val_accuracy: 0.9877\n",
            "Epoch 11/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0598 - accuracy: 0.9775\n",
            "Epoch 11: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0597 - accuracy: 0.9775 - val_loss: 0.0398 - val_accuracy: 0.9877\n",
            "Epoch 12/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0632 - accuracy: 0.9766\n",
            "Epoch 12: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0632 - accuracy: 0.9766 - val_loss: 0.0490 - val_accuracy: 0.9825\n",
            "Epoch 13/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0613 - accuracy: 0.9780\n",
            "Epoch 13: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0623 - accuracy: 0.9778 - val_loss: 0.0455 - val_accuracy: 0.9840\n",
            "Epoch 14/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0570 - accuracy: 0.9785\n",
            "Epoch 14: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0565 - accuracy: 0.9788 - val_loss: 0.0451 - val_accuracy: 0.9835\n",
            "Epoch 15/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0545 - accuracy: 0.9796\n",
            "Epoch 15: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0545 - accuracy: 0.9796 - val_loss: 0.0484 - val_accuracy: 0.9830\n",
            "Epoch 16/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0538 - accuracy: 0.9807\n",
            "Epoch 16: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0540 - accuracy: 0.9808 - val_loss: 0.0452 - val_accuracy: 0.9830\n",
            "Epoch 17/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.0512 - accuracy: 0.9820\n",
            "Epoch 17: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0515 - accuracy: 0.9819 - val_loss: 0.0448 - val_accuracy: 0.9837\n",
            "Epoch 18/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.0567 - accuracy: 0.9786\n",
            "Epoch 18: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 2s 24ms/step - loss: 0.0566 - accuracy: 0.9787 - val_loss: 0.0468 - val_accuracy: 0.9832\n",
            "Epoch 19/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0480 - accuracy: 0.9825\n",
            "Epoch 19: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0480 - accuracy: 0.9825 - val_loss: 0.0522 - val_accuracy: 0.9812\n",
            "Epoch 20/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0495 - accuracy: 0.9808\n",
            "Epoch 20: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0495 - accuracy: 0.9808 - val_loss: 0.0453 - val_accuracy: 0.9815\n",
            "Epoch 21/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.9796\n",
            "Epoch 21: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0518 - accuracy: 0.9796 - val_loss: 0.0519 - val_accuracy: 0.9817\n",
            "Epoch 22/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0502 - accuracy: 0.9816\n",
            "Epoch 22: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0502 - accuracy: 0.9816 - val_loss: 0.0570 - val_accuracy: 0.9798\n",
            "Epoch 23/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0549 - accuracy: 0.9801\n",
            "Epoch 23: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0549 - accuracy: 0.9800 - val_loss: 0.0526 - val_accuracy: 0.9825\n",
            "Epoch 24/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.9799\n",
            "Epoch 24: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0528 - accuracy: 0.9799 - val_loss: 0.0484 - val_accuracy: 0.9812\n",
            "Epoch 25/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.9811\n",
            "Epoch 25: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0517 - accuracy: 0.9811 - val_loss: 0.0569 - val_accuracy: 0.9775\n",
            "Epoch 26/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0497 - accuracy: 0.9820\n",
            "Epoch 26: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0494 - accuracy: 0.9822 - val_loss: 0.0515 - val_accuracy: 0.9817\n",
            "Epoch 27/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0500 - accuracy: 0.9809\n",
            "Epoch 27: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0496 - accuracy: 0.9810 - val_loss: 0.0509 - val_accuracy: 0.9815\n",
            "Epoch 28/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0500 - accuracy: 0.9817\n",
            "Epoch 28: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0500 - accuracy: 0.9817 - val_loss: 0.0620 - val_accuracy: 0.9773\n",
            "Epoch 29/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0496 - accuracy: 0.9827\n",
            "Epoch 29: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0496 - accuracy: 0.9827 - val_loss: 0.0482 - val_accuracy: 0.9812\n",
            "Epoch 30/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0464 - accuracy: 0.9824\n",
            "Epoch 30: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0465 - accuracy: 0.9824 - val_loss: 0.0543 - val_accuracy: 0.9825\n",
            "Epoch 31/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0486 - accuracy: 0.9806\n",
            "Epoch 31: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0486 - accuracy: 0.9806 - val_loss: 0.0548 - val_accuracy: 0.9783\n",
            "Epoch 32/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0453 - accuracy: 0.9832\n",
            "Epoch 32: val_accuracy did not improve from 0.99112\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0453 - accuracy: 0.9831 - val_loss: 0.0547 - val_accuracy: 0.9822\n",
            "127/127 [==============================] - 0s 2ms/step\n",
            "159/159 [==============================] - 0s 2ms/step\n",
            "159/159 [==============================] - 0s 2ms/step\n",
            "KFold 4 of 5\n",
            "Epoch 1/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.0651 - accuracy: 0.9759\n",
            "Epoch 1: val_accuracy improved from -inf to 0.99408, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 2s 24ms/step - loss: 0.0649 - accuracy: 0.9759 - val_loss: 0.0205 - val_accuracy: 0.9941\n",
            "Epoch 2/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0583 - accuracy: 0.9780\n",
            "Epoch 2: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0580 - accuracy: 0.9782 - val_loss: 0.0227 - val_accuracy: 0.9933\n",
            "Epoch 3/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0578 - accuracy: 0.9797\n",
            "Epoch 3: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0578 - accuracy: 0.9797 - val_loss: 0.0404 - val_accuracy: 0.9830\n",
            "Epoch 4/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0617 - accuracy: 0.9762\n",
            "Epoch 4: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0612 - accuracy: 0.9762 - val_loss: 0.0270 - val_accuracy: 0.9889\n",
            "Epoch 5/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0553 - accuracy: 0.9796\n",
            "Epoch 5: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0553 - accuracy: 0.9796 - val_loss: 0.0226 - val_accuracy: 0.9928\n",
            "Epoch 6/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0569 - accuracy: 0.9793\n",
            "Epoch 6: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0565 - accuracy: 0.9795 - val_loss: 0.0223 - val_accuracy: 0.9933\n",
            "Epoch 7/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9795\n",
            "Epoch 7: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0521 - accuracy: 0.9795 - val_loss: 0.0225 - val_accuracy: 0.9931\n",
            "Epoch 8/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0539 - accuracy: 0.9803\n",
            "Epoch 8: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0539 - accuracy: 0.9803 - val_loss: 0.0234 - val_accuracy: 0.9928\n",
            "Epoch 9/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0497 - accuracy: 0.9816\n",
            "Epoch 9: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0489 - accuracy: 0.9819 - val_loss: 0.0234 - val_accuracy: 0.9931\n",
            "Epoch 10/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0515 - accuracy: 0.9799\n",
            "Epoch 10: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0515 - accuracy: 0.9799 - val_loss: 0.0263 - val_accuracy: 0.9906\n",
            "Epoch 11/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.9814\n",
            "Epoch 11: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0484 - accuracy: 0.9814 - val_loss: 0.0254 - val_accuracy: 0.9909\n",
            "Epoch 12/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.9815\n",
            "Epoch 12: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0484 - accuracy: 0.9815 - val_loss: 0.0255 - val_accuracy: 0.9926\n",
            "Epoch 13/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0465 - accuracy: 0.9821\n",
            "Epoch 13: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0465 - accuracy: 0.9821 - val_loss: 0.0253 - val_accuracy: 0.9936\n",
            "Epoch 14/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0496 - accuracy: 0.9810\n",
            "Epoch 14: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0496 - accuracy: 0.9810 - val_loss: 0.0268 - val_accuracy: 0.9921\n",
            "Epoch 15/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0494 - accuracy: 0.9812\n",
            "Epoch 15: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0494 - accuracy: 0.9812 - val_loss: 0.0262 - val_accuracy: 0.9921\n",
            "Epoch 16/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0557 - accuracy: 0.9798\n",
            "Epoch 16: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0557 - accuracy: 0.9798 - val_loss: 0.0284 - val_accuracy: 0.9906\n",
            "Epoch 17/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0456 - accuracy: 0.9830\n",
            "Epoch 17: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0456 - accuracy: 0.9832 - val_loss: 0.0253 - val_accuracy: 0.9926\n",
            "Epoch 18/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 0.9839\n",
            "Epoch 18: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0454 - accuracy: 0.9839 - val_loss: 0.0322 - val_accuracy: 0.9891\n",
            "Epoch 19/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0483 - accuracy: 0.9836\n",
            "Epoch 19: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0483 - accuracy: 0.9836 - val_loss: 0.0301 - val_accuracy: 0.9889\n",
            "Epoch 20/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0416 - accuracy: 0.9844\n",
            "Epoch 20: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0416 - accuracy: 0.9844 - val_loss: 0.0325 - val_accuracy: 0.9884\n",
            "Epoch 21/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0498 - accuracy: 0.9819\n",
            "Epoch 21: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0498 - accuracy: 0.9819 - val_loss: 0.0271 - val_accuracy: 0.9901\n",
            "Epoch 22/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0441 - accuracy: 0.9832\n",
            "Epoch 22: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0441 - accuracy: 0.9832 - val_loss: 0.0361 - val_accuracy: 0.9857\n",
            "Epoch 23/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0455 - accuracy: 0.9828\n",
            "Epoch 23: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0455 - accuracy: 0.9828 - val_loss: 0.0292 - val_accuracy: 0.9886\n",
            "Epoch 24/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 0.9828\n",
            "Epoch 24: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0454 - accuracy: 0.9828 - val_loss: 0.0362 - val_accuracy: 0.9874\n",
            "Epoch 25/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0441 - accuracy: 0.9841\n",
            "Epoch 25: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0441 - accuracy: 0.9841 - val_loss: 0.0455 - val_accuracy: 0.9815\n",
            "Epoch 26/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.0404 - accuracy: 0.9857\n",
            "Epoch 26: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0404 - accuracy: 0.9857 - val_loss: 0.0383 - val_accuracy: 0.9879\n",
            "Epoch 27/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0464 - accuracy: 0.9829\n",
            "Epoch 27: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0465 - accuracy: 0.9828 - val_loss: 0.0261 - val_accuracy: 0.9923\n",
            "Epoch 28/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0472 - accuracy: 0.9839\n",
            "Epoch 28: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0473 - accuracy: 0.9838 - val_loss: 0.0339 - val_accuracy: 0.9862\n",
            "Epoch 29/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 0.9828\n",
            "Epoch 29: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0454 - accuracy: 0.9828 - val_loss: 0.0326 - val_accuracy: 0.9884\n",
            "Epoch 30/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0451 - accuracy: 0.9841\n",
            "Epoch 30: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0453 - accuracy: 0.9840 - val_loss: 0.0324 - val_accuracy: 0.9886\n",
            "Epoch 31/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0539 - accuracy: 0.9814\n",
            "Epoch 31: val_accuracy did not improve from 0.99408\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0536 - accuracy: 0.9814 - val_loss: 0.0384 - val_accuracy: 0.9852\n",
            "127/127 [==============================] - 0s 3ms/step\n",
            "159/159 [==============================] - 0s 2ms/step\n",
            "159/159 [==============================] - 0s 2ms/step\n",
            "KFold 5 of 5\n",
            "Epoch 1/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0556 - accuracy: 0.9788\n",
            "Epoch 1: val_accuracy improved from -inf to 0.99605, saving model to ./cnn1d.h5\n",
            "64/64 [==============================] - 2s 32ms/step - loss: 0.0556 - accuracy: 0.9788 - val_loss: 0.0166 - val_accuracy: 0.9961\n",
            "Epoch 2/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9804\n",
            "Epoch 2: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0521 - accuracy: 0.9804 - val_loss: 0.0155 - val_accuracy: 0.9958\n",
            "Epoch 3/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0489 - accuracy: 0.9816\n",
            "Epoch 3: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0489 - accuracy: 0.9816 - val_loss: 0.0225 - val_accuracy: 0.9923\n",
            "Epoch 4/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0465 - accuracy: 0.9828\n",
            "Epoch 4: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0465 - accuracy: 0.9828 - val_loss: 0.0180 - val_accuracy: 0.9946\n",
            "Epoch 5/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0442 - accuracy: 0.9832\n",
            "Epoch 5: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0442 - accuracy: 0.9831 - val_loss: 0.0208 - val_accuracy: 0.9936\n",
            "Epoch 6/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0462 - accuracy: 0.9825\n",
            "Epoch 6: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0462 - accuracy: 0.9827 - val_loss: 0.0188 - val_accuracy: 0.9936\n",
            "Epoch 7/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0432 - accuracy: 0.9841\n",
            "Epoch 7: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0432 - accuracy: 0.9841 - val_loss: 0.0180 - val_accuracy: 0.9943\n",
            "Epoch 8/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0427 - accuracy: 0.9833\n",
            "Epoch 8: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0427 - accuracy: 0.9833 - val_loss: 0.0211 - val_accuracy: 0.9936\n",
            "Epoch 9/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0444 - accuracy: 0.9835\n",
            "Epoch 9: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0441 - accuracy: 0.9837 - val_loss: 0.0186 - val_accuracy: 0.9951\n",
            "Epoch 10/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.0448 - accuracy: 0.9841\n",
            "Epoch 10: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0447 - accuracy: 0.9842 - val_loss: 0.0237 - val_accuracy: 0.9923\n",
            "Epoch 11/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0443 - accuracy: 0.9836\n",
            "Epoch 11: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0443 - accuracy: 0.9836 - val_loss: 0.0274 - val_accuracy: 0.9906\n",
            "Epoch 12/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0497 - accuracy: 0.9819\n",
            "Epoch 12: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0497 - accuracy: 0.9819 - val_loss: 0.0239 - val_accuracy: 0.9916\n",
            "Epoch 13/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0409 - accuracy: 0.9850\n",
            "Epoch 13: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0407 - accuracy: 0.9850 - val_loss: 0.0186 - val_accuracy: 0.9941\n",
            "Epoch 14/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0396 - accuracy: 0.9858\n",
            "Epoch 14: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0399 - accuracy: 0.9855 - val_loss: 0.0268 - val_accuracy: 0.9904\n",
            "Epoch 15/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0428 - accuracy: 0.9846\n",
            "Epoch 15: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0428 - accuracy: 0.9846 - val_loss: 0.0200 - val_accuracy: 0.9936\n",
            "Epoch 16/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.0402 - accuracy: 0.9846\n",
            "Epoch 16: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0403 - accuracy: 0.9846 - val_loss: 0.0212 - val_accuracy: 0.9933\n",
            "Epoch 17/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0414 - accuracy: 0.9856\n",
            "Epoch 17: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0414 - accuracy: 0.9856 - val_loss: 0.0240 - val_accuracy: 0.9911\n",
            "Epoch 18/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0414 - accuracy: 0.9834\n",
            "Epoch 18: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0415 - accuracy: 0.9834 - val_loss: 0.0231 - val_accuracy: 0.9923\n",
            "Epoch 19/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0444 - accuracy: 0.9840\n",
            "Epoch 19: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0444 - accuracy: 0.9840 - val_loss: 0.0271 - val_accuracy: 0.9896\n",
            "Epoch 20/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0422 - accuracy: 0.9849\n",
            "Epoch 20: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0431 - accuracy: 0.9846 - val_loss: 0.0267 - val_accuracy: 0.9909\n",
            "Epoch 21/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.0450 - accuracy: 0.9836\n",
            "Epoch 21: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0449 - accuracy: 0.9836 - val_loss: 0.0283 - val_accuracy: 0.9896\n",
            "Epoch 22/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9846\n",
            "Epoch 22: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 2s 24ms/step - loss: 0.0403 - accuracy: 0.9846 - val_loss: 0.0366 - val_accuracy: 0.9882\n",
            "Epoch 23/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0425 - accuracy: 0.9840\n",
            "Epoch 23: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0425 - accuracy: 0.9840 - val_loss: 0.0328 - val_accuracy: 0.9884\n",
            "Epoch 24/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0416 - accuracy: 0.9846\n",
            "Epoch 24: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0417 - accuracy: 0.9848 - val_loss: 0.0272 - val_accuracy: 0.9889\n",
            "Epoch 25/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.0397 - accuracy: 0.9860\n",
            "Epoch 25: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0396 - accuracy: 0.9861 - val_loss: 0.0317 - val_accuracy: 0.9899\n",
            "Epoch 26/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0411 - accuracy: 0.9844\n",
            "Epoch 26: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0414 - accuracy: 0.9845 - val_loss: 0.0314 - val_accuracy: 0.9879\n",
            "Epoch 27/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9850\n",
            "Epoch 27: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0403 - accuracy: 0.9850 - val_loss: 0.0261 - val_accuracy: 0.9906\n",
            "Epoch 28/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0416 - accuracy: 0.9839\n",
            "Epoch 28: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0413 - accuracy: 0.9842 - val_loss: 0.0381 - val_accuracy: 0.9862\n",
            "Epoch 29/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0458 - accuracy: 0.9837\n",
            "Epoch 29: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0458 - accuracy: 0.9837 - val_loss: 0.0286 - val_accuracy: 0.9899\n",
            "Epoch 30/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9873\n",
            "Epoch 30: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.0348 - accuracy: 0.9873 - val_loss: 0.0286 - val_accuracy: 0.9884\n",
            "Epoch 31/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0410 - accuracy: 0.9853\n",
            "Epoch 31: val_accuracy did not improve from 0.99605\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.0410 - accuracy: 0.9853 - val_loss: 0.0355 - val_accuracy: 0.9872\n",
            "127/127 [==============================] - 0s 4ms/step\n",
            "159/159 [==============================] - 0s 2ms/step\n",
            "159/159 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"###1D-CNN Classifier###\\n\")\n",
        "print(\"Accuracy: \" + str(round(mean(acc_score_cnn1d),4)) + \" +- \"+ str(round(std(acc_score_cnn1d),4)))\n",
        "print(\"ROC-AUC: \" + str(round(mean(auc_score_cnn1d),4)) + \" +- \" + str(round(std(auc_score_cnn1d),4)))\n",
        "print(\"F1-Score: \" + str(round(mean(f1_cnn1d), 4)) +\" +- \"+ str(round(std(f1_cnn1d),4)))\n",
        "\n",
        "yhat = cnn1d.predict(X_test_cnn).round()\n",
        "print(classification_report(y_test, yhat))\n",
        "cm = confusion_matrix(yhat, y_test, labels=[0,1])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
        "disp.plot()\n",
        "RocCurveDisplay.from_predictions(yhat, y_test)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        },
        "id": "s1UGL0bXXRgH",
        "outputId": "97b2e105-b643-45a6-891e-5fb22f4a7373"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###1D-CNN Classifier###\n",
            "\n",
            "Accuracy: 0.9398 +- 0.0041\n",
            "ROC-AUC: 0.9397 +- 0.0041\n",
            "F1-Score: 0.9504 +- 0.0033\n",
            "159/159 [==============================] - 0s 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.92      0.93      2024\n",
            "           1       0.95      0.96      0.95      3041\n",
            "\n",
            "    accuracy                           0.95      5065\n",
            "   macro avg       0.94      0.94      0.94      5065\n",
            "weighted avg       0.95      0.95      0.95      5065\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "free_gpu_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8y-U23N_XXng",
        "outputId": "c58ff7ca-af94-42b2-9081-00e6de0df1ed"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial GPU Usage\n",
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 |  0% | 36% |\n",
            "GPU Usage after emptying the cache\n",
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 | 97% |  1% |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########### TabNet #############"
      ],
      "metadata": {
        "id": "MXHRwWEOXbvF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class roc_auc(Metric):\n",
        "    def __init__(self):\n",
        "        self._name = \"roc_auc\"\n",
        "        self._maximize = True\n",
        "\n",
        "    def __call__(self, y_true, y_score):\n",
        "        roc_auc = roc_auc_score(y_true, y_score[:, 1])\n",
        "        return roc_auc"
      ],
      "metadata": {
        "id": "gI_XTzOuXeF2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tabnet = TabNetClassifier(\n",
        "                        n_d = 64,\n",
        "                        n_a = 64,\n",
        "                        n_steps = 3,\n",
        "                        gamma = 1.3,\n",
        "                        n_independent = 1,\n",
        "                        n_shared = 2,\n",
        "                        momentum = 0.02,\n",
        "                        clip_value = None,\n",
        "                        lambda_sparse = 1e-3,\n",
        "                        optimizer_fn = torch.optim.Adam,\n",
        "                        optimizer_params = dict(lr = 1e-3, weight_decay=1e-3),\n",
        "                        scheduler_fn = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,\n",
        "                        scheduler_params = {'T_0':5,\n",
        "                                            'eta_min':1e-4,\n",
        "                                            'T_mult':1,\n",
        "                                            'last_epoch':-1},\n",
        "                        mask_type = 'entmax',\n",
        "                        verbose = 1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxMFluquXf9s",
        "outputId": "3bf8da47-2830-46cc-c9b2-12bb8d3607e6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=n_fold, shuffle = False)\n",
        "acc_score_tabnet = []\n",
        "auc_score_tabnet = []\n",
        "f1_tabnet = []\n",
        "meta_train_tabnet = []\n",
        "meta_test_tabnet = []\n",
        "i = 1\n",
        "for train_index, valid_index in kf.split(X_train, y_train):\n",
        "    print('KFold {} of {}'.format(i,kf.n_splits))\n",
        "    train_X, val_X = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
        "    train_y, val_y = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
        "    tabnet.fit(np.array(train_X), np.array(train_y).ravel(),\n",
        "      eval_set=[(np.array(val_X), np.array(val_y).ravel())],\n",
        "      eval_metric=[roc_auc, \"accuracy\"],\n",
        "      max_epochs = 500,\n",
        "      patience = 50,\n",
        "      batch_size = 256)\n",
        "    ####meta\n",
        "    meta_train_tabnet = np.append(meta_train_tabnet, tabnet.predict_proba(np.array(val_X))[:,1].reshape(-1,1))\n",
        "    \n",
        "    if len(meta_test_tabnet) == 0:\n",
        "        meta_test_tabnet = tabnet.predict_proba(np.array(X_test))[:,1].reshape(-1,1)\n",
        "    else:\n",
        "        meta_test_tabnet = np.add(meta_test_tabnet, tabnet.predict_proba(np.array(X_test))[:,1].reshape(-1,1))\n",
        "    #####\n",
        "    yhat = tabnet.predict(np.array(X_test)).round()\n",
        "    acc_score_tabnet.append(accuracy_score(yhat,y_test))\n",
        "    auc_score_tabnet.append(roc_auc_score(yhat,y_test))\n",
        "    f1_tabnet.append(f1_score(yhat,y_test))\n",
        "    i += 1\n",
        "meta_test_tabnet = np.divide(meta_test_tabnet, n_fold)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDuyXkcpXhTU",
        "outputId": "e97ffe14-1854-49a6-f39d-5d2772bea7c5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KFold 1 of 5\n",
            "epoch 0  | loss: 0.82342 | val_0_roc_auc: 0.71505 | val_0_accuracy: 0.65573 |  0:00:02s\n",
            "epoch 1  | loss: 0.63359 | val_0_roc_auc: 0.76195 | val_0_accuracy: 0.69472 |  0:00:03s\n",
            "epoch 2  | loss: 0.58195 | val_0_roc_auc: 0.79178 | val_0_accuracy: 0.71594 |  0:00:04s\n",
            "epoch 3  | loss: 0.55473 | val_0_roc_auc: 0.8016  | val_0_accuracy: 0.72211 |  0:00:06s\n",
            "epoch 4  | loss: 0.54467 | val_0_roc_auc: 0.80299 | val_0_accuracy: 0.72384 |  0:00:07s\n",
            "epoch 5  | loss: 0.53444 | val_0_roc_auc: 0.81709 | val_0_accuracy: 0.73297 |  0:00:09s\n",
            "epoch 6  | loss: 0.51497 | val_0_roc_auc: 0.8265  | val_0_accuracy: 0.73618 |  0:00:10s\n",
            "epoch 7  | loss: 0.50351 | val_0_roc_auc: 0.8326  | val_0_accuracy: 0.7421  |  0:00:12s\n",
            "epoch 8  | loss: 0.48797 | val_0_roc_auc: 0.83589 | val_0_accuracy: 0.74013 |  0:00:13s\n",
            "epoch 9  | loss: 0.48411 | val_0_roc_auc: 0.83886 | val_0_accuracy: 0.7421  |  0:00:15s\n",
            "epoch 10 | loss: 0.4842  | val_0_roc_auc: 0.84601 | val_0_accuracy: 0.75913 |  0:00:16s\n",
            "epoch 11 | loss: 0.47121 | val_0_roc_auc: 0.85423 | val_0_accuracy: 0.76925 |  0:00:18s\n",
            "epoch 12 | loss: 0.45553 | val_0_roc_auc: 0.85541 | val_0_accuracy: 0.77073 |  0:00:19s\n",
            "epoch 13 | loss: 0.45296 | val_0_roc_auc: 0.85959 | val_0_accuracy: 0.77838 |  0:00:21s\n",
            "epoch 14 | loss: 0.44801 | val_0_roc_auc: 0.86261 | val_0_accuracy: 0.77937 |  0:00:22s\n",
            "epoch 15 | loss: 0.44778 | val_0_roc_auc: 0.86743 | val_0_accuracy: 0.77739 |  0:00:23s\n",
            "epoch 16 | loss: 0.44069 | val_0_roc_auc: 0.87319 | val_0_accuracy: 0.78998 |  0:00:25s\n",
            "epoch 17 | loss: 0.43408 | val_0_roc_auc: 0.87465 | val_0_accuracy: 0.78504 |  0:00:26s\n",
            "epoch 18 | loss: 0.42826 | val_0_roc_auc: 0.87685 | val_0_accuracy: 0.79121 |  0:00:28s\n",
            "epoch 19 | loss: 0.42047 | val_0_roc_auc: 0.87799 | val_0_accuracy: 0.7922  |  0:00:29s\n",
            "epoch 20 | loss: 0.42536 | val_0_roc_auc: 0.87902 | val_0_accuracy: 0.79418 |  0:00:31s\n",
            "epoch 21 | loss: 0.41798 | val_0_roc_auc: 0.88173 | val_0_accuracy: 0.7922  |  0:00:32s\n",
            "epoch 22 | loss: 0.41247 | val_0_roc_auc: 0.88425 | val_0_accuracy: 0.79319 |  0:00:34s\n",
            "epoch 23 | loss: 0.40254 | val_0_roc_auc: 0.88824 | val_0_accuracy: 0.79862 |  0:00:35s\n",
            "epoch 24 | loss: 0.39676 | val_0_roc_auc: 0.88978 | val_0_accuracy: 0.79911 |  0:00:36s\n",
            "epoch 25 | loss: 0.40235 | val_0_roc_auc: 0.88789 | val_0_accuracy: 0.79738 |  0:00:38s\n",
            "epoch 26 | loss: 0.39257 | val_0_roc_auc: 0.8933  | val_0_accuracy: 0.80652 |  0:00:40s\n",
            "epoch 27 | loss: 0.3833  | val_0_roc_auc: 0.89386 | val_0_accuracy: 0.80183 |  0:00:42s\n",
            "epoch 28 | loss: 0.37544 | val_0_roc_auc: 0.89956 | val_0_accuracy: 0.81441 |  0:00:43s\n",
            "epoch 29 | loss: 0.36861 | val_0_roc_auc: 0.90152 | val_0_accuracy: 0.81713 |  0:00:45s\n",
            "epoch 30 | loss: 0.37787 | val_0_roc_auc: 0.89988 | val_0_accuracy: 0.81441 |  0:00:46s\n",
            "epoch 31 | loss: 0.37957 | val_0_roc_auc: 0.89998 | val_0_accuracy: 0.81269 |  0:00:48s\n",
            "epoch 32 | loss: 0.36395 | val_0_roc_auc: 0.90444 | val_0_accuracy: 0.81466 |  0:00:49s\n",
            "epoch 33 | loss: 0.35557 | val_0_roc_auc: 0.90681 | val_0_accuracy: 0.8191  |  0:00:51s\n",
            "epoch 34 | loss: 0.34606 | val_0_roc_auc: 0.90806 | val_0_accuracy: 0.8233  |  0:00:52s\n",
            "epoch 35 | loss: 0.35875 | val_0_roc_auc: 0.90442 | val_0_accuracy: 0.8154  |  0:00:53s\n",
            "epoch 36 | loss: 0.3525  | val_0_roc_auc: 0.91042 | val_0_accuracy: 0.8233  |  0:00:55s\n",
            "epoch 37 | loss: 0.3372  | val_0_roc_auc: 0.91206 | val_0_accuracy: 0.82552 |  0:00:56s\n",
            "epoch 38 | loss: 0.33492 | val_0_roc_auc: 0.91589 | val_0_accuracy: 0.83638 |  0:00:58s\n",
            "epoch 39 | loss: 0.32756 | val_0_roc_auc: 0.92081 | val_0_accuracy: 0.83885 |  0:00:59s\n",
            "epoch 40 | loss: 0.34341 | val_0_roc_auc: 0.9127  | val_0_accuracy: 0.83144 |  0:01:01s\n",
            "epoch 41 | loss: 0.33712 | val_0_roc_auc: 0.91551 | val_0_accuracy: 0.83292 |  0:01:02s\n",
            "epoch 42 | loss: 0.32652 | val_0_roc_auc: 0.9192  | val_0_accuracy: 0.83564 |  0:01:04s\n",
            "epoch 43 | loss: 0.31143 | val_0_roc_auc: 0.92663 | val_0_accuracy: 0.8497  |  0:01:05s\n",
            "epoch 44 | loss: 0.30363 | val_0_roc_auc: 0.92892 | val_0_accuracy: 0.85168 |  0:01:07s\n",
            "epoch 45 | loss: 0.32049 | val_0_roc_auc: 0.91771 | val_0_accuracy: 0.82527 |  0:01:08s\n",
            "epoch 46 | loss: 0.31745 | val_0_roc_auc: 0.92503 | val_0_accuracy: 0.84427 |  0:01:09s\n",
            "epoch 47 | loss: 0.31101 | val_0_roc_auc: 0.92314 | val_0_accuracy: 0.83983 |  0:01:11s\n",
            "epoch 48 | loss: 0.29435 | val_0_roc_auc: 0.92988 | val_0_accuracy: 0.85168 |  0:01:12s\n",
            "epoch 49 | loss: 0.2845  | val_0_roc_auc: 0.93384 | val_0_accuracy: 0.86032 |  0:01:14s\n",
            "epoch 50 | loss: 0.31248 | val_0_roc_auc: 0.92026 | val_0_accuracy: 0.83317 |  0:01:15s\n",
            "epoch 51 | loss: 0.30842 | val_0_roc_auc: 0.92715 | val_0_accuracy: 0.84304 |  0:01:17s\n",
            "epoch 52 | loss: 0.29714 | val_0_roc_auc: 0.92931 | val_0_accuracy: 0.85316 |  0:01:18s\n",
            "epoch 53 | loss: 0.27867 | val_0_roc_auc: 0.93539 | val_0_accuracy: 0.8618  |  0:01:20s\n",
            "epoch 54 | loss: 0.26315 | val_0_roc_auc: 0.93988 | val_0_accuracy: 0.8692  |  0:01:21s\n",
            "epoch 55 | loss: 0.29767 | val_0_roc_auc: 0.92705 | val_0_accuracy: 0.84896 |  0:01:23s\n",
            "epoch 56 | loss: 0.29564 | val_0_roc_auc: 0.93027 | val_0_accuracy: 0.85044 |  0:01:24s\n",
            "epoch 57 | loss: 0.28152 | val_0_roc_auc: 0.93039 | val_0_accuracy: 0.85464 |  0:01:25s\n",
            "epoch 58 | loss: 0.26835 | val_0_roc_auc: 0.93772 | val_0_accuracy: 0.86525 |  0:01:27s\n",
            "epoch 59 | loss: 0.25502 | val_0_roc_auc: 0.94042 | val_0_accuracy: 0.8692  |  0:01:28s\n",
            "epoch 60 | loss: 0.28539 | val_0_roc_auc: 0.92747 | val_0_accuracy: 0.84872 |  0:01:30s\n",
            "epoch 61 | loss: 0.28655 | val_0_roc_auc: 0.93167 | val_0_accuracy: 0.85489 |  0:01:31s\n",
            "epoch 62 | loss: 0.27542 | val_0_roc_auc: 0.93453 | val_0_accuracy: 0.8576  |  0:01:33s\n",
            "epoch 63 | loss: 0.25869 | val_0_roc_auc: 0.93932 | val_0_accuracy: 0.87068 |  0:01:34s\n",
            "epoch 64 | loss: 0.23893 | val_0_roc_auc: 0.94324 | val_0_accuracy: 0.8771  |  0:01:35s\n",
            "epoch 65 | loss: 0.27623 | val_0_roc_auc: 0.92408 | val_0_accuracy: 0.84452 |  0:01:37s\n",
            "epoch 66 | loss: 0.27768 | val_0_roc_auc: 0.92843 | val_0_accuracy: 0.85587 |  0:01:38s\n",
            "epoch 67 | loss: 0.2622  | val_0_roc_auc: 0.93477 | val_0_accuracy: 0.85908 |  0:01:40s\n",
            "epoch 68 | loss: 0.24352 | val_0_roc_auc: 0.94431 | val_0_accuracy: 0.87216 |  0:01:41s\n",
            "epoch 69 | loss: 0.22863 | val_0_roc_auc: 0.94649 | val_0_accuracy: 0.88031 |  0:01:42s\n",
            "epoch 70 | loss: 0.26084 | val_0_roc_auc: 0.93311 | val_0_accuracy: 0.85908 |  0:01:44s\n",
            "epoch 71 | loss: 0.27072 | val_0_roc_auc: 0.9331  | val_0_accuracy: 0.85859 |  0:01:45s\n",
            "epoch 72 | loss: 0.256   | val_0_roc_auc: 0.93628 | val_0_accuracy: 0.86599 |  0:01:47s\n",
            "epoch 73 | loss: 0.2343  | val_0_roc_auc: 0.94621 | val_0_accuracy: 0.88055 |  0:01:48s\n",
            "epoch 74 | loss: 0.22185 | val_0_roc_auc: 0.94873 | val_0_accuracy: 0.88401 |  0:01:50s\n",
            "epoch 75 | loss: 0.26155 | val_0_roc_auc: 0.93462 | val_0_accuracy: 0.86081 |  0:01:51s\n",
            "epoch 76 | loss: 0.26069 | val_0_roc_auc: 0.938   | val_0_accuracy: 0.86698 |  0:01:53s\n",
            "epoch 77 | loss: 0.2429  | val_0_roc_auc: 0.9421  | val_0_accuracy: 0.87019 |  0:01:54s\n",
            "epoch 78 | loss: 0.22266 | val_0_roc_auc: 0.94736 | val_0_accuracy: 0.88179 |  0:01:56s\n",
            "epoch 79 | loss: 0.20692 | val_0_roc_auc: 0.94847 | val_0_accuracy: 0.88574 |  0:01:57s\n",
            "epoch 80 | loss: 0.24627 | val_0_roc_auc: 0.93321 | val_0_accuracy: 0.85884 |  0:01:58s\n",
            "epoch 81 | loss: 0.26598 | val_0_roc_auc: 0.93736 | val_0_accuracy: 0.86476 |  0:02:00s\n",
            "epoch 82 | loss: 0.24978 | val_0_roc_auc: 0.94532 | val_0_accuracy: 0.88203 |  0:02:01s\n",
            "epoch 83 | loss: 0.21839 | val_0_roc_auc: 0.95004 | val_0_accuracy: 0.885   |  0:02:03s\n",
            "epoch 84 | loss: 0.20564 | val_0_roc_auc: 0.9517  | val_0_accuracy: 0.885   |  0:02:04s\n",
            "epoch 85 | loss: 0.24327 | val_0_roc_auc: 0.94048 | val_0_accuracy: 0.86797 |  0:02:06s\n",
            "epoch 86 | loss: 0.24578 | val_0_roc_auc: 0.94421 | val_0_accuracy: 0.86821 |  0:02:07s\n",
            "epoch 87 | loss: 0.23311 | val_0_roc_auc: 0.94264 | val_0_accuracy: 0.8729  |  0:02:09s\n",
            "epoch 88 | loss: 0.21252 | val_0_roc_auc: 0.95042 | val_0_accuracy: 0.88475 |  0:02:10s\n",
            "epoch 89 | loss: 0.19392 | val_0_roc_auc: 0.95378 | val_0_accuracy: 0.89265 |  0:02:11s\n",
            "epoch 90 | loss: 0.24059 | val_0_roc_auc: 0.93228 | val_0_accuracy: 0.85785 |  0:02:13s\n",
            "epoch 91 | loss: 0.2536  | val_0_roc_auc: 0.94062 | val_0_accuracy: 0.87241 |  0:02:14s\n",
            "epoch 92 | loss: 0.22924 | val_0_roc_auc: 0.94544 | val_0_accuracy: 0.87685 |  0:02:16s\n",
            "epoch 93 | loss: 0.21156 | val_0_roc_auc: 0.95093 | val_0_accuracy: 0.88894 |  0:02:17s\n",
            "epoch 94 | loss: 0.19658 | val_0_roc_auc: 0.95348 | val_0_accuracy: 0.89215 |  0:02:19s\n",
            "epoch 95 | loss: 0.23264 | val_0_roc_auc: 0.94157 | val_0_accuracy: 0.87315 |  0:02:20s\n",
            "epoch 96 | loss: 0.24711 | val_0_roc_auc: 0.94294 | val_0_accuracy: 0.87142 |  0:02:22s\n",
            "epoch 97 | loss: 0.22883 | val_0_roc_auc: 0.94916 | val_0_accuracy: 0.88425 |  0:02:24s\n",
            "epoch 98 | loss: 0.20132 | val_0_roc_auc: 0.95212 | val_0_accuracy: 0.8882  |  0:02:25s\n",
            "epoch 99 | loss: 0.19097 | val_0_roc_auc: 0.95426 | val_0_accuracy: 0.89437 |  0:02:27s\n",
            "epoch 100| loss: 0.23096 | val_0_roc_auc: 0.93723 | val_0_accuracy: 0.86649 |  0:02:28s\n",
            "epoch 101| loss: 0.2425  | val_0_roc_auc: 0.94235 | val_0_accuracy: 0.8734  |  0:02:30s\n",
            "epoch 102| loss: 0.21884 | val_0_roc_auc: 0.95163 | val_0_accuracy: 0.88771 |  0:02:31s\n",
            "epoch 103| loss: 0.19942 | val_0_roc_auc: 0.95274 | val_0_accuracy: 0.8887  |  0:02:33s\n",
            "epoch 104| loss: 0.18264 | val_0_roc_auc: 0.95651 | val_0_accuracy: 0.89808 |  0:02:34s\n",
            "epoch 105| loss: 0.22591 | val_0_roc_auc: 0.94277 | val_0_accuracy: 0.87389 |  0:02:36s\n",
            "epoch 106| loss: 0.23753 | val_0_roc_auc: 0.94417 | val_0_accuracy: 0.8729  |  0:02:37s\n",
            "epoch 107| loss: 0.21959 | val_0_roc_auc: 0.94972 | val_0_accuracy: 0.88549 |  0:02:38s\n",
            "epoch 108| loss: 0.20004 | val_0_roc_auc: 0.95469 | val_0_accuracy: 0.89191 |  0:02:40s\n",
            "epoch 109| loss: 0.18291 | val_0_roc_auc: 0.9567  | val_0_accuracy: 0.8924  |  0:02:41s\n",
            "epoch 110| loss: 0.21862 | val_0_roc_auc: 0.94388 | val_0_accuracy: 0.8729  |  0:02:43s\n",
            "epoch 111| loss: 0.23383 | val_0_roc_auc: 0.94281 | val_0_accuracy: 0.87586 |  0:02:44s\n",
            "epoch 112| loss: 0.21136 | val_0_roc_auc: 0.9504  | val_0_accuracy: 0.8845  |  0:02:46s\n",
            "epoch 113| loss: 0.18966 | val_0_roc_auc: 0.95705 | val_0_accuracy: 0.89462 |  0:02:47s\n",
            "epoch 114| loss: 0.17542 | val_0_roc_auc: 0.95876 | val_0_accuracy: 0.89956 |  0:02:49s\n",
            "epoch 115| loss: 0.21933 | val_0_roc_auc: 0.94306 | val_0_accuracy: 0.87266 |  0:02:50s\n",
            "epoch 116| loss: 0.2263  | val_0_roc_auc: 0.94864 | val_0_accuracy: 0.88253 |  0:02:51s\n",
            "epoch 117| loss: 0.20899 | val_0_roc_auc: 0.95276 | val_0_accuracy: 0.89092 |  0:02:53s\n",
            "epoch 118| loss: 0.18774 | val_0_roc_auc: 0.95553 | val_0_accuracy: 0.89092 |  0:02:54s\n",
            "epoch 119| loss: 0.16823 | val_0_roc_auc: 0.95817 | val_0_accuracy: 0.89808 |  0:02:56s\n",
            "epoch 120| loss: 0.20885 | val_0_roc_auc: 0.94103 | val_0_accuracy: 0.87438 |  0:02:57s\n",
            "epoch 121| loss: 0.21477 | val_0_roc_auc: 0.94896 | val_0_accuracy: 0.88203 |  0:02:59s\n",
            "epoch 122| loss: 0.20599 | val_0_roc_auc: 0.94982 | val_0_accuracy: 0.88796 |  0:03:00s\n",
            "epoch 123| loss: 0.18253 | val_0_roc_auc: 0.95596 | val_0_accuracy: 0.89116 |  0:03:01s\n",
            "epoch 124| loss: 0.16689 | val_0_roc_auc: 0.95985 | val_0_accuracy: 0.90153 |  0:03:03s\n",
            "epoch 125| loss: 0.20766 | val_0_roc_auc: 0.94465 | val_0_accuracy: 0.87216 |  0:03:04s\n",
            "epoch 126| loss: 0.21958 | val_0_roc_auc: 0.94351 | val_0_accuracy: 0.87907 |  0:03:06s\n",
            "epoch 127| loss: 0.20304 | val_0_roc_auc: 0.95635 | val_0_accuracy: 0.8998  |  0:03:07s\n",
            "epoch 128| loss: 0.17728 | val_0_roc_auc: 0.95721 | val_0_accuracy: 0.8961  |  0:03:09s\n",
            "epoch 129| loss: 0.16212 | val_0_roc_auc: 0.95997 | val_0_accuracy: 0.90375 |  0:03:10s\n",
            "epoch 130| loss: 0.21145 | val_0_roc_auc: 0.94358 | val_0_accuracy: 0.87512 |  0:03:12s\n",
            "epoch 131| loss: 0.21386 | val_0_roc_auc: 0.94883 | val_0_accuracy: 0.88524 |  0:03:13s\n",
            "epoch 132| loss: 0.20059 | val_0_roc_auc: 0.95447 | val_0_accuracy: 0.89536 |  0:03:14s\n",
            "epoch 133| loss: 0.1736  | val_0_roc_auc: 0.9581  | val_0_accuracy: 0.90104 |  0:03:16s\n",
            "epoch 134| loss: 0.16291 | val_0_roc_auc: 0.96119 | val_0_accuracy: 0.90647 |  0:03:17s\n",
            "epoch 135| loss: 0.20563 | val_0_roc_auc: 0.94595 | val_0_accuracy: 0.88277 |  0:03:19s\n",
            "epoch 136| loss: 0.22629 | val_0_roc_auc: 0.94549 | val_0_accuracy: 0.87932 |  0:03:20s\n",
            "epoch 137| loss: 0.19695 | val_0_roc_auc: 0.95352 | val_0_accuracy: 0.88722 |  0:03:22s\n",
            "epoch 138| loss: 0.17085 | val_0_roc_auc: 0.95503 | val_0_accuracy: 0.89289 |  0:03:23s\n",
            "epoch 139| loss: 0.15976 | val_0_roc_auc: 0.9581  | val_0_accuracy: 0.89956 |  0:03:25s\n",
            "epoch 140| loss: 0.19737 | val_0_roc_auc: 0.94542 | val_0_accuracy: 0.87488 |  0:03:26s\n",
            "epoch 141| loss: 0.21407 | val_0_roc_auc: 0.94853 | val_0_accuracy: 0.88105 |  0:03:28s\n",
            "epoch 142| loss: 0.19361 | val_0_roc_auc: 0.95346 | val_0_accuracy: 0.88968 |  0:03:29s\n",
            "epoch 143| loss: 0.17734 | val_0_roc_auc: 0.95913 | val_0_accuracy: 0.89832 |  0:03:30s\n",
            "epoch 144| loss: 0.15877 | val_0_roc_auc: 0.96259 | val_0_accuracy: 0.904   |  0:03:32s\n",
            "epoch 145| loss: 0.19421 | val_0_roc_auc: 0.94598 | val_0_accuracy: 0.87932 |  0:03:33s\n",
            "epoch 146| loss: 0.20968 | val_0_roc_auc: 0.95096 | val_0_accuracy: 0.88648 |  0:03:35s\n",
            "epoch 147| loss: 0.19025 | val_0_roc_auc: 0.95722 | val_0_accuracy: 0.89931 |  0:03:36s\n",
            "epoch 148| loss: 0.16667 | val_0_roc_auc: 0.95987 | val_0_accuracy: 0.90326 |  0:03:38s\n",
            "epoch 149| loss: 0.14779 | val_0_roc_auc: 0.96217 | val_0_accuracy: 0.9077  |  0:03:39s\n",
            "epoch 150| loss: 0.19909 | val_0_roc_auc: 0.95237 | val_0_accuracy: 0.88722 |  0:03:40s\n",
            "epoch 151| loss: 0.20556 | val_0_roc_auc: 0.95105 | val_0_accuracy: 0.89215 |  0:03:42s\n",
            "epoch 152| loss: 0.18713 | val_0_roc_auc: 0.95081 | val_0_accuracy: 0.88425 |  0:03:43s\n",
            "epoch 153| loss: 0.16597 | val_0_roc_auc: 0.95851 | val_0_accuracy: 0.90104 |  0:03:45s\n",
            "epoch 154| loss: 0.14848 | val_0_roc_auc: 0.96193 | val_0_accuracy: 0.9077  |  0:03:46s\n",
            "epoch 155| loss: 0.19823 | val_0_roc_auc: 0.94833 | val_0_accuracy: 0.88031 |  0:03:48s\n",
            "epoch 156| loss: 0.19823 | val_0_roc_auc: 0.94874 | val_0_accuracy: 0.88401 |  0:03:49s\n",
            "epoch 157| loss: 0.1875  | val_0_roc_auc: 0.95431 | val_0_accuracy: 0.88722 |  0:03:51s\n",
            "epoch 158| loss: 0.16723 | val_0_roc_auc: 0.9583  | val_0_accuracy: 0.89709 |  0:03:52s\n",
            "epoch 159| loss: 0.14797 | val_0_roc_auc: 0.9605  | val_0_accuracy: 0.90424 |  0:03:53s\n",
            "epoch 160| loss: 0.19091 | val_0_roc_auc: 0.95175 | val_0_accuracy: 0.88672 |  0:03:55s\n",
            "epoch 161| loss: 0.20097 | val_0_roc_auc: 0.95408 | val_0_accuracy: 0.89388 |  0:03:57s\n",
            "epoch 162| loss: 0.1836  | val_0_roc_auc: 0.95801 | val_0_accuracy: 0.89733 |  0:03:59s\n",
            "epoch 163| loss: 0.16768 | val_0_roc_auc: 0.96336 | val_0_accuracy: 0.90721 |  0:04:00s\n",
            "epoch 164| loss: 0.1475  | val_0_roc_auc: 0.96383 | val_0_accuracy: 0.90893 |  0:04:02s\n",
            "epoch 165| loss: 0.19612 | val_0_roc_auc: 0.94542 | val_0_accuracy: 0.87858 |  0:04:03s\n",
            "epoch 166| loss: 0.19927 | val_0_roc_auc: 0.9514  | val_0_accuracy: 0.88203 |  0:04:04s\n",
            "epoch 167| loss: 0.18073 | val_0_roc_auc: 0.95799 | val_0_accuracy: 0.89783 |  0:04:06s\n",
            "epoch 168| loss: 0.15718 | val_0_roc_auc: 0.96465 | val_0_accuracy: 0.90745 |  0:04:07s\n",
            "epoch 169| loss: 0.14219 | val_0_roc_auc: 0.96526 | val_0_accuracy: 0.90795 |  0:04:09s\n",
            "epoch 170| loss: 0.19048 | val_0_roc_auc: 0.94825 | val_0_accuracy: 0.88327 |  0:04:10s\n",
            "epoch 171| loss: 0.19622 | val_0_roc_auc: 0.95361 | val_0_accuracy: 0.88598 |  0:04:12s\n",
            "epoch 172| loss: 0.1795  | val_0_roc_auc: 0.95806 | val_0_accuracy: 0.89832 |  0:04:13s\n",
            "epoch 173| loss: 0.15582 | val_0_roc_auc: 0.9618  | val_0_accuracy: 0.8998  |  0:04:15s\n",
            "epoch 174| loss: 0.14149 | val_0_roc_auc: 0.96375 | val_0_accuracy: 0.90745 |  0:04:16s\n",
            "epoch 175| loss: 0.18377 | val_0_roc_auc: 0.9478  | val_0_accuracy: 0.88154 |  0:04:17s\n",
            "epoch 176| loss: 0.19825 | val_0_roc_auc: 0.95329 | val_0_accuracy: 0.88524 |  0:04:19s\n",
            "epoch 177| loss: 0.18125 | val_0_roc_auc: 0.95619 | val_0_accuracy: 0.89388 |  0:04:20s\n",
            "epoch 178| loss: 0.15692 | val_0_roc_auc: 0.96111 | val_0_accuracy: 0.904   |  0:04:22s\n",
            "epoch 179| loss: 0.14134 | val_0_roc_auc: 0.96358 | val_0_accuracy: 0.91017 |  0:04:23s\n",
            "epoch 180| loss: 0.18032 | val_0_roc_auc: 0.95198 | val_0_accuracy: 0.88993 |  0:04:25s\n",
            "epoch 181| loss: 0.20811 | val_0_roc_auc: 0.95275 | val_0_accuracy: 0.88277 |  0:04:26s\n",
            "epoch 182| loss: 0.18771 | val_0_roc_auc: 0.95853 | val_0_accuracy: 0.89906 |  0:04:28s\n",
            "epoch 183| loss: 0.15325 | val_0_roc_auc: 0.96285 | val_0_accuracy: 0.90819 |  0:04:29s\n",
            "epoch 184| loss: 0.13801 | val_0_roc_auc: 0.96334 | val_0_accuracy: 0.90721 |  0:04:31s\n",
            "epoch 185| loss: 0.18163 | val_0_roc_auc: 0.94937 | val_0_accuracy: 0.88746 |  0:04:32s\n",
            "epoch 186| loss: 0.19352 | val_0_roc_auc: 0.95475 | val_0_accuracy: 0.89808 |  0:04:33s\n",
            "epoch 187| loss: 0.1738  | val_0_roc_auc: 0.95644 | val_0_accuracy: 0.89116 |  0:04:35s\n",
            "epoch 188| loss: 0.15475 | val_0_roc_auc: 0.96293 | val_0_accuracy: 0.90671 |  0:04:36s\n",
            "epoch 189| loss: 0.13731 | val_0_roc_auc: 0.96478 | val_0_accuracy: 0.91017 |  0:04:38s\n",
            "epoch 190| loss: 0.1805  | val_0_roc_auc: 0.94513 | val_0_accuracy: 0.88475 |  0:04:39s\n",
            "epoch 191| loss: 0.19969 | val_0_roc_auc: 0.95058 | val_0_accuracy: 0.88722 |  0:04:41s\n",
            "epoch 192| loss: 0.17766 | val_0_roc_auc: 0.95815 | val_0_accuracy: 0.89511 |  0:04:42s\n",
            "epoch 193| loss: 0.15048 | val_0_roc_auc: 0.96289 | val_0_accuracy: 0.90869 |  0:04:43s\n",
            "epoch 194| loss: 0.13398 | val_0_roc_auc: 0.96492 | val_0_accuracy: 0.90869 |  0:04:45s\n",
            "epoch 195| loss: 0.18327 | val_0_roc_auc: 0.95175 | val_0_accuracy: 0.88327 |  0:04:46s\n",
            "epoch 196| loss: 0.18852 | val_0_roc_auc: 0.95904 | val_0_accuracy: 0.89906 |  0:04:48s\n",
            "epoch 197| loss: 0.17177 | val_0_roc_auc: 0.95962 | val_0_accuracy: 0.89733 |  0:04:49s\n",
            "epoch 198| loss: 0.15075 | val_0_roc_auc: 0.96607 | val_0_accuracy: 0.9151  |  0:04:51s\n",
            "epoch 199| loss: 0.131   | val_0_roc_auc: 0.96725 | val_0_accuracy: 0.91609 |  0:04:52s\n",
            "epoch 200| loss: 0.17748 | val_0_roc_auc: 0.95506 | val_0_accuracy: 0.88919 |  0:04:54s\n",
            "epoch 201| loss: 0.19083 | val_0_roc_auc: 0.95735 | val_0_accuracy: 0.89635 |  0:04:55s\n",
            "epoch 202| loss: 0.16697 | val_0_roc_auc: 0.96034 | val_0_accuracy: 0.90696 |  0:04:56s\n",
            "epoch 203| loss: 0.15168 | val_0_roc_auc: 0.96345 | val_0_accuracy: 0.90721 |  0:04:58s\n",
            "epoch 204| loss: 0.1308  | val_0_roc_auc: 0.96573 | val_0_accuracy: 0.91412 |  0:04:59s\n",
            "epoch 205| loss: 0.17094 | val_0_roc_auc: 0.9546  | val_0_accuracy: 0.88722 |  0:05:01s\n",
            "epoch 206| loss: 0.19809 | val_0_roc_auc: 0.95531 | val_0_accuracy: 0.89462 |  0:05:02s\n",
            "epoch 207| loss: 0.16487 | val_0_roc_auc: 0.95972 | val_0_accuracy: 0.90276 |  0:05:03s\n",
            "epoch 208| loss: 0.14443 | val_0_roc_auc: 0.9633  | val_0_accuracy: 0.90918 |  0:05:05s\n",
            "epoch 209| loss: 0.12974 | val_0_roc_auc: 0.96561 | val_0_accuracy: 0.91486 |  0:05:06s\n",
            "epoch 210| loss: 0.16819 | val_0_roc_auc: 0.95638 | val_0_accuracy: 0.89561 |  0:05:08s\n",
            "epoch 211| loss: 0.18216 | val_0_roc_auc: 0.9554  | val_0_accuracy: 0.89487 |  0:05:09s\n",
            "epoch 212| loss: 0.16534 | val_0_roc_auc: 0.96058 | val_0_accuracy: 0.90301 |  0:05:11s\n",
            "epoch 213| loss: 0.14635 | val_0_roc_auc: 0.96514 | val_0_accuracy: 0.91066 |  0:05:12s\n",
            "epoch 214| loss: 0.12768 | val_0_roc_auc: 0.96788 | val_0_accuracy: 0.91757 |  0:05:13s\n",
            "epoch 215| loss: 0.1675  | val_0_roc_auc: 0.94804 | val_0_accuracy: 0.87957 |  0:05:15s\n",
            "epoch 216| loss: 0.18712 | val_0_roc_auc: 0.95477 | val_0_accuracy: 0.89339 |  0:05:16s\n",
            "epoch 217| loss: 0.16073 | val_0_roc_auc: 0.96055 | val_0_accuracy: 0.90252 |  0:05:18s\n",
            "epoch 218| loss: 0.14355 | val_0_roc_auc: 0.9638  | val_0_accuracy: 0.90869 |  0:05:19s\n",
            "epoch 219| loss: 0.12125 | val_0_roc_auc: 0.96721 | val_0_accuracy: 0.91461 |  0:05:21s\n",
            "epoch 220| loss: 0.18098 | val_0_roc_auc: 0.956   | val_0_accuracy: 0.88894 |  0:05:22s\n",
            "epoch 221| loss: 0.17834 | val_0_roc_auc: 0.95834 | val_0_accuracy: 0.89733 |  0:05:24s\n",
            "epoch 222| loss: 0.16126 | val_0_roc_auc: 0.96224 | val_0_accuracy: 0.90227 |  0:05:25s\n",
            "epoch 223| loss: 0.13426 | val_0_roc_auc: 0.96663 | val_0_accuracy: 0.91634 |  0:05:26s\n",
            "epoch 224| loss: 0.12468 | val_0_roc_auc: 0.96773 | val_0_accuracy: 0.91115 |  0:05:28s\n",
            "epoch 225| loss: 0.16536 | val_0_roc_auc: 0.95014 | val_0_accuracy: 0.88253 |  0:05:29s\n",
            "epoch 226| loss: 0.1806  | val_0_roc_auc: 0.95934 | val_0_accuracy: 0.8998  |  0:05:31s\n",
            "epoch 227| loss: 0.15905 | val_0_roc_auc: 0.96106 | val_0_accuracy: 0.90178 |  0:05:32s\n",
            "epoch 228| loss: 0.13887 | val_0_roc_auc: 0.9673  | val_0_accuracy: 0.91115 |  0:05:34s\n",
            "epoch 229| loss: 0.12213 | val_0_roc_auc: 0.96907 | val_0_accuracy: 0.91708 |  0:05:35s\n",
            "epoch 230| loss: 0.16683 | val_0_roc_auc: 0.95874 | val_0_accuracy: 0.89363 |  0:05:37s\n",
            "epoch 231| loss: 0.18155 | val_0_roc_auc: 0.9591  | val_0_accuracy: 0.8998  |  0:05:38s\n",
            "epoch 232| loss: 0.16159 | val_0_roc_auc: 0.96353 | val_0_accuracy: 0.90597 |  0:05:40s\n",
            "epoch 233| loss: 0.14255 | val_0_roc_auc: 0.96895 | val_0_accuracy: 0.91362 |  0:05:42s\n",
            "epoch 234| loss: 0.12667 | val_0_roc_auc: 0.96959 | val_0_accuracy: 0.91708 |  0:05:43s\n",
            "epoch 235| loss: 0.16298 | val_0_roc_auc: 0.95803 | val_0_accuracy: 0.89363 |  0:05:45s\n",
            "epoch 236| loss: 0.18471 | val_0_roc_auc: 0.95808 | val_0_accuracy: 0.89808 |  0:05:46s\n",
            "epoch 237| loss: 0.16181 | val_0_roc_auc: 0.96319 | val_0_accuracy: 0.90252 |  0:05:48s\n",
            "epoch 238| loss: 0.13941 | val_0_roc_auc: 0.96736 | val_0_accuracy: 0.90844 |  0:05:49s\n",
            "epoch 239| loss: 0.12129 | val_0_roc_auc: 0.96983 | val_0_accuracy: 0.91807 |  0:05:51s\n",
            "epoch 240| loss: 0.16353 | val_0_roc_auc: 0.95653 | val_0_accuracy: 0.89067 |  0:05:52s\n",
            "epoch 241| loss: 0.1734  | val_0_roc_auc: 0.95804 | val_0_accuracy: 0.89832 |  0:05:53s\n",
            "epoch 242| loss: 0.15483 | val_0_roc_auc: 0.96302 | val_0_accuracy: 0.90474 |  0:05:55s\n",
            "epoch 243| loss: 0.13753 | val_0_roc_auc: 0.96636 | val_0_accuracy: 0.91115 |  0:05:56s\n",
            "epoch 244| loss: 0.12304 | val_0_roc_auc: 0.96897 | val_0_accuracy: 0.91609 |  0:05:58s\n",
            "epoch 245| loss: 0.16171 | val_0_roc_auc: 0.95509 | val_0_accuracy: 0.89042 |  0:05:59s\n",
            "epoch 246| loss: 0.17637 | val_0_roc_auc: 0.95694 | val_0_accuracy: 0.89437 |  0:06:01s\n",
            "epoch 247| loss: 0.15555 | val_0_roc_auc: 0.96327 | val_0_accuracy: 0.90622 |  0:06:02s\n",
            "epoch 248| loss: 0.13883 | val_0_roc_auc: 0.96436 | val_0_accuracy: 0.90844 |  0:06:04s\n",
            "epoch 249| loss: 0.11808 | val_0_roc_auc: 0.96752 | val_0_accuracy: 0.91658 |  0:06:05s\n",
            "epoch 250| loss: 0.15381 | val_0_roc_auc: 0.95786 | val_0_accuracy: 0.89413 |  0:06:06s\n",
            "epoch 251| loss: 0.17922 | val_0_roc_auc: 0.9581  | val_0_accuracy: 0.8998  |  0:06:08s\n",
            "epoch 252| loss: 0.15397 | val_0_roc_auc: 0.96145 | val_0_accuracy: 0.904   |  0:06:09s\n",
            "epoch 253| loss: 0.13549 | val_0_roc_auc: 0.96601 | val_0_accuracy: 0.91584 |  0:06:11s\n",
            "epoch 254| loss: 0.12053 | val_0_roc_auc: 0.96987 | val_0_accuracy: 0.92103 |  0:06:12s\n",
            "epoch 255| loss: 0.16233 | val_0_roc_auc: 0.95428 | val_0_accuracy: 0.89289 |  0:06:14s\n",
            "epoch 256| loss: 0.17323 | val_0_roc_auc: 0.96308 | val_0_accuracy: 0.90079 |  0:06:15s\n",
            "epoch 257| loss: 0.14847 | val_0_roc_auc: 0.96571 | val_0_accuracy: 0.90523 |  0:06:17s\n",
            "epoch 258| loss: 0.13078 | val_0_roc_auc: 0.96906 | val_0_accuracy: 0.91708 |  0:06:18s\n",
            "epoch 259| loss: 0.1165  | val_0_roc_auc: 0.96967 | val_0_accuracy: 0.91732 |  0:06:19s\n",
            "epoch 260| loss: 0.15458 | val_0_roc_auc: 0.95093 | val_0_accuracy: 0.88697 |  0:06:21s\n",
            "epoch 261| loss: 0.17418 | val_0_roc_auc: 0.95682 | val_0_accuracy: 0.9003  |  0:06:22s\n",
            "epoch 262| loss: 0.15209 | val_0_roc_auc: 0.96533 | val_0_accuracy: 0.9119  |  0:06:24s\n",
            "epoch 263| loss: 0.1315  | val_0_roc_auc: 0.96875 | val_0_accuracy: 0.91658 |  0:06:25s\n",
            "epoch 264| loss: 0.11227 | val_0_roc_auc: 0.97    | val_0_accuracy: 0.91905 |  0:06:27s\n",
            "epoch 265| loss: 0.1509  | val_0_roc_auc: 0.95627 | val_0_accuracy: 0.89215 |  0:06:28s\n",
            "epoch 266| loss: 0.17077 | val_0_roc_auc: 0.95918 | val_0_accuracy: 0.89585 |  0:06:30s\n",
            "epoch 267| loss: 0.15431 | val_0_roc_auc: 0.96563 | val_0_accuracy: 0.90819 |  0:06:31s\n",
            "epoch 268| loss: 0.13155 | val_0_roc_auc: 0.96767 | val_0_accuracy: 0.91362 |  0:06:33s\n",
            "epoch 269| loss: 0.11883 | val_0_roc_auc: 0.97041 | val_0_accuracy: 0.92053 |  0:06:34s\n",
            "epoch 270| loss: 0.16148 | val_0_roc_auc: 0.95527 | val_0_accuracy: 0.89314 |  0:06:36s\n",
            "epoch 271| loss: 0.16601 | val_0_roc_auc: 0.96041 | val_0_accuracy: 0.904   |  0:06:37s\n",
            "epoch 272| loss: 0.14644 | val_0_roc_auc: 0.96426 | val_0_accuracy: 0.90474 |  0:06:38s\n",
            "epoch 273| loss: 0.12954 | val_0_roc_auc: 0.96808 | val_0_accuracy: 0.91584 |  0:06:40s\n",
            "epoch 274| loss: 0.11401 | val_0_roc_auc: 0.97038 | val_0_accuracy: 0.91732 |  0:06:41s\n",
            "epoch 275| loss: 0.15113 | val_0_roc_auc: 0.95525 | val_0_accuracy: 0.88475 |  0:06:43s\n",
            "epoch 276| loss: 0.1716  | val_0_roc_auc: 0.96166 | val_0_accuracy: 0.90005 |  0:06:44s\n",
            "epoch 277| loss: 0.1487  | val_0_roc_auc: 0.96538 | val_0_accuracy: 0.9077  |  0:06:46s\n",
            "epoch 278| loss: 0.13008 | val_0_roc_auc: 0.96981 | val_0_accuracy: 0.9151  |  0:06:47s\n",
            "epoch 279| loss: 0.11317 | val_0_roc_auc: 0.97164 | val_0_accuracy: 0.92127 |  0:06:48s\n",
            "epoch 280| loss: 0.13993 | val_0_roc_auc: 0.96473 | val_0_accuracy: 0.904   |  0:06:50s\n",
            "epoch 281| loss: 0.16183 | val_0_roc_auc: 0.96234 | val_0_accuracy: 0.90202 |  0:06:51s\n",
            "epoch 282| loss: 0.1427  | val_0_roc_auc: 0.96853 | val_0_accuracy: 0.91708 |  0:06:53s\n",
            "epoch 283| loss: 0.12414 | val_0_roc_auc: 0.97087 | val_0_accuracy: 0.91831 |  0:06:54s\n",
            "epoch 284| loss: 0.10802 | val_0_roc_auc: 0.9727  | val_0_accuracy: 0.92251 |  0:06:56s\n",
            "epoch 285| loss: 0.15087 | val_0_roc_auc: 0.96219 | val_0_accuracy: 0.89857 |  0:06:57s\n",
            "epoch 286| loss: 0.16221 | val_0_roc_auc: 0.96499 | val_0_accuracy: 0.90375 |  0:06:59s\n",
            "epoch 287| loss: 0.14375 | val_0_roc_auc: 0.96353 | val_0_accuracy: 0.90795 |  0:07:00s\n",
            "epoch 288| loss: 0.12734 | val_0_roc_auc: 0.97115 | val_0_accuracy: 0.91807 |  0:07:02s\n",
            "epoch 289| loss: 0.11195 | val_0_roc_auc: 0.97139 | val_0_accuracy: 0.91757 |  0:07:03s\n",
            "epoch 290| loss: 0.14511 | val_0_roc_auc: 0.95957 | val_0_accuracy: 0.8998  |  0:07:04s\n",
            "epoch 291| loss: 0.15733 | val_0_roc_auc: 0.96339 | val_0_accuracy: 0.90128 |  0:07:06s\n",
            "epoch 292| loss: 0.14474 | val_0_roc_auc: 0.96941 | val_0_accuracy: 0.91214 |  0:07:07s\n",
            "epoch 293| loss: 0.12556 | val_0_roc_auc: 0.97299 | val_0_accuracy: 0.923   |  0:07:09s\n",
            "epoch 294| loss: 0.10899 | val_0_roc_auc: 0.97366 | val_0_accuracy: 0.92498 |  0:07:10s\n",
            "epoch 295| loss: 0.14541 | val_0_roc_auc: 0.95913 | val_0_accuracy: 0.89363 |  0:07:12s\n",
            "epoch 296| loss: 0.16575 | val_0_roc_auc: 0.96447 | val_0_accuracy: 0.90745 |  0:07:13s\n",
            "epoch 297| loss: 0.14347 | val_0_roc_auc: 0.96744 | val_0_accuracy: 0.9151  |  0:07:15s\n",
            "epoch 298| loss: 0.12222 | val_0_roc_auc: 0.97144 | val_0_accuracy: 0.91955 |  0:07:16s\n",
            "epoch 299| loss: 0.10947 | val_0_roc_auc: 0.97287 | val_0_accuracy: 0.92226 |  0:07:18s\n",
            "epoch 300| loss: 0.14596 | val_0_roc_auc: 0.96309 | val_0_accuracy: 0.90178 |  0:07:20s\n",
            "epoch 301| loss: 0.16125 | val_0_roc_auc: 0.96397 | val_0_accuracy: 0.90745 |  0:07:21s\n",
            "epoch 302| loss: 0.14    | val_0_roc_auc: 0.96584 | val_0_accuracy: 0.91436 |  0:07:23s\n",
            "epoch 303| loss: 0.1207  | val_0_roc_auc: 0.97103 | val_0_accuracy: 0.92029 |  0:07:24s\n",
            "epoch 304| loss: 0.10608 | val_0_roc_auc: 0.97151 | val_0_accuracy: 0.92078 |  0:07:26s\n",
            "epoch 305| loss: 0.15054 | val_0_roc_auc: 0.96027 | val_0_accuracy: 0.89709 |  0:07:27s\n",
            "epoch 306| loss: 0.16119 | val_0_roc_auc: 0.96492 | val_0_accuracy: 0.90721 |  0:07:29s\n",
            "epoch 307| loss: 0.1419  | val_0_roc_auc: 0.96632 | val_0_accuracy: 0.91313 |  0:07:30s\n",
            "epoch 308| loss: 0.11692 | val_0_roc_auc: 0.96816 | val_0_accuracy: 0.91634 |  0:07:31s\n",
            "epoch 309| loss: 0.10803 | val_0_roc_auc: 0.96979 | val_0_accuracy: 0.91905 |  0:07:33s\n",
            "epoch 310| loss: 0.14744 | val_0_roc_auc: 0.95859 | val_0_accuracy: 0.8961  |  0:07:34s\n",
            "epoch 311| loss: 0.17006 | val_0_roc_auc: 0.96302 | val_0_accuracy: 0.90326 |  0:07:36s\n",
            "epoch 312| loss: 0.14579 | val_0_roc_auc: 0.96652 | val_0_accuracy: 0.91264 |  0:07:37s\n",
            "epoch 313| loss: 0.12354 | val_0_roc_auc: 0.97205 | val_0_accuracy: 0.92325 |  0:07:39s\n",
            "epoch 314| loss: 0.10474 | val_0_roc_auc: 0.97228 | val_0_accuracy: 0.92448 |  0:07:40s\n",
            "epoch 315| loss: 0.13765 | val_0_roc_auc: 0.96029 | val_0_accuracy: 0.90523 |  0:07:42s\n",
            "epoch 316| loss: 0.152   | val_0_roc_auc: 0.96235 | val_0_accuracy: 0.90128 |  0:07:43s\n",
            "epoch 317| loss: 0.13885 | val_0_roc_auc: 0.96791 | val_0_accuracy: 0.91634 |  0:07:44s\n",
            "epoch 318| loss: 0.11811 | val_0_roc_auc: 0.96959 | val_0_accuracy: 0.91609 |  0:07:46s\n",
            "epoch 319| loss: 0.10496 | val_0_roc_auc: 0.97185 | val_0_accuracy: 0.92399 |  0:07:47s\n",
            "epoch 320| loss: 0.13527 | val_0_roc_auc: 0.96266 | val_0_accuracy: 0.90795 |  0:07:49s\n",
            "epoch 321| loss: 0.15692 | val_0_roc_auc: 0.95707 | val_0_accuracy: 0.89931 |  0:07:50s\n",
            "epoch 322| loss: 0.14379 | val_0_roc_auc: 0.96091 | val_0_accuracy: 0.90202 |  0:07:52s\n",
            "epoch 323| loss: 0.12134 | val_0_roc_auc: 0.96734 | val_0_accuracy: 0.91807 |  0:07:53s\n",
            "epoch 324| loss: 0.10996 | val_0_roc_auc: 0.96965 | val_0_accuracy: 0.91955 |  0:07:55s\n",
            "epoch 325| loss: 0.13974 | val_0_roc_auc: 0.96247 | val_0_accuracy: 0.90375 |  0:07:56s\n",
            "epoch 326| loss: 0.16917 | val_0_roc_auc: 0.9635  | val_0_accuracy: 0.90202 |  0:07:57s\n",
            "epoch 327| loss: 0.13955 | val_0_roc_auc: 0.96759 | val_0_accuracy: 0.91535 |  0:07:59s\n",
            "epoch 328| loss: 0.11449 | val_0_roc_auc: 0.9712  | val_0_accuracy: 0.91955 |  0:08:00s\n",
            "epoch 329| loss: 0.09788 | val_0_roc_auc: 0.97258 | val_0_accuracy: 0.92794 |  0:08:02s\n",
            "epoch 330| loss: 0.13359 | val_0_roc_auc: 0.96059 | val_0_accuracy: 0.90178 |  0:08:03s\n",
            "epoch 331| loss: 0.16261 | val_0_roc_auc: 0.96347 | val_0_accuracy: 0.90499 |  0:08:05s\n",
            "epoch 332| loss: 0.13764 | val_0_roc_auc: 0.96751 | val_0_accuracy: 0.91313 |  0:08:06s\n",
            "epoch 333| loss: 0.11666 | val_0_roc_auc: 0.9714  | val_0_accuracy: 0.923   |  0:08:08s\n",
            "epoch 334| loss: 0.10103 | val_0_roc_auc: 0.97332 | val_0_accuracy: 0.9272  |  0:08:09s\n",
            "epoch 335| loss: 0.13686 | val_0_roc_auc: 0.9601  | val_0_accuracy: 0.90079 |  0:08:11s\n",
            "epoch 336| loss: 0.15547 | val_0_roc_auc: 0.96544 | val_0_accuracy: 0.90375 |  0:08:12s\n",
            "epoch 337| loss: 0.13099 | val_0_roc_auc: 0.96988 | val_0_accuracy: 0.91461 |  0:08:13s\n",
            "epoch 338| loss: 0.11348 | val_0_roc_auc: 0.97104 | val_0_accuracy: 0.91807 |  0:08:15s\n",
            "epoch 339| loss: 0.10217 | val_0_roc_auc: 0.97327 | val_0_accuracy: 0.92423 |  0:08:16s\n",
            "epoch 340| loss: 0.13421 | val_0_roc_auc: 0.96341 | val_0_accuracy: 0.90795 |  0:08:18s\n",
            "epoch 341| loss: 0.1492  | val_0_roc_auc: 0.96477 | val_0_accuracy: 0.904   |  0:08:19s\n",
            "epoch 342| loss: 0.13428 | val_0_roc_auc: 0.96788 | val_0_accuracy: 0.91239 |  0:08:21s\n",
            "epoch 343| loss: 0.11112 | val_0_roc_auc: 0.97142 | val_0_accuracy: 0.91584 |  0:08:22s\n",
            "epoch 344| loss: 0.10451 | val_0_roc_auc: 0.97398 | val_0_accuracy: 0.92646 |  0:08:24s\n",
            "epoch 345| loss: 0.12871 | val_0_roc_auc: 0.95745 | val_0_accuracy: 0.89635 |  0:08:25s\n",
            "epoch 346| loss: 0.15651 | val_0_roc_auc: 0.9624  | val_0_accuracy: 0.90844 |  0:08:26s\n",
            "epoch 347| loss: 0.12812 | val_0_roc_auc: 0.96928 | val_0_accuracy: 0.92004 |  0:08:28s\n",
            "epoch 348| loss: 0.1076  | val_0_roc_auc: 0.97116 | val_0_accuracy: 0.92399 |  0:08:29s\n",
            "epoch 349| loss: 0.10215 | val_0_roc_auc: 0.9736  | val_0_accuracy: 0.9309  |  0:08:31s\n",
            "epoch 350| loss: 0.12977 | val_0_roc_auc: 0.9574  | val_0_accuracy: 0.89561 |  0:08:32s\n",
            "epoch 351| loss: 0.156   | val_0_roc_auc: 0.96591 | val_0_accuracy: 0.9114  |  0:08:34s\n",
            "epoch 352| loss: 0.12733 | val_0_roc_auc: 0.97047 | val_0_accuracy: 0.92399 |  0:08:35s\n",
            "epoch 353| loss: 0.10763 | val_0_roc_auc: 0.97119 | val_0_accuracy: 0.92374 |  0:08:37s\n",
            "epoch 354| loss: 0.10297 | val_0_roc_auc: 0.97401 | val_0_accuracy: 0.92547 |  0:08:38s\n",
            "epoch 355| loss: 0.134   | val_0_roc_auc: 0.96207 | val_0_accuracy: 0.9035  |  0:08:39s\n",
            "epoch 356| loss: 0.15499 | val_0_roc_auc: 0.96386 | val_0_accuracy: 0.90893 |  0:08:41s\n",
            "epoch 357| loss: 0.13264 | val_0_roc_auc: 0.96701 | val_0_accuracy: 0.90967 |  0:08:42s\n",
            "epoch 358| loss: 0.11644 | val_0_roc_auc: 0.97123 | val_0_accuracy: 0.91881 |  0:08:44s\n",
            "epoch 359| loss: 0.0969  | val_0_roc_auc: 0.97324 | val_0_accuracy: 0.9267  |  0:08:45s\n",
            "epoch 360| loss: 0.13601 | val_0_roc_auc: 0.95288 | val_0_accuracy: 0.89215 |  0:08:47s\n",
            "epoch 361| loss: 0.15738 | val_0_roc_auc: 0.96192 | val_0_accuracy: 0.90943 |  0:08:48s\n",
            "epoch 362| loss: 0.13921 | val_0_roc_auc: 0.96647 | val_0_accuracy: 0.90943 |  0:08:50s\n",
            "epoch 363| loss: 0.1089  | val_0_roc_auc: 0.97111 | val_0_accuracy: 0.92621 |  0:08:51s\n",
            "epoch 364| loss: 0.09777 | val_0_roc_auc: 0.97054 | val_0_accuracy: 0.9272  |  0:08:52s\n",
            "epoch 365| loss: 0.13228 | val_0_roc_auc: 0.96402 | val_0_accuracy: 0.91239 |  0:08:54s\n",
            "epoch 366| loss: 0.14405 | val_0_roc_auc: 0.96274 | val_0_accuracy: 0.90499 |  0:08:55s\n",
            "epoch 367| loss: 0.13144 | val_0_roc_auc: 0.96361 | val_0_accuracy: 0.90721 |  0:08:57s\n",
            "epoch 368| loss: 0.10818 | val_0_roc_auc: 0.97101 | val_0_accuracy: 0.92053 |  0:08:59s\n",
            "epoch 369| loss: 0.09722 | val_0_roc_auc: 0.97248 | val_0_accuracy: 0.92695 |  0:09:01s\n",
            "epoch 370| loss: 0.1303  | val_0_roc_auc: 0.95965 | val_0_accuracy: 0.90104 |  0:09:02s\n",
            "epoch 371| loss: 0.15193 | val_0_roc_auc: 0.96303 | val_0_accuracy: 0.91017 |  0:09:04s\n",
            "epoch 372| loss: 0.13301 | val_0_roc_auc: 0.96698 | val_0_accuracy: 0.9151  |  0:09:05s\n",
            "epoch 373| loss: 0.11172 | val_0_roc_auc: 0.96956 | val_0_accuracy: 0.92103 |  0:09:06s\n",
            "epoch 374| loss: 0.09788 | val_0_roc_auc: 0.97157 | val_0_accuracy: 0.92522 |  0:09:08s\n",
            "epoch 375| loss: 0.12857 | val_0_roc_auc: 0.96481 | val_0_accuracy: 0.91214 |  0:09:09s\n",
            "epoch 376| loss: 0.13548 | val_0_roc_auc: 0.96675 | val_0_accuracy: 0.90967 |  0:09:11s\n",
            "epoch 377| loss: 0.12306 | val_0_roc_auc: 0.96981 | val_0_accuracy: 0.92103 |  0:09:12s\n",
            "epoch 378| loss: 0.10583 | val_0_roc_auc: 0.97173 | val_0_accuracy: 0.9272  |  0:09:14s\n",
            "epoch 379| loss: 0.09488 | val_0_roc_auc: 0.97352 | val_0_accuracy: 0.92892 |  0:09:15s\n",
            "epoch 380| loss: 0.12902 | val_0_roc_auc: 0.96129 | val_0_accuracy: 0.90326 |  0:09:16s\n",
            "epoch 381| loss: 0.14594 | val_0_roc_auc: 0.96116 | val_0_accuracy: 0.90202 |  0:09:18s\n",
            "epoch 382| loss: 0.12801 | val_0_roc_auc: 0.96876 | val_0_accuracy: 0.92029 |  0:09:19s\n",
            "epoch 383| loss: 0.11057 | val_0_roc_auc: 0.97075 | val_0_accuracy: 0.92473 |  0:09:21s\n",
            "epoch 384| loss: 0.09139 | val_0_roc_auc: 0.97155 | val_0_accuracy: 0.92473 |  0:09:22s\n",
            "epoch 385| loss: 0.12577 | val_0_roc_auc: 0.96373 | val_0_accuracy: 0.90721 |  0:09:24s\n",
            "epoch 386| loss: 0.14439 | val_0_roc_auc: 0.96565 | val_0_accuracy: 0.91461 |  0:09:25s\n",
            "epoch 387| loss: 0.12262 | val_0_roc_auc: 0.97036 | val_0_accuracy: 0.91732 |  0:09:27s\n",
            "epoch 388| loss: 0.10511 | val_0_roc_auc: 0.97108 | val_0_accuracy: 0.9272  |  0:09:28s\n",
            "epoch 389| loss: 0.09192 | val_0_roc_auc: 0.97362 | val_0_accuracy: 0.92917 |  0:09:30s\n",
            "epoch 390| loss: 0.1288  | val_0_roc_auc: 0.96215 | val_0_accuracy: 0.90424 |  0:09:31s\n",
            "epoch 391| loss: 0.14461 | val_0_roc_auc: 0.96375 | val_0_accuracy: 0.90819 |  0:09:33s\n",
            "epoch 392| loss: 0.12651 | val_0_roc_auc: 0.96895 | val_0_accuracy: 0.92004 |  0:09:34s\n",
            "epoch 393| loss: 0.10411 | val_0_roc_auc: 0.9726  | val_0_accuracy: 0.92226 |  0:09:35s\n",
            "epoch 394| loss: 0.09378 | val_0_roc_auc: 0.97273 | val_0_accuracy: 0.92892 |  0:09:37s\n",
            "epoch 395| loss: 0.12367 | val_0_roc_auc: 0.96151 | val_0_accuracy: 0.91115 |  0:09:38s\n",
            "epoch 396| loss: 0.14929 | val_0_roc_auc: 0.96507 | val_0_accuracy: 0.91066 |  0:09:40s\n",
            "epoch 397| loss: 0.12447 | val_0_roc_auc: 0.96942 | val_0_accuracy: 0.91905 |  0:09:41s\n",
            "epoch 398| loss: 0.10087 | val_0_roc_auc: 0.97197 | val_0_accuracy: 0.92053 |  0:09:43s\n",
            "epoch 399| loss: 0.09264 | val_0_roc_auc: 0.97346 | val_0_accuracy: 0.92498 |  0:09:44s\n",
            "\n",
            "Early stopping occurred at epoch 399 with best_epoch = 349 and best_val_0_accuracy = 0.9309\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KFold 2 of 5\n",
            "epoch 0  | loss: 0.79475 | val_0_roc_auc: 0.71712 | val_0_accuracy: 0.64265 |  0:00:01s\n",
            "epoch 1  | loss: 0.62634 | val_0_roc_auc: 0.76321 | val_0_accuracy: 0.70237 |  0:00:02s\n",
            "epoch 2  | loss: 0.57563 | val_0_roc_auc: 0.78932 | val_0_accuracy: 0.71175 |  0:00:04s\n",
            "epoch 3  | loss: 0.55027 | val_0_roc_auc: 0.80278 | val_0_accuracy: 0.72038 |  0:00:05s\n",
            "epoch 4  | loss: 0.5352  | val_0_roc_auc: 0.80501 | val_0_accuracy: 0.72359 |  0:00:07s\n",
            "epoch 5  | loss: 0.52391 | val_0_roc_auc: 0.82099 | val_0_accuracy: 0.73791 |  0:00:08s\n",
            "epoch 6  | loss: 0.50681 | val_0_roc_auc: 0.82947 | val_0_accuracy: 0.7463  |  0:00:10s\n",
            "epoch 7  | loss: 0.49572 | val_0_roc_auc: 0.84037 | val_0_accuracy: 0.75592 |  0:00:11s\n",
            "epoch 8  | loss: 0.47837 | val_0_roc_auc: 0.84488 | val_0_accuracy: 0.76111 |  0:00:13s\n",
            "epoch 9  | loss: 0.47831 | val_0_roc_auc: 0.84708 | val_0_accuracy: 0.76185 |  0:00:14s\n",
            "epoch 10 | loss: 0.47128 | val_0_roc_auc: 0.85111 | val_0_accuracy: 0.76382 |  0:00:16s\n",
            "epoch 11 | loss: 0.46397 | val_0_roc_auc: 0.86147 | val_0_accuracy: 0.77246 |  0:00:17s\n",
            "epoch 12 | loss: 0.45275 | val_0_roc_auc: 0.86451 | val_0_accuracy: 0.77591 |  0:00:19s\n",
            "epoch 13 | loss: 0.44775 | val_0_roc_auc: 0.86785 | val_0_accuracy: 0.77863 |  0:00:20s\n",
            "epoch 14 | loss: 0.44275 | val_0_roc_auc: 0.86871 | val_0_accuracy: 0.77887 |  0:00:21s\n",
            "epoch 15 | loss: 0.44509 | val_0_roc_auc: 0.86747 | val_0_accuracy: 0.7885  |  0:00:23s\n",
            "epoch 16 | loss: 0.43719 | val_0_roc_auc: 0.87362 | val_0_accuracy: 0.78578 |  0:00:24s\n",
            "epoch 17 | loss: 0.42865 | val_0_roc_auc: 0.87732 | val_0_accuracy: 0.79393 |  0:00:26s\n",
            "epoch 18 | loss: 0.42265 | val_0_roc_auc: 0.88053 | val_0_accuracy: 0.79689 |  0:00:27s\n",
            "epoch 19 | loss: 0.41466 | val_0_roc_auc: 0.88126 | val_0_accuracy: 0.79911 |  0:00:29s\n",
            "epoch 20 | loss: 0.41673 | val_0_roc_auc: 0.88073 | val_0_accuracy: 0.79442 |  0:00:30s\n",
            "epoch 21 | loss: 0.41479 | val_0_roc_auc: 0.88383 | val_0_accuracy: 0.80035 |  0:00:32s\n",
            "epoch 22 | loss: 0.40374 | val_0_roc_auc: 0.88737 | val_0_accuracy: 0.80183 |  0:00:33s\n",
            "epoch 23 | loss: 0.39185 | val_0_roc_auc: 0.89166 | val_0_accuracy: 0.80676 |  0:00:35s\n",
            "epoch 24 | loss: 0.38894 | val_0_roc_auc: 0.89302 | val_0_accuracy: 0.80923 |  0:00:36s\n",
            "epoch 25 | loss: 0.39953 | val_0_roc_auc: 0.89323 | val_0_accuracy: 0.80726 |  0:00:38s\n",
            "epoch 26 | loss: 0.38706 | val_0_roc_auc: 0.89083 | val_0_accuracy: 0.80849 |  0:00:39s\n",
            "epoch 27 | loss: 0.375   | val_0_roc_auc: 0.89834 | val_0_accuracy: 0.80948 |  0:00:41s\n",
            "epoch 28 | loss: 0.37202 | val_0_roc_auc: 0.90216 | val_0_accuracy: 0.81318 |  0:00:42s\n",
            "epoch 29 | loss: 0.36255 | val_0_roc_auc: 0.90528 | val_0_accuracy: 0.82009 |  0:00:44s\n",
            "epoch 30 | loss: 0.37212 | val_0_roc_auc: 0.90009 | val_0_accuracy: 0.81861 |  0:00:45s\n",
            "epoch 31 | loss: 0.36983 | val_0_roc_auc: 0.90392 | val_0_accuracy: 0.8228  |  0:00:46s\n",
            "epoch 32 | loss: 0.36309 | val_0_roc_auc: 0.90795 | val_0_accuracy: 0.8233  |  0:00:48s\n",
            "epoch 33 | loss: 0.34542 | val_0_roc_auc: 0.91185 | val_0_accuracy: 0.83218 |  0:00:50s\n",
            "epoch 34 | loss: 0.33573 | val_0_roc_auc: 0.9147  | val_0_accuracy: 0.83243 |  0:00:52s\n",
            "epoch 35 | loss: 0.35123 | val_0_roc_auc: 0.90737 | val_0_accuracy: 0.82502 |  0:00:53s\n",
            "epoch 36 | loss: 0.3494  | val_0_roc_auc: 0.91342 | val_0_accuracy: 0.83095 |  0:00:55s\n",
            "epoch 37 | loss: 0.33382 | val_0_roc_auc: 0.91833 | val_0_accuracy: 0.83564 |  0:00:56s\n",
            "epoch 38 | loss: 0.324   | val_0_roc_auc: 0.92173 | val_0_accuracy: 0.84699 |  0:00:58s\n",
            "epoch 39 | loss: 0.31761 | val_0_roc_auc: 0.92277 | val_0_accuracy: 0.84576 |  0:00:59s\n",
            "epoch 40 | loss: 0.33354 | val_0_roc_auc: 0.91688 | val_0_accuracy: 0.83613 |  0:01:00s\n",
            "epoch 41 | loss: 0.3315  | val_0_roc_auc: 0.91678 | val_0_accuracy: 0.8381  |  0:01:02s\n",
            "epoch 42 | loss: 0.32223 | val_0_roc_auc: 0.92153 | val_0_accuracy: 0.84205 |  0:01:03s\n",
            "epoch 43 | loss: 0.3065  | val_0_roc_auc: 0.9282  | val_0_accuracy: 0.85563 |  0:01:05s\n",
            "epoch 44 | loss: 0.29617 | val_0_roc_auc: 0.92996 | val_0_accuracy: 0.85933 |  0:01:06s\n",
            "epoch 45 | loss: 0.31305 | val_0_roc_auc: 0.91878 | val_0_accuracy: 0.8381  |  0:01:08s\n",
            "epoch 46 | loss: 0.3198  | val_0_roc_auc: 0.92211 | val_0_accuracy: 0.8497  |  0:01:09s\n",
            "epoch 47 | loss: 0.31016 | val_0_roc_auc: 0.92774 | val_0_accuracy: 0.84847 |  0:01:11s\n",
            "epoch 48 | loss: 0.28972 | val_0_roc_auc: 0.93222 | val_0_accuracy: 0.85711 |  0:01:12s\n",
            "epoch 49 | loss: 0.27311 | val_0_roc_auc: 0.9362  | val_0_accuracy: 0.8655  |  0:01:14s\n",
            "epoch 50 | loss: 0.30434 | val_0_roc_auc: 0.92255 | val_0_accuracy: 0.84378 |  0:01:15s\n",
            "epoch 51 | loss: 0.3022  | val_0_roc_auc: 0.92456 | val_0_accuracy: 0.84427 |  0:01:17s\n",
            "epoch 52 | loss: 0.29066 | val_0_roc_auc: 0.92953 | val_0_accuracy: 0.85538 |  0:01:18s\n",
            "epoch 53 | loss: 0.27031 | val_0_roc_auc: 0.93701 | val_0_accuracy: 0.86871 |  0:01:19s\n",
            "epoch 54 | loss: 0.26126 | val_0_roc_auc: 0.93878 | val_0_accuracy: 0.86969 |  0:01:21s\n",
            "epoch 55 | loss: 0.29201 | val_0_roc_auc: 0.92603 | val_0_accuracy: 0.84699 |  0:01:22s\n",
            "epoch 56 | loss: 0.29183 | val_0_roc_auc: 0.93325 | val_0_accuracy: 0.85735 |  0:01:24s\n",
            "epoch 57 | loss: 0.27691 | val_0_roc_auc: 0.93342 | val_0_accuracy: 0.86007 |  0:01:25s\n",
            "epoch 58 | loss: 0.2639  | val_0_roc_auc: 0.93931 | val_0_accuracy: 0.8734  |  0:01:27s\n",
            "epoch 59 | loss: 0.24665 | val_0_roc_auc: 0.94194 | val_0_accuracy: 0.87043 |  0:01:28s\n",
            "epoch 60 | loss: 0.27988 | val_0_roc_auc: 0.9321  | val_0_accuracy: 0.85834 |  0:01:30s\n",
            "epoch 61 | loss: 0.28202 | val_0_roc_auc: 0.93406 | val_0_accuracy: 0.85859 |  0:01:31s\n",
            "epoch 62 | loss: 0.264   | val_0_roc_auc: 0.9334  | val_0_accuracy: 0.86032 |  0:01:33s\n",
            "epoch 63 | loss: 0.25283 | val_0_roc_auc: 0.94172 | val_0_accuracy: 0.87266 |  0:01:34s\n",
            "epoch 64 | loss: 0.23338 | val_0_roc_auc: 0.94279 | val_0_accuracy: 0.87636 |  0:01:36s\n",
            "epoch 65 | loss: 0.26813 | val_0_roc_auc: 0.9305  | val_0_accuracy: 0.85563 |  0:01:37s\n",
            "epoch 66 | loss: 0.27208 | val_0_roc_auc: 0.92739 | val_0_accuracy: 0.85242 |  0:01:38s\n",
            "epoch 67 | loss: 0.25591 | val_0_roc_auc: 0.93793 | val_0_accuracy: 0.86994 |  0:01:40s\n",
            "epoch 68 | loss: 0.23999 | val_0_roc_auc: 0.94332 | val_0_accuracy: 0.8729  |  0:01:41s\n",
            "epoch 69 | loss: 0.22319 | val_0_roc_auc: 0.94588 | val_0_accuracy: 0.88105 |  0:01:43s\n",
            "epoch 70 | loss: 0.25833 | val_0_roc_auc: 0.93254 | val_0_accuracy: 0.86575 |  0:01:44s\n",
            "epoch 71 | loss: 0.26488 | val_0_roc_auc: 0.93487 | val_0_accuracy: 0.86969 |  0:01:46s\n",
            "epoch 72 | loss: 0.24858 | val_0_roc_auc: 0.93784 | val_0_accuracy: 0.86698 |  0:01:47s\n",
            "epoch 73 | loss: 0.23417 | val_0_roc_auc: 0.94401 | val_0_accuracy: 0.8734  |  0:01:49s\n",
            "epoch 74 | loss: 0.21571 | val_0_roc_auc: 0.94688 | val_0_accuracy: 0.88277 |  0:01:50s\n",
            "epoch 75 | loss: 0.25474 | val_0_roc_auc: 0.9333  | val_0_accuracy: 0.85735 |  0:01:52s\n",
            "epoch 76 | loss: 0.26905 | val_0_roc_auc: 0.93332 | val_0_accuracy: 0.86106 |  0:01:53s\n",
            "epoch 77 | loss: 0.24931 | val_0_roc_auc: 0.9421  | val_0_accuracy: 0.87734 |  0:01:54s\n",
            "epoch 78 | loss: 0.22829 | val_0_roc_auc: 0.94485 | val_0_accuracy: 0.87364 |  0:01:56s\n",
            "epoch 79 | loss: 0.21309 | val_0_roc_auc: 0.9491  | val_0_accuracy: 0.88722 |  0:01:57s\n",
            "epoch 80 | loss: 0.24134 | val_0_roc_auc: 0.93542 | val_0_accuracy: 0.86352 |  0:01:59s\n",
            "epoch 81 | loss: 0.25486 | val_0_roc_auc: 0.93939 | val_0_accuracy: 0.87167 |  0:02:00s\n",
            "epoch 82 | loss: 0.23221 | val_0_roc_auc: 0.94188 | val_0_accuracy: 0.87611 |  0:02:02s\n",
            "epoch 83 | loss: 0.21544 | val_0_roc_auc: 0.94842 | val_0_accuracy: 0.88351 |  0:02:03s\n",
            "epoch 84 | loss: 0.20614 | val_0_roc_auc: 0.95136 | val_0_accuracy: 0.8887  |  0:02:05s\n",
            "epoch 85 | loss: 0.23655 | val_0_roc_auc: 0.93831 | val_0_accuracy: 0.86303 |  0:02:06s\n",
            "epoch 86 | loss: 0.24548 | val_0_roc_auc: 0.93887 | val_0_accuracy: 0.87241 |  0:02:07s\n",
            "epoch 87 | loss: 0.22926 | val_0_roc_auc: 0.9436  | val_0_accuracy: 0.87907 |  0:02:09s\n",
            "epoch 88 | loss: 0.21227 | val_0_roc_auc: 0.94581 | val_0_accuracy: 0.88697 |  0:02:10s\n",
            "epoch 89 | loss: 0.19552 | val_0_roc_auc: 0.9495  | val_0_accuracy: 0.89363 |  0:02:12s\n",
            "epoch 90 | loss: 0.23619 | val_0_roc_auc: 0.93819 | val_0_accuracy: 0.87241 |  0:02:13s\n",
            "epoch 91 | loss: 0.24566 | val_0_roc_auc: 0.94046 | val_0_accuracy: 0.86747 |  0:02:15s\n",
            "epoch 92 | loss: 0.22135 | val_0_roc_auc: 0.94388 | val_0_accuracy: 0.87883 |  0:02:16s\n",
            "epoch 93 | loss: 0.20452 | val_0_roc_auc: 0.94884 | val_0_accuracy: 0.88574 |  0:02:18s\n",
            "epoch 94 | loss: 0.18891 | val_0_roc_auc: 0.95099 | val_0_accuracy: 0.88771 |  0:02:19s\n",
            "epoch 95 | loss: 0.23417 | val_0_roc_auc: 0.93691 | val_0_accuracy: 0.86747 |  0:02:20s\n",
            "epoch 96 | loss: 0.23212 | val_0_roc_auc: 0.94261 | val_0_accuracy: 0.87167 |  0:02:22s\n",
            "epoch 97 | loss: 0.22311 | val_0_roc_auc: 0.94522 | val_0_accuracy: 0.8771  |  0:02:24s\n",
            "epoch 98 | loss: 0.19918 | val_0_roc_auc: 0.95058 | val_0_accuracy: 0.88919 |  0:02:27s\n",
            "epoch 99 | loss: 0.18738 | val_0_roc_auc: 0.95319 | val_0_accuracy: 0.8924  |  0:02:30s\n",
            "epoch 100| loss: 0.22809 | val_0_roc_auc: 0.94488 | val_0_accuracy: 0.87537 |  0:02:34s\n",
            "epoch 101| loss: 0.22801 | val_0_roc_auc: 0.94176 | val_0_accuracy: 0.87463 |  0:02:37s\n",
            "epoch 102| loss: 0.21696 | val_0_roc_auc: 0.94954 | val_0_accuracy: 0.88894 |  0:02:41s\n",
            "epoch 103| loss: 0.19299 | val_0_roc_auc: 0.95364 | val_0_accuracy: 0.89042 |  0:02:44s\n",
            "epoch 104| loss: 0.18036 | val_0_roc_auc: 0.95569 | val_0_accuracy: 0.89511 |  0:02:46s\n",
            "epoch 105| loss: 0.21922 | val_0_roc_auc: 0.93898 | val_0_accuracy: 0.86723 |  0:02:47s\n",
            "epoch 106| loss: 0.22893 | val_0_roc_auc: 0.93919 | val_0_accuracy: 0.87216 |  0:02:48s\n",
            "epoch 107| loss: 0.20974 | val_0_roc_auc: 0.94705 | val_0_accuracy: 0.8845  |  0:02:50s\n",
            "epoch 108| loss: 0.19635 | val_0_roc_auc: 0.95001 | val_0_accuracy: 0.88746 |  0:02:51s\n",
            "epoch 109| loss: 0.17674 | val_0_roc_auc: 0.95312 | val_0_accuracy: 0.89659 |  0:02:53s\n",
            "epoch 110| loss: 0.21748 | val_0_roc_auc: 0.94148 | val_0_accuracy: 0.87636 |  0:02:54s\n",
            "epoch 111| loss: 0.22486 | val_0_roc_auc: 0.94302 | val_0_accuracy: 0.87488 |  0:02:56s\n",
            "epoch 112| loss: 0.20601 | val_0_roc_auc: 0.95144 | val_0_accuracy: 0.88722 |  0:02:57s\n",
            "epoch 113| loss: 0.18459 | val_0_roc_auc: 0.95327 | val_0_accuracy: 0.89388 |  0:02:59s\n",
            "epoch 114| loss: 0.17434 | val_0_roc_auc: 0.95587 | val_0_accuracy: 0.89882 |  0:03:00s\n",
            "epoch 115| loss: 0.21495 | val_0_roc_auc: 0.94208 | val_0_accuracy: 0.86945 |  0:03:02s\n",
            "epoch 116| loss: 0.22528 | val_0_roc_auc: 0.94409 | val_0_accuracy: 0.8766  |  0:03:03s\n",
            "epoch 117| loss: 0.20304 | val_0_roc_auc: 0.94926 | val_0_accuracy: 0.8845  |  0:03:04s\n",
            "epoch 118| loss: 0.18452 | val_0_roc_auc: 0.95677 | val_0_accuracy: 0.8961  |  0:03:06s\n",
            "epoch 119| loss: 0.16803 | val_0_roc_auc: 0.9571  | val_0_accuracy: 0.8998  |  0:03:07s\n",
            "epoch 120| loss: 0.21662 | val_0_roc_auc: 0.94276 | val_0_accuracy: 0.87167 |  0:03:09s\n",
            "epoch 121| loss: 0.22091 | val_0_roc_auc: 0.94794 | val_0_accuracy: 0.88475 |  0:03:10s\n",
            "epoch 122| loss: 0.20639 | val_0_roc_auc: 0.94903 | val_0_accuracy: 0.88722 |  0:03:12s\n",
            "epoch 123| loss: 0.18536 | val_0_roc_auc: 0.95401 | val_0_accuracy: 0.89339 |  0:03:13s\n",
            "epoch 124| loss: 0.16548 | val_0_roc_auc: 0.95605 | val_0_accuracy: 0.90104 |  0:03:15s\n",
            "epoch 125| loss: 0.21487 | val_0_roc_auc: 0.94519 | val_0_accuracy: 0.88006 |  0:03:16s\n",
            "epoch 126| loss: 0.21791 | val_0_roc_auc: 0.94481 | val_0_accuracy: 0.88401 |  0:03:18s\n",
            "epoch 127| loss: 0.20257 | val_0_roc_auc: 0.95228 | val_0_accuracy: 0.89339 |  0:03:19s\n",
            "epoch 128| loss: 0.18638 | val_0_roc_auc: 0.9522  | val_0_accuracy: 0.88672 |  0:03:21s\n",
            "epoch 129| loss: 0.16479 | val_0_roc_auc: 0.95569 | val_0_accuracy: 0.8998  |  0:03:22s\n",
            "epoch 130| loss: 0.20306 | val_0_roc_auc: 0.94967 | val_0_accuracy: 0.88598 |  0:03:24s\n",
            "epoch 131| loss: 0.21292 | val_0_roc_auc: 0.94809 | val_0_accuracy: 0.88648 |  0:03:25s\n",
            "epoch 132| loss: 0.19799 | val_0_roc_auc: 0.955   | val_0_accuracy: 0.89808 |  0:03:26s\n",
            "epoch 133| loss: 0.17298 | val_0_roc_auc: 0.95645 | val_0_accuracy: 0.89783 |  0:03:28s\n",
            "epoch 134| loss: 0.15981 | val_0_roc_auc: 0.95791 | val_0_accuracy: 0.90153 |  0:03:29s\n",
            "epoch 135| loss: 0.20567 | val_0_roc_auc: 0.94104 | val_0_accuracy: 0.87784 |  0:03:31s\n",
            "epoch 136| loss: 0.2167  | val_0_roc_auc: 0.94692 | val_0_accuracy: 0.87907 |  0:03:32s\n",
            "epoch 137| loss: 0.20007 | val_0_roc_auc: 0.95252 | val_0_accuracy: 0.88845 |  0:03:34s\n",
            "epoch 138| loss: 0.18022 | val_0_roc_auc: 0.95602 | val_0_accuracy: 0.89956 |  0:03:37s\n",
            "epoch 139| loss: 0.15837 | val_0_roc_auc: 0.95733 | val_0_accuracy: 0.90597 |  0:03:38s\n",
            "epoch 140| loss: 0.19689 | val_0_roc_auc: 0.94458 | val_0_accuracy: 0.87241 |  0:03:40s\n",
            "epoch 141| loss: 0.21147 | val_0_roc_auc: 0.94598 | val_0_accuracy: 0.88203 |  0:03:41s\n",
            "epoch 142| loss: 0.19554 | val_0_roc_auc: 0.95178 | val_0_accuracy: 0.88894 |  0:03:43s\n",
            "epoch 143| loss: 0.17609 | val_0_roc_auc: 0.95298 | val_0_accuracy: 0.89067 |  0:03:44s\n",
            "epoch 144| loss: 0.15792 | val_0_roc_auc: 0.9587  | val_0_accuracy: 0.9003  |  0:03:45s\n",
            "epoch 145| loss: 0.18945 | val_0_roc_auc: 0.94647 | val_0_accuracy: 0.88105 |  0:03:47s\n",
            "epoch 146| loss: 0.21101 | val_0_roc_auc: 0.94503 | val_0_accuracy: 0.87907 |  0:03:48s\n",
            "epoch 147| loss: 0.18814 | val_0_roc_auc: 0.95112 | val_0_accuracy: 0.89314 |  0:03:50s\n",
            "epoch 148| loss: 0.17051 | val_0_roc_auc: 0.95799 | val_0_accuracy: 0.9003  |  0:03:51s\n",
            "epoch 149| loss: 0.15513 | val_0_roc_auc: 0.95676 | val_0_accuracy: 0.90252 |  0:03:53s\n",
            "epoch 150| loss: 0.20077 | val_0_roc_auc: 0.94294 | val_0_accuracy: 0.87167 |  0:03:54s\n",
            "epoch 151| loss: 0.20387 | val_0_roc_auc: 0.94964 | val_0_accuracy: 0.88425 |  0:03:56s\n",
            "epoch 152| loss: 0.18412 | val_0_roc_auc: 0.95243 | val_0_accuracy: 0.89462 |  0:03:57s\n",
            "epoch 153| loss: 0.17146 | val_0_roc_auc: 0.95666 | val_0_accuracy: 0.89832 |  0:03:59s\n",
            "epoch 154| loss: 0.14971 | val_0_roc_auc: 0.95872 | val_0_accuracy: 0.90745 |  0:04:00s\n",
            "epoch 155| loss: 0.19188 | val_0_roc_auc: 0.94761 | val_0_accuracy: 0.88771 |  0:04:01s\n",
            "epoch 156| loss: 0.1933  | val_0_roc_auc: 0.94578 | val_0_accuracy: 0.88105 |  0:04:03s\n",
            "epoch 157| loss: 0.18679 | val_0_roc_auc: 0.95131 | val_0_accuracy: 0.88944 |  0:04:04s\n",
            "epoch 158| loss: 0.16351 | val_0_roc_auc: 0.95671 | val_0_accuracy: 0.90104 |  0:04:06s\n",
            "epoch 159| loss: 0.15113 | val_0_roc_auc: 0.95928 | val_0_accuracy: 0.90227 |  0:04:07s\n",
            "epoch 160| loss: 0.19181 | val_0_roc_auc: 0.94301 | val_0_accuracy: 0.88327 |  0:04:09s\n",
            "epoch 161| loss: 0.20501 | val_0_roc_auc: 0.95128 | val_0_accuracy: 0.89166 |  0:04:11s\n",
            "epoch 162| loss: 0.18424 | val_0_roc_auc: 0.95614 | val_0_accuracy: 0.89659 |  0:04:13s\n",
            "epoch 163| loss: 0.16456 | val_0_roc_auc: 0.95878 | val_0_accuracy: 0.90301 |  0:04:14s\n",
            "epoch 164| loss: 0.14786 | val_0_roc_auc: 0.96068 | val_0_accuracy: 0.90869 |  0:04:15s\n",
            "epoch 165| loss: 0.19516 | val_0_roc_auc: 0.94219 | val_0_accuracy: 0.88154 |  0:04:17s\n",
            "epoch 166| loss: 0.2007  | val_0_roc_auc: 0.94894 | val_0_accuracy: 0.88672 |  0:04:18s\n",
            "epoch 167| loss: 0.18152 | val_0_roc_auc: 0.95594 | val_0_accuracy: 0.8961  |  0:04:20s\n",
            "epoch 168| loss: 0.16062 | val_0_roc_auc: 0.95769 | val_0_accuracy: 0.90499 |  0:04:21s\n",
            "epoch 169| loss: 0.14218 | val_0_roc_auc: 0.95995 | val_0_accuracy: 0.90449 |  0:04:23s\n",
            "epoch 170| loss: 0.185   | val_0_roc_auc: 0.94899 | val_0_accuracy: 0.8887  |  0:04:24s\n",
            "epoch 171| loss: 0.18627 | val_0_roc_auc: 0.94796 | val_0_accuracy: 0.88129 |  0:04:26s\n",
            "epoch 172| loss: 0.182   | val_0_roc_auc: 0.95439 | val_0_accuracy: 0.89487 |  0:04:27s\n",
            "epoch 173| loss: 0.16065 | val_0_roc_auc: 0.95945 | val_0_accuracy: 0.90128 |  0:04:28s\n",
            "epoch 174| loss: 0.14259 | val_0_roc_auc: 0.96045 | val_0_accuracy: 0.9077  |  0:04:30s\n",
            "epoch 175| loss: 0.19209 | val_0_roc_auc: 0.94645 | val_0_accuracy: 0.885   |  0:04:31s\n",
            "epoch 176| loss: 0.19598 | val_0_roc_auc: 0.95269 | val_0_accuracy: 0.8924  |  0:04:33s\n",
            "epoch 177| loss: 0.1826  | val_0_roc_auc: 0.955   | val_0_accuracy: 0.89413 |  0:04:34s\n",
            "epoch 178| loss: 0.1532  | val_0_roc_auc: 0.9596  | val_0_accuracy: 0.904   |  0:04:36s\n",
            "epoch 179| loss: 0.13942 | val_0_roc_auc: 0.96072 | val_0_accuracy: 0.90795 |  0:04:37s\n",
            "epoch 180| loss: 0.17524 | val_0_roc_auc: 0.95153 | val_0_accuracy: 0.8845  |  0:04:38s\n",
            "epoch 181| loss: 0.1868  | val_0_roc_auc: 0.94983 | val_0_accuracy: 0.885   |  0:04:40s\n",
            "epoch 182| loss: 0.17593 | val_0_roc_auc: 0.95476 | val_0_accuracy: 0.9003  |  0:04:41s\n",
            "epoch 183| loss: 0.15551 | val_0_roc_auc: 0.95967 | val_0_accuracy: 0.90375 |  0:04:43s\n",
            "epoch 184| loss: 0.13695 | val_0_roc_auc: 0.96266 | val_0_accuracy: 0.90844 |  0:04:44s\n",
            "epoch 185| loss: 0.17616 | val_0_roc_auc: 0.94903 | val_0_accuracy: 0.88475 |  0:04:46s\n",
            "epoch 186| loss: 0.18876 | val_0_roc_auc: 0.94904 | val_0_accuracy: 0.88894 |  0:04:47s\n",
            "epoch 187| loss: 0.17659 | val_0_roc_auc: 0.95278 | val_0_accuracy: 0.8961  |  0:04:48s\n",
            "epoch 188| loss: 0.15534 | val_0_roc_auc: 0.96016 | val_0_accuracy: 0.90424 |  0:04:50s\n",
            "epoch 189| loss: 0.13438 | val_0_roc_auc: 0.96167 | val_0_accuracy: 0.90548 |  0:04:51s\n",
            "epoch 190| loss: 0.17806 | val_0_roc_auc: 0.94846 | val_0_accuracy: 0.88598 |  0:04:53s\n",
            "epoch 191| loss: 0.19834 | val_0_roc_auc: 0.95039 | val_0_accuracy: 0.88574 |  0:04:54s\n",
            "epoch 192| loss: 0.17427 | val_0_roc_auc: 0.95711 | val_0_accuracy: 0.90178 |  0:04:56s\n",
            "epoch 193| loss: 0.14938 | val_0_roc_auc: 0.96114 | val_0_accuracy: 0.90819 |  0:04:57s\n",
            "epoch 194| loss: 0.13303 | val_0_roc_auc: 0.96156 | val_0_accuracy: 0.90474 |  0:04:58s\n",
            "epoch 195| loss: 0.1729  | val_0_roc_auc: 0.94545 | val_0_accuracy: 0.87981 |  0:05:00s\n",
            "epoch 196| loss: 0.19414 | val_0_roc_auc: 0.95499 | val_0_accuracy: 0.89882 |  0:05:01s\n",
            "epoch 197| loss: 0.16333 | val_0_roc_auc: 0.95527 | val_0_accuracy: 0.89191 |  0:05:03s\n",
            "epoch 198| loss: 0.14712 | val_0_roc_auc: 0.96125 | val_0_accuracy: 0.90992 |  0:05:04s\n",
            "epoch 199| loss: 0.1292  | val_0_roc_auc: 0.9638  | val_0_accuracy: 0.9156  |  0:05:06s\n",
            "epoch 200| loss: 0.1723  | val_0_roc_auc: 0.95418 | val_0_accuracy: 0.89363 |  0:05:07s\n",
            "epoch 201| loss: 0.18321 | val_0_roc_auc: 0.95553 | val_0_accuracy: 0.88993 |  0:05:09s\n",
            "epoch 202| loss: 0.16842 | val_0_roc_auc: 0.95844 | val_0_accuracy: 0.904   |  0:05:10s\n",
            "epoch 203| loss: 0.14616 | val_0_roc_auc: 0.96218 | val_0_accuracy: 0.91214 |  0:05:11s\n",
            "epoch 204| loss: 0.12815 | val_0_roc_auc: 0.96234 | val_0_accuracy: 0.91115 |  0:05:13s\n",
            "epoch 205| loss: 0.16208 | val_0_roc_auc: 0.94745 | val_0_accuracy: 0.88623 |  0:05:14s\n",
            "epoch 206| loss: 0.18685 | val_0_roc_auc: 0.95217 | val_0_accuracy: 0.885   |  0:05:16s\n",
            "epoch 207| loss: 0.16434 | val_0_roc_auc: 0.95947 | val_0_accuracy: 0.90573 |  0:05:17s\n",
            "epoch 208| loss: 0.14642 | val_0_roc_auc: 0.96234 | val_0_accuracy: 0.91239 |  0:05:19s\n",
            "epoch 209| loss: 0.12659 | val_0_roc_auc: 0.9632  | val_0_accuracy: 0.91091 |  0:05:20s\n",
            "epoch 210| loss: 0.17072 | val_0_roc_auc: 0.95043 | val_0_accuracy: 0.89437 |  0:05:22s\n",
            "epoch 211| loss: 0.18301 | val_0_roc_auc: 0.95239 | val_0_accuracy: 0.89487 |  0:05:23s\n",
            "epoch 212| loss: 0.16224 | val_0_roc_auc: 0.95612 | val_0_accuracy: 0.9003  |  0:05:24s\n",
            "epoch 213| loss: 0.14058 | val_0_roc_auc: 0.95975 | val_0_accuracy: 0.90597 |  0:05:26s\n",
            "epoch 214| loss: 0.12621 | val_0_roc_auc: 0.96286 | val_0_accuracy: 0.90819 |  0:05:27s\n",
            "epoch 215| loss: 0.16782 | val_0_roc_auc: 0.95123 | val_0_accuracy: 0.89487 |  0:05:29s\n",
            "epoch 216| loss: 0.17296 | val_0_roc_auc: 0.95291 | val_0_accuracy: 0.88968 |  0:05:30s\n",
            "epoch 217| loss: 0.16207 | val_0_roc_auc: 0.95519 | val_0_accuracy: 0.90005 |  0:05:32s\n",
            "epoch 218| loss: 0.14297 | val_0_roc_auc: 0.96217 | val_0_accuracy: 0.9077  |  0:05:33s\n",
            "epoch 219| loss: 0.12602 | val_0_roc_auc: 0.96252 | val_0_accuracy: 0.91313 |  0:05:34s\n",
            "epoch 220| loss: 0.16384 | val_0_roc_auc: 0.95217 | val_0_accuracy: 0.88845 |  0:05:36s\n",
            "epoch 221| loss: 0.17422 | val_0_roc_auc: 0.95401 | val_0_accuracy: 0.89659 |  0:05:37s\n",
            "epoch 222| loss: 0.15316 | val_0_roc_auc: 0.95781 | val_0_accuracy: 0.89808 |  0:05:39s\n",
            "epoch 223| loss: 0.13266 | val_0_roc_auc: 0.96187 | val_0_accuracy: 0.9119  |  0:05:40s\n",
            "epoch 224| loss: 0.12275 | val_0_roc_auc: 0.9624  | val_0_accuracy: 0.91288 |  0:05:42s\n",
            "epoch 225| loss: 0.16136 | val_0_roc_auc: 0.95008 | val_0_accuracy: 0.88796 |  0:05:43s\n",
            "epoch 226| loss: 0.17771 | val_0_roc_auc: 0.95292 | val_0_accuracy: 0.88894 |  0:05:45s\n",
            "epoch 227| loss: 0.15904 | val_0_roc_auc: 0.95914 | val_0_accuracy: 0.90252 |  0:05:46s\n",
            "epoch 228| loss: 0.13349 | val_0_roc_auc: 0.96195 | val_0_accuracy: 0.90474 |  0:05:48s\n",
            "epoch 229| loss: 0.12163 | val_0_roc_auc: 0.96344 | val_0_accuracy: 0.91115 |  0:05:49s\n",
            "epoch 230| loss: 0.16602 | val_0_roc_auc: 0.94956 | val_0_accuracy: 0.89166 |  0:05:51s\n",
            "epoch 231| loss: 0.17863 | val_0_roc_auc: 0.94976 | val_0_accuracy: 0.8887  |  0:05:53s\n",
            "epoch 232| loss: 0.16346 | val_0_roc_auc: 0.96009 | val_0_accuracy: 0.90326 |  0:05:54s\n",
            "epoch 233| loss: 0.13485 | val_0_roc_auc: 0.96156 | val_0_accuracy: 0.90474 |  0:05:56s\n",
            "epoch 234| loss: 0.12059 | val_0_roc_auc: 0.96382 | val_0_accuracy: 0.90893 |  0:05:57s\n",
            "epoch 235| loss: 0.15915 | val_0_roc_auc: 0.95058 | val_0_accuracy: 0.89314 |  0:05:59s\n",
            "epoch 236| loss: 0.17364 | val_0_roc_auc: 0.95833 | val_0_accuracy: 0.90573 |  0:06:00s\n",
            "epoch 237| loss: 0.1531  | val_0_roc_auc: 0.95772 | val_0_accuracy: 0.90819 |  0:06:02s\n",
            "epoch 238| loss: 0.13266 | val_0_roc_auc: 0.96199 | val_0_accuracy: 0.9077  |  0:06:03s\n",
            "epoch 239| loss: 0.11618 | val_0_roc_auc: 0.96398 | val_0_accuracy: 0.91362 |  0:06:05s\n",
            "epoch 240| loss: 0.16519 | val_0_roc_auc: 0.95156 | val_0_accuracy: 0.89215 |  0:06:06s\n",
            "epoch 241| loss: 0.17169 | val_0_roc_auc: 0.95422 | val_0_accuracy: 0.90153 |  0:06:08s\n",
            "epoch 242| loss: 0.15323 | val_0_roc_auc: 0.95433 | val_0_accuracy: 0.89487 |  0:06:09s\n",
            "epoch 243| loss: 0.13568 | val_0_roc_auc: 0.9623  | val_0_accuracy: 0.91239 |  0:06:11s\n",
            "epoch 244| loss: 0.11881 | val_0_roc_auc: 0.96338 | val_0_accuracy: 0.91658 |  0:06:12s\n",
            "epoch 245| loss: 0.16637 | val_0_roc_auc: 0.95252 | val_0_accuracy: 0.89339 |  0:06:14s\n",
            "epoch 246| loss: 0.16018 | val_0_roc_auc: 0.95338 | val_0_accuracy: 0.89561 |  0:06:15s\n",
            "epoch 247| loss: 0.15262 | val_0_roc_auc: 0.95397 | val_0_accuracy: 0.9035  |  0:06:17s\n",
            "epoch 248| loss: 0.13207 | val_0_roc_auc: 0.9626  | val_0_accuracy: 0.91634 |  0:06:18s\n",
            "epoch 249| loss: 0.11336 | val_0_roc_auc: 0.96344 | val_0_accuracy: 0.91535 |  0:06:20s\n",
            "epoch 250| loss: 0.15906 | val_0_roc_auc: 0.95304 | val_0_accuracy: 0.89733 |  0:06:21s\n",
            "epoch 251| loss: 0.16409 | val_0_roc_auc: 0.95619 | val_0_accuracy: 0.89709 |  0:06:23s\n",
            "epoch 252| loss: 0.15052 | val_0_roc_auc: 0.95921 | val_0_accuracy: 0.90375 |  0:06:24s\n",
            "epoch 253| loss: 0.13117 | val_0_roc_auc: 0.96423 | val_0_accuracy: 0.91658 |  0:06:25s\n",
            "epoch 254| loss: 0.11596 | val_0_roc_auc: 0.96474 | val_0_accuracy: 0.91782 |  0:06:27s\n",
            "epoch 255| loss: 0.15189 | val_0_roc_auc: 0.95274 | val_0_accuracy: 0.8998  |  0:06:28s\n",
            "epoch 256| loss: 0.16723 | val_0_roc_auc: 0.95471 | val_0_accuracy: 0.9035  |  0:06:30s\n",
            "epoch 257| loss: 0.13786 | val_0_roc_auc: 0.95992 | val_0_accuracy: 0.90943 |  0:06:31s\n",
            "epoch 258| loss: 0.1275  | val_0_roc_auc: 0.96213 | val_0_accuracy: 0.91486 |  0:06:33s\n",
            "epoch 259| loss: 0.10772 | val_0_roc_auc: 0.96389 | val_0_accuracy: 0.91881 |  0:06:34s\n",
            "epoch 260| loss: 0.14976 | val_0_roc_auc: 0.95226 | val_0_accuracy: 0.89709 |  0:06:36s\n",
            "epoch 261| loss: 0.16218 | val_0_roc_auc: 0.95592 | val_0_accuracy: 0.90326 |  0:06:37s\n",
            "epoch 262| loss: 0.14069 | val_0_roc_auc: 0.96116 | val_0_accuracy: 0.9114  |  0:06:39s\n",
            "epoch 263| loss: 0.12623 | val_0_roc_auc: 0.96224 | val_0_accuracy: 0.9156  |  0:06:40s\n",
            "epoch 264| loss: 0.108   | val_0_roc_auc: 0.9639  | val_0_accuracy: 0.91757 |  0:06:41s\n",
            "epoch 265| loss: 0.15129 | val_0_roc_auc: 0.94575 | val_0_accuracy: 0.88722 |  0:06:43s\n",
            "epoch 266| loss: 0.16697 | val_0_roc_auc: 0.95347 | val_0_accuracy: 0.8961  |  0:06:44s\n",
            "epoch 267| loss: 0.13846 | val_0_roc_auc: 0.95573 | val_0_accuracy: 0.9035  |  0:06:46s\n",
            "epoch 268| loss: 0.12579 | val_0_roc_auc: 0.95865 | val_0_accuracy: 0.90943 |  0:06:47s\n",
            "epoch 269| loss: 0.10823 | val_0_roc_auc: 0.96054 | val_0_accuracy: 0.91412 |  0:06:49s\n",
            "epoch 270| loss: 0.14869 | val_0_roc_auc: 0.94922 | val_0_accuracy: 0.88697 |  0:06:50s\n",
            "epoch 271| loss: 0.16633 | val_0_roc_auc: 0.95812 | val_0_accuracy: 0.904   |  0:06:52s\n",
            "epoch 272| loss: 0.14413 | val_0_roc_auc: 0.95807 | val_0_accuracy: 0.90671 |  0:06:53s\n",
            "epoch 273| loss: 0.12265 | val_0_roc_auc: 0.96118 | val_0_accuracy: 0.91091 |  0:06:54s\n",
            "epoch 274| loss: 0.10618 | val_0_roc_auc: 0.96353 | val_0_accuracy: 0.91584 |  0:06:56s\n",
            "epoch 275| loss: 0.1499  | val_0_roc_auc: 0.95164 | val_0_accuracy: 0.89585 |  0:06:57s\n",
            "epoch 276| loss: 0.15954 | val_0_roc_auc: 0.95494 | val_0_accuracy: 0.90079 |  0:06:59s\n",
            "epoch 277| loss: 0.13573 | val_0_roc_auc: 0.95863 | val_0_accuracy: 0.90276 |  0:07:00s\n",
            "epoch 278| loss: 0.12418 | val_0_roc_auc: 0.9603  | val_0_accuracy: 0.9156  |  0:07:02s\n",
            "epoch 279| loss: 0.10984 | val_0_roc_auc: 0.96227 | val_0_accuracy: 0.91757 |  0:07:03s\n",
            "epoch 280| loss: 0.14916 | val_0_roc_auc: 0.95586 | val_0_accuracy: 0.90005 |  0:07:05s\n",
            "epoch 281| loss: 0.15608 | val_0_roc_auc: 0.95487 | val_0_accuracy: 0.89339 |  0:07:06s\n",
            "epoch 282| loss: 0.145   | val_0_roc_auc: 0.96287 | val_0_accuracy: 0.91091 |  0:07:07s\n",
            "epoch 283| loss: 0.12037 | val_0_roc_auc: 0.9641  | val_0_accuracy: 0.91288 |  0:07:09s\n",
            "epoch 284| loss: 0.1026  | val_0_roc_auc: 0.96488 | val_0_accuracy: 0.91955 |  0:07:10s\n",
            "epoch 285| loss: 0.1351  | val_0_roc_auc: 0.95514 | val_0_accuracy: 0.89931 |  0:07:12s\n",
            "epoch 286| loss: 0.16239 | val_0_roc_auc: 0.96221 | val_0_accuracy: 0.90844 |  0:07:13s\n",
            "epoch 287| loss: 0.13226 | val_0_roc_auc: 0.96497 | val_0_accuracy: 0.91214 |  0:07:15s\n",
            "epoch 288| loss: 0.11557 | val_0_roc_auc: 0.96598 | val_0_accuracy: 0.91412 |  0:07:16s\n",
            "epoch 289| loss: 0.10073 | val_0_roc_auc: 0.96743 | val_0_accuracy: 0.92251 |  0:07:18s\n",
            "epoch 290| loss: 0.14628 | val_0_roc_auc: 0.95384 | val_0_accuracy: 0.89857 |  0:07:19s\n",
            "epoch 291| loss: 0.16139 | val_0_roc_auc: 0.96011 | val_0_accuracy: 0.90326 |  0:07:21s\n",
            "epoch 292| loss: 0.1336  | val_0_roc_auc: 0.96343 | val_0_accuracy: 0.91387 |  0:07:22s\n",
            "epoch 293| loss: 0.11831 | val_0_roc_auc: 0.96419 | val_0_accuracy: 0.91486 |  0:07:24s\n",
            "epoch 294| loss: 0.10362 | val_0_roc_auc: 0.96549 | val_0_accuracy: 0.91708 |  0:07:25s\n",
            "epoch 295| loss: 0.14303 | val_0_roc_auc: 0.94607 | val_0_accuracy: 0.89314 |  0:07:26s\n",
            "epoch 296| loss: 0.15345 | val_0_roc_auc: 0.95417 | val_0_accuracy: 0.90005 |  0:07:28s\n",
            "epoch 297| loss: 0.14178 | val_0_roc_auc: 0.96021 | val_0_accuracy: 0.91239 |  0:07:29s\n",
            "epoch 298| loss: 0.1182  | val_0_roc_auc: 0.9638  | val_0_accuracy: 0.91535 |  0:07:31s\n",
            "epoch 299| loss: 0.101   | val_0_roc_auc: 0.96469 | val_0_accuracy: 0.91683 |  0:07:33s\n",
            "epoch 300| loss: 0.14152 | val_0_roc_auc: 0.95076 | val_0_accuracy: 0.89289 |  0:07:35s\n",
            "epoch 301| loss: 0.15819 | val_0_roc_auc: 0.9554  | val_0_accuracy: 0.90005 |  0:07:36s\n",
            "epoch 302| loss: 0.13743 | val_0_roc_auc: 0.96238 | val_0_accuracy: 0.91264 |  0:07:38s\n",
            "epoch 303| loss: 0.11571 | val_0_roc_auc: 0.96541 | val_0_accuracy: 0.92029 |  0:07:39s\n",
            "epoch 304| loss: 0.10083 | val_0_roc_auc: 0.96651 | val_0_accuracy: 0.91881 |  0:07:41s\n",
            "epoch 305| loss: 0.14644 | val_0_roc_auc: 0.95403 | val_0_accuracy: 0.904   |  0:07:42s\n",
            "epoch 306| loss: 0.15172 | val_0_roc_auc: 0.95636 | val_0_accuracy: 0.90474 |  0:07:44s\n",
            "epoch 307| loss: 0.13533 | val_0_roc_auc: 0.95928 | val_0_accuracy: 0.90795 |  0:07:45s\n",
            "epoch 308| loss: 0.11285 | val_0_roc_auc: 0.96187 | val_0_accuracy: 0.91436 |  0:07:47s\n",
            "epoch 309| loss: 0.10115 | val_0_roc_auc: 0.96355 | val_0_accuracy: 0.91831 |  0:07:48s\n",
            "epoch 310| loss: 0.14497 | val_0_roc_auc: 0.95698 | val_0_accuracy: 0.90474 |  0:07:50s\n",
            "epoch 311| loss: 0.15564 | val_0_roc_auc: 0.96188 | val_0_accuracy: 0.90795 |  0:07:51s\n",
            "epoch 312| loss: 0.14029 | val_0_roc_auc: 0.95952 | val_0_accuracy: 0.90943 |  0:07:53s\n",
            "epoch 313| loss: 0.12088 | val_0_roc_auc: 0.96449 | val_0_accuracy: 0.91757 |  0:07:54s\n",
            "epoch 314| loss: 0.09898 | val_0_roc_auc: 0.96656 | val_0_accuracy: 0.92448 |  0:07:56s\n",
            "epoch 315| loss: 0.13783 | val_0_roc_auc: 0.95817 | val_0_accuracy: 0.90499 |  0:07:57s\n",
            "epoch 316| loss: 0.1467  | val_0_roc_auc: 0.9573  | val_0_accuracy: 0.90869 |  0:07:59s\n",
            "epoch 317| loss: 0.13384 | val_0_roc_auc: 0.95969 | val_0_accuracy: 0.91288 |  0:08:00s\n",
            "epoch 318| loss: 0.10922 | val_0_roc_auc: 0.96279 | val_0_accuracy: 0.91387 |  0:08:01s\n",
            "epoch 319| loss: 0.10178 | val_0_roc_auc: 0.96317 | val_0_accuracy: 0.92127 |  0:08:03s\n",
            "epoch 320| loss: 0.1408  | val_0_roc_auc: 0.95625 | val_0_accuracy: 0.90499 |  0:08:04s\n",
            "epoch 321| loss: 0.15046 | val_0_roc_auc: 0.95548 | val_0_accuracy: 0.90745 |  0:08:06s\n",
            "epoch 322| loss: 0.13163 | val_0_roc_auc: 0.96288 | val_0_accuracy: 0.91609 |  0:08:07s\n",
            "epoch 323| loss: 0.11179 | val_0_roc_auc: 0.9645  | val_0_accuracy: 0.92004 |  0:08:09s\n",
            "epoch 324| loss: 0.09892 | val_0_roc_auc: 0.96631 | val_0_accuracy: 0.923   |  0:08:10s\n",
            "epoch 325| loss: 0.13519 | val_0_roc_auc: 0.9567  | val_0_accuracy: 0.90647 |  0:08:12s\n",
            "epoch 326| loss: 0.14851 | val_0_roc_auc: 0.95717 | val_0_accuracy: 0.90795 |  0:08:13s\n",
            "epoch 327| loss: 0.13166 | val_0_roc_auc: 0.96213 | val_0_accuracy: 0.91288 |  0:08:15s\n",
            "epoch 328| loss: 0.112   | val_0_roc_auc: 0.96376 | val_0_accuracy: 0.92127 |  0:08:16s\n",
            "epoch 329| loss: 0.09586 | val_0_roc_auc: 0.96607 | val_0_accuracy: 0.92547 |  0:08:18s\n",
            "epoch 330| loss: 0.13095 | val_0_roc_auc: 0.95429 | val_0_accuracy: 0.89561 |  0:08:19s\n",
            "epoch 331| loss: 0.14429 | val_0_roc_auc: 0.95845 | val_0_accuracy: 0.90893 |  0:08:20s\n",
            "epoch 332| loss: 0.12222 | val_0_roc_auc: 0.96071 | val_0_accuracy: 0.91856 |  0:08:22s\n",
            "epoch 333| loss: 0.10684 | val_0_roc_auc: 0.96418 | val_0_accuracy: 0.91881 |  0:08:23s\n",
            "epoch 334| loss: 0.09807 | val_0_roc_auc: 0.96484 | val_0_accuracy: 0.92029 |  0:08:25s\n",
            "epoch 335| loss: 0.14907 | val_0_roc_auc: 0.9542  | val_0_accuracy: 0.9003  |  0:08:26s\n",
            "epoch 336| loss: 0.15337 | val_0_roc_auc: 0.95571 | val_0_accuracy: 0.8998  |  0:08:28s\n",
            "epoch 337| loss: 0.13889 | val_0_roc_auc: 0.96275 | val_0_accuracy: 0.91387 |  0:08:29s\n",
            "epoch 338| loss: 0.10914 | val_0_roc_auc: 0.96646 | val_0_accuracy: 0.92004 |  0:08:31s\n",
            "epoch 339| loss: 0.09646 | val_0_roc_auc: 0.96719 | val_0_accuracy: 0.92374 |  0:08:32s\n",
            "epoch 340| loss: 0.12993 | val_0_roc_auc: 0.96072 | val_0_accuracy: 0.9114  |  0:08:34s\n",
            "epoch 341| loss: 0.14212 | val_0_roc_auc: 0.95862 | val_0_accuracy: 0.90523 |  0:08:35s\n",
            "epoch 342| loss: 0.13232 | val_0_roc_auc: 0.96156 | val_0_accuracy: 0.9119  |  0:08:37s\n",
            "epoch 343| loss: 0.1065  | val_0_roc_auc: 0.96403 | val_0_accuracy: 0.91955 |  0:08:38s\n",
            "epoch 344| loss: 0.09409 | val_0_roc_auc: 0.96519 | val_0_accuracy: 0.91831 |  0:08:40s\n",
            "epoch 345| loss: 0.12688 | val_0_roc_auc: 0.95603 | val_0_accuracy: 0.90128 |  0:08:41s\n",
            "epoch 346| loss: 0.14705 | val_0_roc_auc: 0.95106 | val_0_accuracy: 0.89808 |  0:08:42s\n",
            "epoch 347| loss: 0.13064 | val_0_roc_auc: 0.96036 | val_0_accuracy: 0.90918 |  0:08:44s\n",
            "epoch 348| loss: 0.1082  | val_0_roc_auc: 0.96437 | val_0_accuracy: 0.92152 |  0:08:45s\n",
            "epoch 349| loss: 0.09697 | val_0_roc_auc: 0.96552 | val_0_accuracy: 0.91955 |  0:08:47s\n",
            "epoch 350| loss: 0.12869 | val_0_roc_auc: 0.95292 | val_0_accuracy: 0.89783 |  0:08:48s\n",
            "epoch 351| loss: 0.15018 | val_0_roc_auc: 0.95888 | val_0_accuracy: 0.90647 |  0:08:50s\n",
            "epoch 352| loss: 0.12599 | val_0_roc_auc: 0.96196 | val_0_accuracy: 0.91288 |  0:08:51s\n",
            "epoch 353| loss: 0.10406 | val_0_roc_auc: 0.96445 | val_0_accuracy: 0.91782 |  0:08:53s\n",
            "epoch 354| loss: 0.09457 | val_0_roc_auc: 0.96518 | val_0_accuracy: 0.91856 |  0:08:54s\n",
            "epoch 355| loss: 0.13683 | val_0_roc_auc: 0.95443 | val_0_accuracy: 0.90178 |  0:08:56s\n",
            "epoch 356| loss: 0.14222 | val_0_roc_auc: 0.96012 | val_0_accuracy: 0.9077  |  0:08:57s\n",
            "epoch 357| loss: 0.12514 | val_0_roc_auc: 0.96252 | val_0_accuracy: 0.91757 |  0:08:59s\n",
            "epoch 358| loss: 0.10616 | val_0_roc_auc: 0.96426 | val_0_accuracy: 0.91757 |  0:09:00s\n",
            "epoch 359| loss: 0.09172 | val_0_roc_auc: 0.96532 | val_0_accuracy: 0.92127 |  0:09:01s\n",
            "epoch 360| loss: 0.12194 | val_0_roc_auc: 0.96036 | val_0_accuracy: 0.90918 |  0:09:03s\n",
            "epoch 361| loss: 0.13968 | val_0_roc_auc: 0.95833 | val_0_accuracy: 0.9077  |  0:09:04s\n",
            "epoch 362| loss: 0.12803 | val_0_roc_auc: 0.96185 | val_0_accuracy: 0.91239 |  0:09:06s\n",
            "epoch 363| loss: 0.10598 | val_0_roc_auc: 0.9645  | val_0_accuracy: 0.92004 |  0:09:07s\n",
            "epoch 364| loss: 0.08919 | val_0_roc_auc: 0.96591 | val_0_accuracy: 0.92177 |  0:09:09s\n",
            "epoch 365| loss: 0.1323  | val_0_roc_auc: 0.95591 | val_0_accuracy: 0.904   |  0:09:10s\n",
            "epoch 366| loss: 0.14238 | val_0_roc_auc: 0.95892 | val_0_accuracy: 0.90696 |  0:09:12s\n",
            "epoch 367| loss: 0.12734 | val_0_roc_auc: 0.96014 | val_0_accuracy: 0.90918 |  0:09:14s\n",
            "epoch 368| loss: 0.10118 | val_0_roc_auc: 0.96405 | val_0_accuracy: 0.91461 |  0:09:16s\n",
            "epoch 369| loss: 0.08524 | val_0_roc_auc: 0.96566 | val_0_accuracy: 0.92275 |  0:09:17s\n",
            "epoch 370| loss: 0.12832 | val_0_roc_auc: 0.95569 | val_0_accuracy: 0.90424 |  0:09:19s\n",
            "epoch 371| loss: 0.14421 | val_0_roc_auc: 0.96115 | val_0_accuracy: 0.90819 |  0:09:20s\n",
            "epoch 372| loss: 0.11647 | val_0_roc_auc: 0.96354 | val_0_accuracy: 0.91708 |  0:09:22s\n",
            "epoch 373| loss: 0.09904 | val_0_roc_auc: 0.96705 | val_0_accuracy: 0.92029 |  0:09:23s\n",
            "epoch 374| loss: 0.08448 | val_0_roc_auc: 0.96837 | val_0_accuracy: 0.92399 |  0:09:24s\n",
            "epoch 375| loss: 0.12437 | val_0_roc_auc: 0.95757 | val_0_accuracy: 0.90499 |  0:09:26s\n",
            "epoch 376| loss: 0.13898 | val_0_roc_auc: 0.95871 | val_0_accuracy: 0.90523 |  0:09:27s\n",
            "epoch 377| loss: 0.13053 | val_0_roc_auc: 0.96376 | val_0_accuracy: 0.92029 |  0:09:29s\n",
            "epoch 378| loss: 0.10579 | val_0_roc_auc: 0.96466 | val_0_accuracy: 0.92177 |  0:09:30s\n",
            "epoch 379| loss: 0.091   | val_0_roc_auc: 0.96669 | val_0_accuracy: 0.92349 |  0:09:32s\n",
            "\n",
            "Early stopping occurred at epoch 379 with best_epoch = 329 and best_val_0_accuracy = 0.92547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KFold 3 of 5\n",
            "epoch 0  | loss: 0.78435 | val_0_roc_auc: 0.72363 | val_0_accuracy: 0.65499 |  0:00:01s\n",
            "epoch 1  | loss: 0.6242  | val_0_roc_auc: 0.78039 | val_0_accuracy: 0.70953 |  0:00:02s\n",
            "epoch 2  | loss: 0.57557 | val_0_roc_auc: 0.79451 | val_0_accuracy: 0.71767 |  0:00:04s\n",
            "epoch 3  | loss: 0.55329 | val_0_roc_auc: 0.80627 | val_0_accuracy: 0.72335 |  0:00:05s\n",
            "epoch 4  | loss: 0.54273 | val_0_roc_auc: 0.81175 | val_0_accuracy: 0.72828 |  0:00:07s\n",
            "epoch 5  | loss: 0.52596 | val_0_roc_auc: 0.82737 | val_0_accuracy: 0.74013 |  0:00:08s\n",
            "epoch 6  | loss: 0.50786 | val_0_roc_auc: 0.83331 | val_0_accuracy: 0.7463  |  0:00:10s\n",
            "epoch 7  | loss: 0.49439 | val_0_roc_auc: 0.8413  | val_0_accuracy: 0.75222 |  0:00:11s\n",
            "epoch 8  | loss: 0.48464 | val_0_roc_auc: 0.84666 | val_0_accuracy: 0.75864 |  0:00:13s\n",
            "epoch 9  | loss: 0.47764 | val_0_roc_auc: 0.84858 | val_0_accuracy: 0.75716 |  0:00:14s\n",
            "epoch 10 | loss: 0.47983 | val_0_roc_auc: 0.84752 | val_0_accuracy: 0.75543 |  0:00:16s\n",
            "epoch 11 | loss: 0.47107 | val_0_roc_auc: 0.85597 | val_0_accuracy: 0.76555 |  0:00:17s\n",
            "epoch 12 | loss: 0.45708 | val_0_roc_auc: 0.8595  | val_0_accuracy: 0.77591 |  0:00:19s\n",
            "epoch 13 | loss: 0.45221 | val_0_roc_auc: 0.86136 | val_0_accuracy: 0.77246 |  0:00:20s\n",
            "epoch 14 | loss: 0.44474 | val_0_roc_auc: 0.86441 | val_0_accuracy: 0.77468 |  0:00:22s\n",
            "epoch 15 | loss: 0.4434  | val_0_roc_auc: 0.86543 | val_0_accuracy: 0.77295 |  0:00:23s\n",
            "epoch 16 | loss: 0.44165 | val_0_roc_auc: 0.87008 | val_0_accuracy: 0.78282 |  0:00:24s\n",
            "epoch 17 | loss: 0.43098 | val_0_roc_auc: 0.87227 | val_0_accuracy: 0.78208 |  0:00:26s\n",
            "epoch 18 | loss: 0.42253 | val_0_roc_auc: 0.87593 | val_0_accuracy: 0.79121 |  0:00:27s\n",
            "epoch 19 | loss: 0.41744 | val_0_roc_auc: 0.87726 | val_0_accuracy: 0.79121 |  0:00:29s\n",
            "epoch 20 | loss: 0.42313 | val_0_roc_auc: 0.88066 | val_0_accuracy: 0.79418 |  0:00:30s\n",
            "epoch 21 | loss: 0.41685 | val_0_roc_auc: 0.88368 | val_0_accuracy: 0.79516 |  0:00:32s\n",
            "epoch 22 | loss: 0.41079 | val_0_roc_auc: 0.8883  | val_0_accuracy: 0.80257 |  0:00:33s\n",
            "epoch 23 | loss: 0.39432 | val_0_roc_auc: 0.88718 | val_0_accuracy: 0.79961 |  0:00:35s\n",
            "epoch 24 | loss: 0.3902  | val_0_roc_auc: 0.8908  | val_0_accuracy: 0.8038  |  0:00:36s\n",
            "epoch 25 | loss: 0.39758 | val_0_roc_auc: 0.88936 | val_0_accuracy: 0.80281 |  0:00:37s\n",
            "epoch 26 | loss: 0.39215 | val_0_roc_auc: 0.89141 | val_0_accuracy: 0.79961 |  0:00:39s\n",
            "epoch 27 | loss: 0.38404 | val_0_roc_auc: 0.89791 | val_0_accuracy: 0.81417 |  0:00:40s\n",
            "epoch 28 | loss: 0.36997 | val_0_roc_auc: 0.9036  | val_0_accuracy: 0.8191  |  0:00:42s\n",
            "epoch 29 | loss: 0.36136 | val_0_roc_auc: 0.90478 | val_0_accuracy: 0.82083 |  0:00:43s\n",
            "epoch 30 | loss: 0.3757  | val_0_roc_auc: 0.89962 | val_0_accuracy: 0.81417 |  0:00:45s\n",
            "epoch 31 | loss: 0.37325 | val_0_roc_auc: 0.90349 | val_0_accuracy: 0.81392 |  0:00:46s\n",
            "epoch 32 | loss: 0.36124 | val_0_roc_auc: 0.91075 | val_0_accuracy: 0.83095 |  0:00:48s\n",
            "epoch 33 | loss: 0.34317 | val_0_roc_auc: 0.91265 | val_0_accuracy: 0.82502 |  0:00:49s\n",
            "epoch 34 | loss: 0.33519 | val_0_roc_auc: 0.91616 | val_0_accuracy: 0.83193 |  0:00:50s\n",
            "epoch 35 | loss: 0.35762 | val_0_roc_auc: 0.90751 | val_0_accuracy: 0.8233  |  0:00:52s\n",
            "epoch 36 | loss: 0.35069 | val_0_roc_auc: 0.91431 | val_0_accuracy: 0.83021 |  0:00:53s\n",
            "epoch 37 | loss: 0.33301 | val_0_roc_auc: 0.91709 | val_0_accuracy: 0.82897 |  0:00:55s\n",
            "epoch 38 | loss: 0.32066 | val_0_roc_auc: 0.92369 | val_0_accuracy: 0.84551 |  0:00:56s\n",
            "epoch 39 | loss: 0.31554 | val_0_roc_auc: 0.92561 | val_0_accuracy: 0.84427 |  0:00:58s\n",
            "epoch 40 | loss: 0.33236 | val_0_roc_auc: 0.91696 | val_0_accuracy: 0.83662 |  0:00:59s\n",
            "epoch 41 | loss: 0.33299 | val_0_roc_auc: 0.92149 | val_0_accuracy: 0.83983 |  0:01:01s\n",
            "epoch 42 | loss: 0.31874 | val_0_roc_auc: 0.92536 | val_0_accuracy: 0.84576 |  0:01:02s\n",
            "epoch 43 | loss: 0.30438 | val_0_roc_auc: 0.92885 | val_0_accuracy: 0.85094 |  0:01:03s\n",
            "epoch 44 | loss: 0.29489 | val_0_roc_auc: 0.9302  | val_0_accuracy: 0.84995 |  0:01:05s\n",
            "epoch 45 | loss: 0.32459 | val_0_roc_auc: 0.91752 | val_0_accuracy: 0.83144 |  0:01:06s\n",
            "epoch 46 | loss: 0.3224  | val_0_roc_auc: 0.91879 | val_0_accuracy: 0.83613 |  0:01:08s\n",
            "epoch 47 | loss: 0.30862 | val_0_roc_auc: 0.92677 | val_0_accuracy: 0.84773 |  0:01:09s\n",
            "epoch 48 | loss: 0.29388 | val_0_roc_auc: 0.93219 | val_0_accuracy: 0.85563 |  0:01:11s\n",
            "epoch 49 | loss: 0.27755 | val_0_roc_auc: 0.93557 | val_0_accuracy: 0.86155 |  0:01:12s\n",
            "epoch 50 | loss: 0.30601 | val_0_roc_auc: 0.92646 | val_0_accuracy: 0.84156 |  0:01:13s\n",
            "epoch 51 | loss: 0.3055  | val_0_roc_auc: 0.92604 | val_0_accuracy: 0.84181 |  0:01:15s\n",
            "epoch 52 | loss: 0.29377 | val_0_roc_auc: 0.93134 | val_0_accuracy: 0.85069 |  0:01:16s\n",
            "epoch 53 | loss: 0.27601 | val_0_roc_auc: 0.93786 | val_0_accuracy: 0.86673 |  0:01:18s\n",
            "epoch 54 | loss: 0.2591  | val_0_roc_auc: 0.93956 | val_0_accuracy: 0.86525 |  0:01:19s\n",
            "epoch 55 | loss: 0.29178 | val_0_roc_auc: 0.92931 | val_0_accuracy: 0.846   |  0:01:21s\n",
            "epoch 56 | loss: 0.29657 | val_0_roc_auc: 0.93363 | val_0_accuracy: 0.85464 |  0:01:23s\n",
            "epoch 57 | loss: 0.27787 | val_0_roc_auc: 0.93619 | val_0_accuracy: 0.85686 |  0:01:24s\n",
            "epoch 58 | loss: 0.25998 | val_0_roc_auc: 0.93914 | val_0_accuracy: 0.86377 |  0:01:26s\n",
            "epoch 59 | loss: 0.25375 | val_0_roc_auc: 0.94202 | val_0_accuracy: 0.87068 |  0:01:27s\n",
            "epoch 60 | loss: 0.27777 | val_0_roc_auc: 0.92442 | val_0_accuracy: 0.84452 |  0:01:29s\n",
            "epoch 61 | loss: 0.297   | val_0_roc_auc: 0.93435 | val_0_accuracy: 0.85884 |  0:01:30s\n",
            "epoch 62 | loss: 0.27272 | val_0_roc_auc: 0.93995 | val_0_accuracy: 0.86525 |  0:01:32s\n",
            "epoch 63 | loss: 0.25942 | val_0_roc_auc: 0.94245 | val_0_accuracy: 0.86673 |  0:01:33s\n",
            "epoch 64 | loss: 0.23908 | val_0_roc_auc: 0.94563 | val_0_accuracy: 0.87611 |  0:01:35s\n",
            "epoch 65 | loss: 0.26792 | val_0_roc_auc: 0.93117 | val_0_accuracy: 0.84452 |  0:01:36s\n",
            "epoch 66 | loss: 0.27153 | val_0_roc_auc: 0.93284 | val_0_accuracy: 0.8497  |  0:01:37s\n",
            "epoch 67 | loss: 0.25944 | val_0_roc_auc: 0.9436  | val_0_accuracy: 0.87043 |  0:01:39s\n",
            "epoch 68 | loss: 0.24524 | val_0_roc_auc: 0.94652 | val_0_accuracy: 0.87438 |  0:01:40s\n",
            "epoch 69 | loss: 0.22942 | val_0_roc_auc: 0.94947 | val_0_accuracy: 0.88105 |  0:01:42s\n",
            "epoch 70 | loss: 0.26518 | val_0_roc_auc: 0.93488 | val_0_accuracy: 0.86081 |  0:01:43s\n",
            "epoch 71 | loss: 0.26881 | val_0_roc_auc: 0.93749 | val_0_accuracy: 0.865   |  0:01:45s\n",
            "epoch 72 | loss: 0.25104 | val_0_roc_auc: 0.94507 | val_0_accuracy: 0.87463 |  0:01:46s\n",
            "epoch 73 | loss: 0.23216 | val_0_roc_auc: 0.94827 | val_0_accuracy: 0.87685 |  0:01:48s\n",
            "epoch 74 | loss: 0.21381 | val_0_roc_auc: 0.95018 | val_0_accuracy: 0.88401 |  0:01:49s\n",
            "epoch 75 | loss: 0.25652 | val_0_roc_auc: 0.93844 | val_0_accuracy: 0.86797 |  0:01:51s\n",
            "epoch 76 | loss: 0.25889 | val_0_roc_auc: 0.9379  | val_0_accuracy: 0.86254 |  0:01:52s\n",
            "epoch 77 | loss: 0.24601 | val_0_roc_auc: 0.94708 | val_0_accuracy: 0.87192 |  0:01:53s\n",
            "epoch 78 | loss: 0.22456 | val_0_roc_auc: 0.94915 | val_0_accuracy: 0.87883 |  0:01:55s\n",
            "epoch 79 | loss: 0.20945 | val_0_roc_auc: 0.95247 | val_0_accuracy: 0.88697 |  0:01:56s\n",
            "epoch 80 | loss: 0.2578  | val_0_roc_auc: 0.93763 | val_0_accuracy: 0.86797 |  0:01:58s\n",
            "epoch 81 | loss: 0.24634 | val_0_roc_auc: 0.94079 | val_0_accuracy: 0.865   |  0:01:59s\n",
            "epoch 82 | loss: 0.23585 | val_0_roc_auc: 0.94595 | val_0_accuracy: 0.87611 |  0:02:01s\n",
            "epoch 83 | loss: 0.21486 | val_0_roc_auc: 0.95175 | val_0_accuracy: 0.88598 |  0:02:02s\n",
            "epoch 84 | loss: 0.20276 | val_0_roc_auc: 0.95389 | val_0_accuracy: 0.88623 |  0:02:04s\n",
            "epoch 85 | loss: 0.24928 | val_0_roc_auc: 0.94068 | val_0_accuracy: 0.86377 |  0:02:05s\n",
            "epoch 86 | loss: 0.24859 | val_0_roc_auc: 0.94538 | val_0_accuracy: 0.87562 |  0:02:06s\n",
            "epoch 87 | loss: 0.2322  | val_0_roc_auc: 0.94817 | val_0_accuracy: 0.88203 |  0:02:08s\n",
            "epoch 88 | loss: 0.21319 | val_0_roc_auc: 0.95164 | val_0_accuracy: 0.88425 |  0:02:09s\n",
            "epoch 89 | loss: 0.19401 | val_0_roc_auc: 0.95454 | val_0_accuracy: 0.88771 |  0:02:11s\n",
            "epoch 90 | loss: 0.23439 | val_0_roc_auc: 0.94297 | val_0_accuracy: 0.86846 |  0:02:12s\n",
            "epoch 91 | loss: 0.25009 | val_0_roc_auc: 0.93994 | val_0_accuracy: 0.86525 |  0:02:14s\n",
            "epoch 92 | loss: 0.22568 | val_0_roc_auc: 0.94669 | val_0_accuracy: 0.8766  |  0:02:15s\n",
            "epoch 93 | loss: 0.21247 | val_0_roc_auc: 0.94999 | val_0_accuracy: 0.88105 |  0:02:16s\n",
            "epoch 94 | loss: 0.19616 | val_0_roc_auc: 0.95422 | val_0_accuracy: 0.88993 |  0:02:18s\n",
            "epoch 95 | loss: 0.23303 | val_0_roc_auc: 0.93955 | val_0_accuracy: 0.86772 |  0:02:19s\n",
            "epoch 96 | loss: 0.24271 | val_0_roc_auc: 0.94585 | val_0_accuracy: 0.87636 |  0:02:21s\n",
            "epoch 97 | loss: 0.22246 | val_0_roc_auc: 0.94983 | val_0_accuracy: 0.88105 |  0:02:22s\n",
            "epoch 98 | loss: 0.20451 | val_0_roc_auc: 0.95453 | val_0_accuracy: 0.8882  |  0:02:24s\n",
            "epoch 99 | loss: 0.18439 | val_0_roc_auc: 0.95462 | val_0_accuracy: 0.88549 |  0:02:25s\n",
            "epoch 100| loss: 0.22775 | val_0_roc_auc: 0.93955 | val_0_accuracy: 0.86846 |  0:02:27s\n",
            "epoch 101| loss: 0.23603 | val_0_roc_auc: 0.94867 | val_0_accuracy: 0.87562 |  0:02:28s\n",
            "epoch 102| loss: 0.22195 | val_0_roc_auc: 0.95136 | val_0_accuracy: 0.88746 |  0:02:29s\n",
            "epoch 103| loss: 0.19849 | val_0_roc_auc: 0.95541 | val_0_accuracy: 0.89215 |  0:02:31s\n",
            "epoch 104| loss: 0.18121 | val_0_roc_auc: 0.95663 | val_0_accuracy: 0.89215 |  0:02:32s\n",
            "epoch 105| loss: 0.21755 | val_0_roc_auc: 0.94912 | val_0_accuracy: 0.88055 |  0:02:34s\n",
            "epoch 106| loss: 0.22217 | val_0_roc_auc: 0.94176 | val_0_accuracy: 0.87192 |  0:02:35s\n",
            "epoch 107| loss: 0.22297 | val_0_roc_auc: 0.95027 | val_0_accuracy: 0.88623 |  0:02:37s\n",
            "epoch 108| loss: 0.19606 | val_0_roc_auc: 0.9503  | val_0_accuracy: 0.88376 |  0:02:38s\n",
            "epoch 109| loss: 0.18109 | val_0_roc_auc: 0.9556  | val_0_accuracy: 0.89388 |  0:02:40s\n",
            "epoch 110| loss: 0.21558 | val_0_roc_auc: 0.94148 | val_0_accuracy: 0.86871 |  0:02:41s\n",
            "epoch 111| loss: 0.22603 | val_0_roc_auc: 0.94649 | val_0_accuracy: 0.87068 |  0:02:42s\n",
            "epoch 112| loss: 0.20451 | val_0_roc_auc: 0.9494  | val_0_accuracy: 0.88253 |  0:02:44s\n",
            "epoch 113| loss: 0.1914  | val_0_roc_auc: 0.95287 | val_0_accuracy: 0.88771 |  0:02:45s\n",
            "epoch 114| loss: 0.17048 | val_0_roc_auc: 0.95705 | val_0_accuracy: 0.89561 |  0:02:47s\n",
            "epoch 115| loss: 0.21963 | val_0_roc_auc: 0.94297 | val_0_accuracy: 0.87043 |  0:02:48s\n",
            "epoch 116| loss: 0.22331 | val_0_roc_auc: 0.94176 | val_0_accuracy: 0.86846 |  0:02:50s\n",
            "epoch 117| loss: 0.20684 | val_0_roc_auc: 0.95038 | val_0_accuracy: 0.88253 |  0:02:51s\n",
            "epoch 118| loss: 0.18593 | val_0_roc_auc: 0.95297 | val_0_accuracy: 0.88894 |  0:02:53s\n",
            "epoch 119| loss: 0.16869 | val_0_roc_auc: 0.957   | val_0_accuracy: 0.89413 |  0:02:54s\n",
            "epoch 120| loss: 0.21132 | val_0_roc_auc: 0.94372 | val_0_accuracy: 0.86821 |  0:02:55s\n",
            "epoch 121| loss: 0.21851 | val_0_roc_auc: 0.9474  | val_0_accuracy: 0.88401 |  0:02:57s\n",
            "epoch 122| loss: 0.20385 | val_0_roc_auc: 0.95144 | val_0_accuracy: 0.88697 |  0:02:58s\n",
            "epoch 123| loss: 0.18643 | val_0_roc_auc: 0.95559 | val_0_accuracy: 0.89487 |  0:03:00s\n",
            "epoch 124| loss: 0.16477 | val_0_roc_auc: 0.95803 | val_0_accuracy: 0.90153 |  0:03:01s\n",
            "epoch 125| loss: 0.21963 | val_0_roc_auc: 0.93831 | val_0_accuracy: 0.86254 |  0:03:03s\n",
            "epoch 126| loss: 0.21945 | val_0_roc_auc: 0.94909 | val_0_accuracy: 0.88203 |  0:03:05s\n",
            "epoch 127| loss: 0.20331 | val_0_roc_auc: 0.95389 | val_0_accuracy: 0.88746 |  0:03:07s\n",
            "epoch 128| loss: 0.18611 | val_0_roc_auc: 0.9569  | val_0_accuracy: 0.89906 |  0:03:08s\n",
            "epoch 129| loss: 0.16293 | val_0_roc_auc: 0.95901 | val_0_accuracy: 0.89956 |  0:03:09s\n",
            "epoch 130| loss: 0.20425 | val_0_roc_auc: 0.94192 | val_0_accuracy: 0.86871 |  0:03:11s\n",
            "epoch 131| loss: 0.2262  | val_0_roc_auc: 0.94959 | val_0_accuracy: 0.88351 |  0:03:12s\n",
            "epoch 132| loss: 0.20102 | val_0_roc_auc: 0.95628 | val_0_accuracy: 0.89487 |  0:03:14s\n",
            "epoch 133| loss: 0.17706 | val_0_roc_auc: 0.95973 | val_0_accuracy: 0.89956 |  0:03:15s\n",
            "epoch 134| loss: 0.15853 | val_0_roc_auc: 0.96112 | val_0_accuracy: 0.9035  |  0:03:17s\n",
            "epoch 135| loss: 0.20632 | val_0_roc_auc: 0.94929 | val_0_accuracy: 0.88327 |  0:03:18s\n",
            "epoch 136| loss: 0.20852 | val_0_roc_auc: 0.94917 | val_0_accuracy: 0.87957 |  0:03:19s\n",
            "epoch 137| loss: 0.19297 | val_0_roc_auc: 0.95074 | val_0_accuracy: 0.88894 |  0:03:21s\n",
            "epoch 138| loss: 0.17766 | val_0_roc_auc: 0.95788 | val_0_accuracy: 0.89906 |  0:03:22s\n",
            "epoch 139| loss: 0.16031 | val_0_roc_auc: 0.96069 | val_0_accuracy: 0.90178 |  0:03:24s\n",
            "epoch 140| loss: 0.20651 | val_0_roc_auc: 0.94303 | val_0_accuracy: 0.8729  |  0:03:25s\n",
            "epoch 141| loss: 0.20464 | val_0_roc_auc: 0.94829 | val_0_accuracy: 0.87957 |  0:03:27s\n",
            "epoch 142| loss: 0.19298 | val_0_roc_auc: 0.95337 | val_0_accuracy: 0.88845 |  0:03:28s\n",
            "epoch 143| loss: 0.17208 | val_0_roc_auc: 0.95792 | val_0_accuracy: 0.89511 |  0:03:30s\n",
            "epoch 144| loss: 0.15363 | val_0_roc_auc: 0.95956 | val_0_accuracy: 0.90202 |  0:03:31s\n",
            "epoch 145| loss: 0.20063 | val_0_roc_auc: 0.94786 | val_0_accuracy: 0.88006 |  0:03:33s\n",
            "epoch 146| loss: 0.21394 | val_0_roc_auc: 0.94972 | val_0_accuracy: 0.88401 |  0:03:34s\n",
            "epoch 147| loss: 0.18923 | val_0_roc_auc: 0.95272 | val_0_accuracy: 0.89289 |  0:03:35s\n",
            "epoch 148| loss: 0.17172 | val_0_roc_auc: 0.95988 | val_0_accuracy: 0.8998  |  0:03:37s\n",
            "epoch 149| loss: 0.15093 | val_0_roc_auc: 0.9614  | val_0_accuracy: 0.90671 |  0:03:38s\n",
            "epoch 150| loss: 0.19627 | val_0_roc_auc: 0.94703 | val_0_accuracy: 0.88425 |  0:03:40s\n",
            "epoch 151| loss: 0.20544 | val_0_roc_auc: 0.95268 | val_0_accuracy: 0.88697 |  0:03:41s\n",
            "epoch 152| loss: 0.1795  | val_0_roc_auc: 0.95463 | val_0_accuracy: 0.8887  |  0:03:43s\n",
            "epoch 153| loss: 0.16292 | val_0_roc_auc: 0.95809 | val_0_accuracy: 0.89931 |  0:03:44s\n",
            "epoch 154| loss: 0.14377 | val_0_roc_auc: 0.96165 | val_0_accuracy: 0.90523 |  0:03:46s\n",
            "epoch 155| loss: 0.19124 | val_0_roc_auc: 0.9451  | val_0_accuracy: 0.87883 |  0:03:47s\n",
            "epoch 156| loss: 0.19928 | val_0_roc_auc: 0.95209 | val_0_accuracy: 0.89018 |  0:03:48s\n",
            "epoch 157| loss: 0.18255 | val_0_roc_auc: 0.95489 | val_0_accuracy: 0.8882  |  0:03:50s\n",
            "epoch 158| loss: 0.1629  | val_0_roc_auc: 0.95709 | val_0_accuracy: 0.90005 |  0:03:51s\n",
            "epoch 159| loss: 0.14636 | val_0_roc_auc: 0.96138 | val_0_accuracy: 0.90696 |  0:03:53s\n",
            "epoch 160| loss: 0.18995 | val_0_roc_auc: 0.94992 | val_0_accuracy: 0.88672 |  0:03:54s\n",
            "epoch 161| loss: 0.20588 | val_0_roc_auc: 0.95446 | val_0_accuracy: 0.89018 |  0:03:56s\n",
            "epoch 162| loss: 0.18238 | val_0_roc_auc: 0.95423 | val_0_accuracy: 0.89092 |  0:03:57s\n",
            "epoch 163| loss: 0.16276 | val_0_roc_auc: 0.95907 | val_0_accuracy: 0.904   |  0:03:59s\n",
            "epoch 164| loss: 0.14647 | val_0_roc_auc: 0.96198 | val_0_accuracy: 0.90745 |  0:04:00s\n",
            "epoch 165| loss: 0.19295 | val_0_roc_auc: 0.95348 | val_0_accuracy: 0.89116 |  0:04:01s\n",
            "epoch 166| loss: 0.19442 | val_0_roc_auc: 0.95315 | val_0_accuracy: 0.8808  |  0:04:03s\n",
            "epoch 167| loss: 0.18021 | val_0_roc_auc: 0.95638 | val_0_accuracy: 0.8961  |  0:04:04s\n",
            "epoch 168| loss: 0.1532  | val_0_roc_auc: 0.96103 | val_0_accuracy: 0.90375 |  0:04:06s\n",
            "epoch 169| loss: 0.137   | val_0_roc_auc: 0.96302 | val_0_accuracy: 0.91066 |  0:04:07s\n",
            "epoch 170| loss: 0.188   | val_0_roc_auc: 0.94903 | val_0_accuracy: 0.89166 |  0:04:09s\n",
            "epoch 171| loss: 0.19656 | val_0_roc_auc: 0.95376 | val_0_accuracy: 0.89314 |  0:04:10s\n",
            "epoch 172| loss: 0.18145 | val_0_roc_auc: 0.9529  | val_0_accuracy: 0.89561 |  0:04:12s\n",
            "epoch 173| loss: 0.16074 | val_0_roc_auc: 0.95824 | val_0_accuracy: 0.90252 |  0:04:13s\n",
            "epoch 174| loss: 0.13608 | val_0_roc_auc: 0.96116 | val_0_accuracy: 0.90721 |  0:04:14s\n",
            "epoch 175| loss: 0.18356 | val_0_roc_auc: 0.9519  | val_0_accuracy: 0.88771 |  0:04:16s\n",
            "epoch 176| loss: 0.19536 | val_0_roc_auc: 0.95381 | val_0_accuracy: 0.88968 |  0:04:17s\n",
            "epoch 177| loss: 0.18149 | val_0_roc_auc: 0.95823 | val_0_accuracy: 0.89956 |  0:04:19s\n",
            "epoch 178| loss: 0.15675 | val_0_roc_auc: 0.95583 | val_0_accuracy: 0.89733 |  0:04:20s\n",
            "epoch 179| loss: 0.13386 | val_0_roc_auc: 0.96089 | val_0_accuracy: 0.90449 |  0:04:22s\n",
            "epoch 180| loss: 0.18444 | val_0_roc_auc: 0.94908 | val_0_accuracy: 0.87981 |  0:04:23s\n",
            "epoch 181| loss: 0.20161 | val_0_roc_auc: 0.95274 | val_0_accuracy: 0.89141 |  0:04:24s\n",
            "epoch 182| loss: 0.18103 | val_0_roc_auc: 0.95674 | val_0_accuracy: 0.89733 |  0:04:26s\n",
            "epoch 183| loss: 0.15264 | val_0_roc_auc: 0.95968 | val_0_accuracy: 0.90424 |  0:04:27s\n",
            "epoch 184| loss: 0.13718 | val_0_roc_auc: 0.96256 | val_0_accuracy: 0.90819 |  0:04:29s\n",
            "epoch 185| loss: 0.18135 | val_0_roc_auc: 0.94407 | val_0_accuracy: 0.87784 |  0:04:30s\n",
            "epoch 186| loss: 0.20196 | val_0_roc_auc: 0.95557 | val_0_accuracy: 0.89141 |  0:04:32s\n",
            "epoch 187| loss: 0.17456 | val_0_roc_auc: 0.95761 | val_0_accuracy: 0.90227 |  0:04:33s\n",
            "epoch 188| loss: 0.15702 | val_0_roc_auc: 0.96064 | val_0_accuracy: 0.904   |  0:04:35s\n",
            "epoch 189| loss: 0.13445 | val_0_roc_auc: 0.96247 | val_0_accuracy: 0.91017 |  0:04:36s\n",
            "epoch 190| loss: 0.17304 | val_0_roc_auc: 0.95246 | val_0_accuracy: 0.8887  |  0:04:37s\n",
            "epoch 191| loss: 0.18757 | val_0_roc_auc: 0.95544 | val_0_accuracy: 0.89758 |  0:04:39s\n",
            "epoch 192| loss: 0.16857 | val_0_roc_auc: 0.95792 | val_0_accuracy: 0.90104 |  0:04:40s\n",
            "epoch 193| loss: 0.14699 | val_0_roc_auc: 0.96037 | val_0_accuracy: 0.90573 |  0:04:42s\n",
            "epoch 194| loss: 0.13129 | val_0_roc_auc: 0.96283 | val_0_accuracy: 0.90869 |  0:04:43s\n",
            "epoch 195| loss: 0.1812  | val_0_roc_auc: 0.95108 | val_0_accuracy: 0.88401 |  0:04:45s\n",
            "epoch 196| loss: 0.18786 | val_0_roc_auc: 0.95341 | val_0_accuracy: 0.89092 |  0:04:47s\n",
            "epoch 197| loss: 0.16772 | val_0_roc_auc: 0.95826 | val_0_accuracy: 0.89857 |  0:04:49s\n",
            "epoch 198| loss: 0.14724 | val_0_roc_auc: 0.96544 | val_0_accuracy: 0.91165 |  0:04:50s\n",
            "epoch 199| loss: 0.12639 | val_0_roc_auc: 0.96541 | val_0_accuracy: 0.91041 |  0:04:51s\n",
            "epoch 200| loss: 0.17316 | val_0_roc_auc: 0.95298 | val_0_accuracy: 0.89437 |  0:04:53s\n",
            "epoch 201| loss: 0.18265 | val_0_roc_auc: 0.95597 | val_0_accuracy: 0.90227 |  0:04:54s\n",
            "epoch 202| loss: 0.16777 | val_0_roc_auc: 0.95777 | val_0_accuracy: 0.90671 |  0:04:56s\n",
            "epoch 203| loss: 0.14621 | val_0_roc_auc: 0.961   | val_0_accuracy: 0.90992 |  0:04:57s\n",
            "epoch 204| loss: 0.12473 | val_0_roc_auc: 0.96244 | val_0_accuracy: 0.91387 |  0:04:59s\n",
            "epoch 205| loss: 0.17781 | val_0_roc_auc: 0.95078 | val_0_accuracy: 0.88697 |  0:05:00s\n",
            "epoch 206| loss: 0.19125 | val_0_roc_auc: 0.95466 | val_0_accuracy: 0.89042 |  0:05:02s\n",
            "epoch 207| loss: 0.16908 | val_0_roc_auc: 0.95961 | val_0_accuracy: 0.91017 |  0:05:03s\n",
            "epoch 208| loss: 0.14327 | val_0_roc_auc: 0.96301 | val_0_accuracy: 0.91264 |  0:05:05s\n",
            "epoch 209| loss: 0.12725 | val_0_roc_auc: 0.96392 | val_0_accuracy: 0.9151  |  0:05:06s\n",
            "epoch 210| loss: 0.17477 | val_0_roc_auc: 0.95397 | val_0_accuracy: 0.88993 |  0:05:08s\n",
            "epoch 211| loss: 0.18408 | val_0_roc_auc: 0.95785 | val_0_accuracy: 0.90005 |  0:05:09s\n",
            "epoch 212| loss: 0.16395 | val_0_roc_auc: 0.95879 | val_0_accuracy: 0.90449 |  0:05:10s\n",
            "epoch 213| loss: 0.14054 | val_0_roc_auc: 0.96376 | val_0_accuracy: 0.91165 |  0:05:12s\n",
            "epoch 214| loss: 0.12321 | val_0_roc_auc: 0.96586 | val_0_accuracy: 0.91831 |  0:05:13s\n",
            "epoch 215| loss: 0.17196 | val_0_roc_auc: 0.94819 | val_0_accuracy: 0.88154 |  0:05:15s\n",
            "epoch 216| loss: 0.19221 | val_0_roc_auc: 0.95631 | val_0_accuracy: 0.89141 |  0:05:16s\n",
            "epoch 217| loss: 0.15549 | val_0_roc_auc: 0.95879 | val_0_accuracy: 0.904   |  0:05:18s\n",
            "epoch 218| loss: 0.14106 | val_0_roc_auc: 0.96348 | val_0_accuracy: 0.91091 |  0:05:19s\n",
            "epoch 219| loss: 0.12915 | val_0_roc_auc: 0.96429 | val_0_accuracy: 0.91461 |  0:05:21s\n",
            "epoch 220| loss: 0.15982 | val_0_roc_auc: 0.95834 | val_0_accuracy: 0.89783 |  0:05:22s\n",
            "epoch 221| loss: 0.18114 | val_0_roc_auc: 0.9521  | val_0_accuracy: 0.89561 |  0:05:24s\n",
            "epoch 222| loss: 0.16714 | val_0_roc_auc: 0.95777 | val_0_accuracy: 0.90079 |  0:05:25s\n",
            "epoch 223| loss: 0.14057 | val_0_roc_auc: 0.96227 | val_0_accuracy: 0.9114  |  0:05:27s\n",
            "epoch 224| loss: 0.12509 | val_0_roc_auc: 0.96517 | val_0_accuracy: 0.91732 |  0:05:28s\n",
            "epoch 225| loss: 0.16822 | val_0_roc_auc: 0.95042 | val_0_accuracy: 0.8887  |  0:05:30s\n",
            "epoch 226| loss: 0.1806  | val_0_roc_auc: 0.95573 | val_0_accuracy: 0.89339 |  0:05:31s\n",
            "epoch 227| loss: 0.15873 | val_0_roc_auc: 0.9593  | val_0_accuracy: 0.90992 |  0:05:32s\n",
            "epoch 228| loss: 0.13104 | val_0_roc_auc: 0.96369 | val_0_accuracy: 0.91362 |  0:05:34s\n",
            "epoch 229| loss: 0.12495 | val_0_roc_auc: 0.96457 | val_0_accuracy: 0.91856 |  0:05:35s\n",
            "epoch 230| loss: 0.16471 | val_0_roc_auc: 0.95447 | val_0_accuracy: 0.89018 |  0:05:37s\n",
            "epoch 231| loss: 0.17681 | val_0_roc_auc: 0.95305 | val_0_accuracy: 0.89363 |  0:05:38s\n",
            "epoch 232| loss: 0.15559 | val_0_roc_auc: 0.96079 | val_0_accuracy: 0.91362 |  0:05:40s\n",
            "epoch 233| loss: 0.13155 | val_0_roc_auc: 0.96419 | val_0_accuracy: 0.91831 |  0:05:41s\n",
            "epoch 234| loss: 0.12043 | val_0_roc_auc: 0.96458 | val_0_accuracy: 0.91905 |  0:05:43s\n",
            "epoch 235| loss: 0.16795 | val_0_roc_auc: 0.95189 | val_0_accuracy: 0.89215 |  0:05:44s\n",
            "epoch 236| loss: 0.17171 | val_0_roc_auc: 0.94978 | val_0_accuracy: 0.89388 |  0:05:46s\n",
            "epoch 237| loss: 0.16008 | val_0_roc_auc: 0.95875 | val_0_accuracy: 0.90375 |  0:05:47s\n",
            "epoch 238| loss: 0.13623 | val_0_roc_auc: 0.96203 | val_0_accuracy: 0.91214 |  0:05:48s\n",
            "epoch 239| loss: 0.11952 | val_0_roc_auc: 0.96356 | val_0_accuracy: 0.91609 |  0:05:50s\n",
            "epoch 240| loss: 0.16977 | val_0_roc_auc: 0.95119 | val_0_accuracy: 0.89141 |  0:05:51s\n",
            "epoch 241| loss: 0.17775 | val_0_roc_auc: 0.95418 | val_0_accuracy: 0.89413 |  0:05:53s\n",
            "epoch 242| loss: 0.15523 | val_0_roc_auc: 0.96317 | val_0_accuracy: 0.91264 |  0:05:54s\n",
            "epoch 243| loss: 0.13505 | val_0_roc_auc: 0.96496 | val_0_accuracy: 0.91831 |  0:05:56s\n",
            "epoch 244| loss: 0.11638 | val_0_roc_auc: 0.96505 | val_0_accuracy: 0.91683 |  0:05:57s\n",
            "epoch 245| loss: 0.1494  | val_0_roc_auc: 0.9527  | val_0_accuracy: 0.89536 |  0:05:58s\n",
            "epoch 246| loss: 0.16725 | val_0_roc_auc: 0.95317 | val_0_accuracy: 0.89363 |  0:06:00s\n",
            "epoch 247| loss: 0.14756 | val_0_roc_auc: 0.95928 | val_0_accuracy: 0.90276 |  0:06:01s\n",
            "epoch 248| loss: 0.13377 | val_0_roc_auc: 0.96331 | val_0_accuracy: 0.91757 |  0:06:03s\n",
            "epoch 249| loss: 0.11816 | val_0_roc_auc: 0.9647  | val_0_accuracy: 0.91658 |  0:06:04s\n",
            "epoch 250| loss: 0.15582 | val_0_roc_auc: 0.95454 | val_0_accuracy: 0.89832 |  0:06:06s\n",
            "epoch 251| loss: 0.1694  | val_0_roc_auc: 0.95988 | val_0_accuracy: 0.90795 |  0:06:07s\n",
            "epoch 252| loss: 0.14734 | val_0_roc_auc: 0.96212 | val_0_accuracy: 0.91905 |  0:06:08s\n",
            "epoch 253| loss: 0.1232  | val_0_roc_auc: 0.96291 | val_0_accuracy: 0.91683 |  0:06:10s\n",
            "epoch 254| loss: 0.10937 | val_0_roc_auc: 0.96598 | val_0_accuracy: 0.923   |  0:06:11s\n",
            "epoch 255| loss: 0.15654 | val_0_roc_auc: 0.95553 | val_0_accuracy: 0.89808 |  0:06:13s\n",
            "epoch 256| loss: 0.17432 | val_0_roc_auc: 0.95674 | val_0_accuracy: 0.89215 |  0:06:14s\n",
            "epoch 257| loss: 0.14393 | val_0_roc_auc: 0.96231 | val_0_accuracy: 0.91017 |  0:06:16s\n",
            "epoch 258| loss: 0.12596 | val_0_roc_auc: 0.96196 | val_0_accuracy: 0.91412 |  0:06:17s\n",
            "epoch 259| loss: 0.10919 | val_0_roc_auc: 0.96485 | val_0_accuracy: 0.91732 |  0:06:18s\n",
            "epoch 260| loss: 0.15659 | val_0_roc_auc: 0.95434 | val_0_accuracy: 0.89585 |  0:06:20s\n",
            "epoch 261| loss: 0.16455 | val_0_roc_auc: 0.95577 | val_0_accuracy: 0.8998  |  0:06:21s\n",
            "epoch 262| loss: 0.14301 | val_0_roc_auc: 0.9625  | val_0_accuracy: 0.91017 |  0:06:23s\n",
            "epoch 263| loss: 0.12267 | val_0_roc_auc: 0.96418 | val_0_accuracy: 0.9156  |  0:06:25s\n",
            "epoch 264| loss: 0.11247 | val_0_roc_auc: 0.96561 | val_0_accuracy: 0.92029 |  0:06:27s\n",
            "epoch 265| loss: 0.15672 | val_0_roc_auc: 0.95516 | val_0_accuracy: 0.89733 |  0:06:28s\n",
            "epoch 266| loss: 0.15511 | val_0_roc_auc: 0.95665 | val_0_accuracy: 0.90276 |  0:06:29s\n",
            "epoch 267| loss: 0.14995 | val_0_roc_auc: 0.95912 | val_0_accuracy: 0.90202 |  0:06:31s\n",
            "epoch 268| loss: 0.12413 | val_0_roc_auc: 0.9646  | val_0_accuracy: 0.91708 |  0:06:32s\n",
            "epoch 269| loss: 0.10526 | val_0_roc_auc: 0.96596 | val_0_accuracy: 0.92053 |  0:06:34s\n",
            "epoch 270| loss: 0.16059 | val_0_roc_auc: 0.95462 | val_0_accuracy: 0.8961  |  0:06:35s\n",
            "epoch 271| loss: 0.17103 | val_0_roc_auc: 0.95768 | val_0_accuracy: 0.89857 |  0:06:36s\n",
            "epoch 272| loss: 0.14266 | val_0_roc_auc: 0.9585  | val_0_accuracy: 0.90745 |  0:06:38s\n",
            "epoch 273| loss: 0.11905 | val_0_roc_auc: 0.96401 | val_0_accuracy: 0.92004 |  0:06:39s\n",
            "epoch 274| loss: 0.10689 | val_0_roc_auc: 0.9662  | val_0_accuracy: 0.91856 |  0:06:41s\n",
            "epoch 275| loss: 0.14884 | val_0_roc_auc: 0.95755 | val_0_accuracy: 0.90375 |  0:06:42s\n",
            "epoch 276| loss: 0.1632  | val_0_roc_auc: 0.95627 | val_0_accuracy: 0.9003  |  0:06:44s\n",
            "epoch 277| loss: 0.14692 | val_0_roc_auc: 0.96234 | val_0_accuracy: 0.91041 |  0:06:45s\n",
            "epoch 278| loss: 0.12434 | val_0_roc_auc: 0.96697 | val_0_accuracy: 0.92201 |  0:06:46s\n",
            "epoch 279| loss: 0.10841 | val_0_roc_auc: 0.96738 | val_0_accuracy: 0.92399 |  0:06:48s\n",
            "epoch 280| loss: 0.15273 | val_0_roc_auc: 0.95615 | val_0_accuracy: 0.89561 |  0:06:49s\n",
            "epoch 281| loss: 0.15704 | val_0_roc_auc: 0.96014 | val_0_accuracy: 0.9077  |  0:06:51s\n",
            "epoch 282| loss: 0.13867 | val_0_roc_auc: 0.96419 | val_0_accuracy: 0.91313 |  0:06:52s\n",
            "epoch 283| loss: 0.12282 | val_0_roc_auc: 0.96531 | val_0_accuracy: 0.91708 |  0:06:54s\n",
            "epoch 284| loss: 0.10463 | val_0_roc_auc: 0.96602 | val_0_accuracy: 0.92078 |  0:06:55s\n",
            "epoch 285| loss: 0.14288 | val_0_roc_auc: 0.95425 | val_0_accuracy: 0.89783 |  0:06:57s\n",
            "epoch 286| loss: 0.16119 | val_0_roc_auc: 0.95496 | val_0_accuracy: 0.89832 |  0:06:58s\n",
            "epoch 287| loss: 0.13403 | val_0_roc_auc: 0.96212 | val_0_accuracy: 0.91115 |  0:06:59s\n",
            "epoch 288| loss: 0.12382 | val_0_roc_auc: 0.96438 | val_0_accuracy: 0.92029 |  0:07:01s\n",
            "epoch 289| loss: 0.1039  | val_0_roc_auc: 0.96582 | val_0_accuracy: 0.91831 |  0:07:02s\n",
            "epoch 290| loss: 0.13988 | val_0_roc_auc: 0.9546  | val_0_accuracy: 0.89635 |  0:07:04s\n",
            "epoch 291| loss: 0.16753 | val_0_roc_auc: 0.95894 | val_0_accuracy: 0.90202 |  0:07:05s\n",
            "epoch 292| loss: 0.14442 | val_0_roc_auc: 0.96112 | val_0_accuracy: 0.90943 |  0:07:07s\n",
            "epoch 293| loss: 0.11686 | val_0_roc_auc: 0.9654  | val_0_accuracy: 0.92029 |  0:07:08s\n",
            "epoch 294| loss: 0.10264 | val_0_roc_auc: 0.96592 | val_0_accuracy: 0.92004 |  0:07:10s\n",
            "epoch 295| loss: 0.13948 | val_0_roc_auc: 0.95858 | val_0_accuracy: 0.90721 |  0:07:11s\n",
            "epoch 296| loss: 0.14938 | val_0_roc_auc: 0.95978 | val_0_accuracy: 0.90573 |  0:07:12s\n",
            "epoch 297| loss: 0.13622 | val_0_roc_auc: 0.96148 | val_0_accuracy: 0.90474 |  0:07:14s\n",
            "epoch 298| loss: 0.11226 | val_0_roc_auc: 0.96469 | val_0_accuracy: 0.9193  |  0:07:15s\n",
            "epoch 299| loss: 0.10262 | val_0_roc_auc: 0.96715 | val_0_accuracy: 0.92226 |  0:07:17s\n",
            "epoch 300| loss: 0.14545 | val_0_roc_auc: 0.95599 | val_0_accuracy: 0.89191 |  0:07:18s\n",
            "epoch 301| loss: 0.16283 | val_0_roc_auc: 0.95668 | val_0_accuracy: 0.90326 |  0:07:20s\n",
            "epoch 302| loss: 0.13791 | val_0_roc_auc: 0.96224 | val_0_accuracy: 0.91091 |  0:07:21s\n",
            "epoch 303| loss: 0.1205  | val_0_roc_auc: 0.96644 | val_0_accuracy: 0.91905 |  0:07:22s\n",
            "epoch 304| loss: 0.10664 | val_0_roc_auc: 0.96754 | val_0_accuracy: 0.92078 |  0:07:24s\n",
            "epoch 305| loss: 0.13811 | val_0_roc_auc: 0.95869 | val_0_accuracy: 0.90054 |  0:07:25s\n",
            "epoch 306| loss: 0.15458 | val_0_roc_auc: 0.9624  | val_0_accuracy: 0.90671 |  0:07:27s\n",
            "epoch 307| loss: 0.13826 | val_0_roc_auc: 0.9627  | val_0_accuracy: 0.91041 |  0:07:28s\n",
            "epoch 308| loss: 0.11744 | val_0_roc_auc: 0.96667 | val_0_accuracy: 0.92201 |  0:07:30s\n",
            "epoch 309| loss: 0.1029  | val_0_roc_auc: 0.96784 | val_0_accuracy: 0.92201 |  0:07:31s\n",
            "epoch 310| loss: 0.13951 | val_0_roc_auc: 0.96139 | val_0_accuracy: 0.90375 |  0:07:32s\n",
            "epoch 311| loss: 0.15318 | val_0_roc_auc: 0.95885 | val_0_accuracy: 0.8998  |  0:07:34s\n",
            "epoch 312| loss: 0.13431 | val_0_roc_auc: 0.96279 | val_0_accuracy: 0.91436 |  0:07:35s\n",
            "epoch 313| loss: 0.11507 | val_0_roc_auc: 0.9642  | val_0_accuracy: 0.91634 |  0:07:37s\n",
            "epoch 314| loss: 0.09794 | val_0_roc_auc: 0.9667  | val_0_accuracy: 0.91955 |  0:07:38s\n",
            "epoch 315| loss: 0.14365 | val_0_roc_auc: 0.95692 | val_0_accuracy: 0.90128 |  0:07:40s\n",
            "epoch 316| loss: 0.15729 | val_0_roc_auc: 0.95765 | val_0_accuracy: 0.90153 |  0:07:41s\n",
            "epoch 317| loss: 0.13677 | val_0_roc_auc: 0.96546 | val_0_accuracy: 0.91881 |  0:07:43s\n",
            "epoch 318| loss: 0.11332 | val_0_roc_auc: 0.96818 | val_0_accuracy: 0.92349 |  0:07:44s\n",
            "epoch 319| loss: 0.09844 | val_0_roc_auc: 0.96813 | val_0_accuracy: 0.92251 |  0:07:45s\n",
            "epoch 320| loss: 0.13633 | val_0_roc_auc: 0.9581  | val_0_accuracy: 0.9003  |  0:07:47s\n",
            "epoch 321| loss: 0.14913 | val_0_roc_auc: 0.95867 | val_0_accuracy: 0.90869 |  0:07:48s\n",
            "epoch 322| loss: 0.13388 | val_0_roc_auc: 0.95977 | val_0_accuracy: 0.90301 |  0:07:50s\n",
            "epoch 323| loss: 0.11602 | val_0_roc_auc: 0.96688 | val_0_accuracy: 0.91831 |  0:07:51s\n",
            "epoch 324| loss: 0.10186 | val_0_roc_auc: 0.96723 | val_0_accuracy: 0.91757 |  0:07:53s\n",
            "epoch 325| loss: 0.13676 | val_0_roc_auc: 0.95527 | val_0_accuracy: 0.89289 |  0:07:54s\n",
            "epoch 326| loss: 0.14941 | val_0_roc_auc: 0.95777 | val_0_accuracy: 0.89956 |  0:07:55s\n",
            "epoch 327| loss: 0.13141 | val_0_roc_auc: 0.96074 | val_0_accuracy: 0.90869 |  0:07:57s\n",
            "epoch 328| loss: 0.11361 | val_0_roc_auc: 0.9638  | val_0_accuracy: 0.91584 |  0:07:58s\n",
            "epoch 329| loss: 0.09666 | val_0_roc_auc: 0.96553 | val_0_accuracy: 0.92053 |  0:08:00s\n",
            "\n",
            "Early stopping occurred at epoch 329 with best_epoch = 279 and best_val_0_accuracy = 0.92399\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KFold 4 of 5\n",
            "epoch 0  | loss: 0.80072 | val_0_roc_auc: 0.71863 | val_0_accuracy: 0.64635 |  0:00:01s\n",
            "epoch 1  | loss: 0.61358 | val_0_roc_auc: 0.76786 | val_0_accuracy: 0.69447 |  0:00:02s\n",
            "epoch 2  | loss: 0.57238 | val_0_roc_auc: 0.79106 | val_0_accuracy: 0.70879 |  0:00:04s\n",
            "epoch 3  | loss: 0.54031 | val_0_roc_auc: 0.80054 | val_0_accuracy: 0.71594 |  0:00:05s\n",
            "epoch 4  | loss: 0.52865 | val_0_roc_auc: 0.80943 | val_0_accuracy: 0.72187 |  0:00:07s\n",
            "epoch 5  | loss: 0.51843 | val_0_roc_auc: 0.82166 | val_0_accuracy: 0.73889 |  0:00:09s\n",
            "epoch 6  | loss: 0.49671 | val_0_roc_auc: 0.83367 | val_0_accuracy: 0.74087 |  0:00:10s\n",
            "epoch 7  | loss: 0.48594 | val_0_roc_auc: 0.83891 | val_0_accuracy: 0.75    |  0:00:12s\n",
            "epoch 8  | loss: 0.47293 | val_0_roc_auc: 0.84452 | val_0_accuracy: 0.75346 |  0:00:13s\n",
            "epoch 9  | loss: 0.47189 | val_0_roc_auc: 0.84648 | val_0_accuracy: 0.7537  |  0:00:15s\n",
            "epoch 10 | loss: 0.467   | val_0_roc_auc: 0.85141 | val_0_accuracy: 0.75642 |  0:00:16s\n",
            "epoch 11 | loss: 0.46111 | val_0_roc_auc: 0.85706 | val_0_accuracy: 0.76579 |  0:00:18s\n",
            "epoch 12 | loss: 0.44844 | val_0_roc_auc: 0.86075 | val_0_accuracy: 0.76752 |  0:00:19s\n",
            "epoch 13 | loss: 0.4398  | val_0_roc_auc: 0.86438 | val_0_accuracy: 0.77542 |  0:00:21s\n",
            "epoch 14 | loss: 0.43778 | val_0_roc_auc: 0.86511 | val_0_accuracy: 0.7769  |  0:00:22s\n",
            "epoch 15 | loss: 0.44019 | val_0_roc_auc: 0.86128 | val_0_accuracy: 0.77443 |  0:00:24s\n",
            "epoch 16 | loss: 0.42891 | val_0_roc_auc: 0.87424 | val_0_accuracy: 0.77887 |  0:00:25s\n",
            "epoch 17 | loss: 0.42296 | val_0_roc_auc: 0.87795 | val_0_accuracy: 0.78751 |  0:00:26s\n",
            "epoch 18 | loss: 0.41405 | val_0_roc_auc: 0.88371 | val_0_accuracy: 0.79146 |  0:00:28s\n",
            "epoch 19 | loss: 0.40878 | val_0_roc_auc: 0.88455 | val_0_accuracy: 0.79121 |  0:00:29s\n",
            "epoch 20 | loss: 0.41211 | val_0_roc_auc: 0.88587 | val_0_accuracy: 0.79961 |  0:00:31s\n",
            "epoch 21 | loss: 0.40474 | val_0_roc_auc: 0.88654 | val_0_accuracy: 0.79393 |  0:00:32s\n",
            "epoch 22 | loss: 0.40151 | val_0_roc_auc: 0.89209 | val_0_accuracy: 0.80158 |  0:00:34s\n",
            "epoch 23 | loss: 0.38761 | val_0_roc_auc: 0.89337 | val_0_accuracy: 0.8001  |  0:00:35s\n",
            "epoch 24 | loss: 0.3874  | val_0_roc_auc: 0.89557 | val_0_accuracy: 0.80627 |  0:00:36s\n",
            "epoch 25 | loss: 0.39167 | val_0_roc_auc: 0.89263 | val_0_accuracy: 0.80133 |  0:00:38s\n",
            "epoch 26 | loss: 0.38698 | val_0_roc_auc: 0.89895 | val_0_accuracy: 0.81022 |  0:00:39s\n",
            "epoch 27 | loss: 0.37373 | val_0_roc_auc: 0.9055  | val_0_accuracy: 0.8191  |  0:00:41s\n",
            "epoch 28 | loss: 0.36511 | val_0_roc_auc: 0.90842 | val_0_accuracy: 0.82404 |  0:00:42s\n",
            "epoch 29 | loss: 0.36048 | val_0_roc_auc: 0.91164 | val_0_accuracy: 0.82502 |  0:00:44s\n",
            "epoch 30 | loss: 0.36856 | val_0_roc_auc: 0.90678 | val_0_accuracy: 0.82083 |  0:00:45s\n",
            "epoch 31 | loss: 0.37446 | val_0_roc_auc: 0.90707 | val_0_accuracy: 0.8233  |  0:00:47s\n",
            "epoch 32 | loss: 0.35706 | val_0_roc_auc: 0.91288 | val_0_accuracy: 0.82552 |  0:00:48s\n",
            "epoch 33 | loss: 0.34055 | val_0_roc_auc: 0.91557 | val_0_accuracy: 0.82675 |  0:00:49s\n",
            "epoch 34 | loss: 0.33592 | val_0_roc_auc: 0.91951 | val_0_accuracy: 0.83169 |  0:00:51s\n",
            "epoch 35 | loss: 0.35167 | val_0_roc_auc: 0.91326 | val_0_accuracy: 0.82749 |  0:00:52s\n",
            "epoch 36 | loss: 0.34358 | val_0_roc_auc: 0.91571 | val_0_accuracy: 0.83243 |  0:00:54s\n",
            "epoch 37 | loss: 0.33313 | val_0_roc_auc: 0.91788 | val_0_accuracy: 0.83095 |  0:00:55s\n",
            "epoch 38 | loss: 0.32347 | val_0_roc_auc: 0.92454 | val_0_accuracy: 0.84131 |  0:00:57s\n",
            "epoch 39 | loss: 0.30977 | val_0_roc_auc: 0.92677 | val_0_accuracy: 0.84156 |  0:00:58s\n",
            "epoch 40 | loss: 0.32638 | val_0_roc_auc: 0.91408 | val_0_accuracy: 0.82577 |  0:01:00s\n",
            "epoch 41 | loss: 0.33032 | val_0_roc_auc: 0.91633 | val_0_accuracy: 0.83218 |  0:01:01s\n",
            "epoch 42 | loss: 0.31748 | val_0_roc_auc: 0.92021 | val_0_accuracy: 0.83564 |  0:01:03s\n",
            "epoch 43 | loss: 0.29975 | val_0_roc_auc: 0.92331 | val_0_accuracy: 0.83909 |  0:01:04s\n",
            "epoch 44 | loss: 0.29409 | val_0_roc_auc: 0.92851 | val_0_accuracy: 0.8423  |  0:01:05s\n",
            "epoch 45 | loss: 0.30722 | val_0_roc_auc: 0.91745 | val_0_accuracy: 0.83786 |  0:01:07s\n",
            "epoch 46 | loss: 0.31871 | val_0_roc_auc: 0.92241 | val_0_accuracy: 0.84107 |  0:01:08s\n",
            "epoch 47 | loss: 0.30167 | val_0_roc_auc: 0.92549 | val_0_accuracy: 0.84403 |  0:01:10s\n",
            "epoch 48 | loss: 0.28781 | val_0_roc_auc: 0.93212 | val_0_accuracy: 0.85217 |  0:01:11s\n",
            "epoch 49 | loss: 0.27495 | val_0_roc_auc: 0.93584 | val_0_accuracy: 0.85884 |  0:01:13s\n",
            "epoch 50 | loss: 0.29864 | val_0_roc_auc: 0.92405 | val_0_accuracy: 0.84131 |  0:01:14s\n",
            "epoch 51 | loss: 0.29781 | val_0_roc_auc: 0.92637 | val_0_accuracy: 0.84551 |  0:01:15s\n",
            "epoch 52 | loss: 0.29095 | val_0_roc_auc: 0.93081 | val_0_accuracy: 0.84921 |  0:01:17s\n",
            "epoch 53 | loss: 0.26931 | val_0_roc_auc: 0.93566 | val_0_accuracy: 0.86056 |  0:01:18s\n",
            "epoch 54 | loss: 0.25735 | val_0_roc_auc: 0.93977 | val_0_accuracy: 0.8618  |  0:01:20s\n",
            "epoch 55 | loss: 0.28623 | val_0_roc_auc: 0.92506 | val_0_accuracy: 0.84329 |  0:01:21s\n",
            "epoch 56 | loss: 0.29447 | val_0_roc_auc: 0.9286  | val_0_accuracy: 0.84576 |  0:01:23s\n",
            "epoch 57 | loss: 0.27416 | val_0_roc_auc: 0.93557 | val_0_accuracy: 0.85587 |  0:01:24s\n",
            "epoch 58 | loss: 0.2609  | val_0_roc_auc: 0.93789 | val_0_accuracy: 0.86032 |  0:01:26s\n",
            "epoch 59 | loss: 0.24945 | val_0_roc_auc: 0.94352 | val_0_accuracy: 0.8692  |  0:01:27s\n",
            "epoch 60 | loss: 0.27718 | val_0_roc_auc: 0.93054 | val_0_accuracy: 0.85094 |  0:01:28s\n",
            "epoch 61 | loss: 0.27928 | val_0_roc_auc: 0.93347 | val_0_accuracy: 0.85538 |  0:01:30s\n",
            "epoch 62 | loss: 0.26485 | val_0_roc_auc: 0.93597 | val_0_accuracy: 0.8618  |  0:01:31s\n",
            "epoch 63 | loss: 0.25078 | val_0_roc_auc: 0.94334 | val_0_accuracy: 0.87019 |  0:01:33s\n",
            "epoch 64 | loss: 0.23773 | val_0_roc_auc: 0.94654 | val_0_accuracy: 0.87463 |  0:01:34s\n",
            "epoch 65 | loss: 0.27063 | val_0_roc_auc: 0.93488 | val_0_accuracy: 0.85587 |  0:01:36s\n",
            "epoch 66 | loss: 0.27812 | val_0_roc_auc: 0.93351 | val_0_accuracy: 0.85563 |  0:01:37s\n",
            "epoch 67 | loss: 0.26475 | val_0_roc_auc: 0.94071 | val_0_accuracy: 0.86451 |  0:01:39s\n",
            "epoch 68 | loss: 0.24059 | val_0_roc_auc: 0.94614 | val_0_accuracy: 0.87685 |  0:01:40s\n",
            "epoch 69 | loss: 0.22721 | val_0_roc_auc: 0.9494  | val_0_accuracy: 0.87808 |  0:01:41s\n",
            "epoch 70 | loss: 0.26337 | val_0_roc_auc: 0.93132 | val_0_accuracy: 0.85464 |  0:01:43s\n",
            "epoch 71 | loss: 0.27178 | val_0_roc_auc: 0.93962 | val_0_accuracy: 0.86352 |  0:01:44s\n",
            "epoch 72 | loss: 0.25164 | val_0_roc_auc: 0.93922 | val_0_accuracy: 0.85785 |  0:01:46s\n",
            "epoch 73 | loss: 0.2335  | val_0_roc_auc: 0.94663 | val_0_accuracy: 0.87907 |  0:01:48s\n",
            "epoch 74 | loss: 0.21667 | val_0_roc_auc: 0.952   | val_0_accuracy: 0.88475 |  0:01:49s\n",
            "epoch 75 | loss: 0.25144 | val_0_roc_auc: 0.93549 | val_0_accuracy: 0.86229 |  0:01:51s\n",
            "epoch 76 | loss: 0.26397 | val_0_roc_auc: 0.93741 | val_0_accuracy: 0.86426 |  0:01:52s\n",
            "epoch 77 | loss: 0.24827 | val_0_roc_auc: 0.94311 | val_0_accuracy: 0.87512 |  0:01:54s\n",
            "epoch 78 | loss: 0.23016 | val_0_roc_auc: 0.9473  | val_0_accuracy: 0.87636 |  0:01:55s\n",
            "epoch 79 | loss: 0.20962 | val_0_roc_auc: 0.94966 | val_0_accuracy: 0.88253 |  0:01:57s\n",
            "epoch 80 | loss: 0.25284 | val_0_roc_auc: 0.93768 | val_0_accuracy: 0.86278 |  0:01:58s\n",
            "epoch 81 | loss: 0.25159 | val_0_roc_auc: 0.94036 | val_0_accuracy: 0.86723 |  0:02:00s\n",
            "epoch 82 | loss: 0.24198 | val_0_roc_auc: 0.94279 | val_0_accuracy: 0.8692  |  0:02:01s\n",
            "epoch 83 | loss: 0.22114 | val_0_roc_auc: 0.94818 | val_0_accuracy: 0.87784 |  0:02:03s\n",
            "epoch 84 | loss: 0.20599 | val_0_roc_auc: 0.95275 | val_0_accuracy: 0.88623 |  0:02:04s\n",
            "epoch 85 | loss: 0.24902 | val_0_roc_auc: 0.93575 | val_0_accuracy: 0.85884 |  0:02:06s\n",
            "epoch 86 | loss: 0.25786 | val_0_roc_auc: 0.94485 | val_0_accuracy: 0.86426 |  0:02:07s\n",
            "epoch 87 | loss: 0.23407 | val_0_roc_auc: 0.94453 | val_0_accuracy: 0.8729  |  0:02:09s\n",
            "epoch 88 | loss: 0.21881 | val_0_roc_auc: 0.95111 | val_0_accuracy: 0.88277 |  0:02:10s\n",
            "epoch 89 | loss: 0.19935 | val_0_roc_auc: 0.95255 | val_0_accuracy: 0.88771 |  0:02:12s\n",
            "epoch 90 | loss: 0.23659 | val_0_roc_auc: 0.93692 | val_0_accuracy: 0.86451 |  0:02:13s\n",
            "epoch 91 | loss: 0.24654 | val_0_roc_auc: 0.94013 | val_0_accuracy: 0.86426 |  0:02:14s\n",
            "epoch 92 | loss: 0.22873 | val_0_roc_auc: 0.94465 | val_0_accuracy: 0.87611 |  0:02:16s\n",
            "epoch 93 | loss: 0.21055 | val_0_roc_auc: 0.94985 | val_0_accuracy: 0.8771  |  0:02:17s\n",
            "epoch 94 | loss: 0.19657 | val_0_roc_auc: 0.95216 | val_0_accuracy: 0.88968 |  0:02:19s\n",
            "epoch 95 | loss: 0.23664 | val_0_roc_auc: 0.93478 | val_0_accuracy: 0.85069 |  0:02:20s\n",
            "epoch 96 | loss: 0.24742 | val_0_roc_auc: 0.94204 | val_0_accuracy: 0.87216 |  0:02:22s\n",
            "epoch 97 | loss: 0.22537 | val_0_roc_auc: 0.9482  | val_0_accuracy: 0.87734 |  0:02:23s\n",
            "epoch 98 | loss: 0.20588 | val_0_roc_auc: 0.94965 | val_0_accuracy: 0.87833 |  0:02:24s\n",
            "epoch 99 | loss: 0.18941 | val_0_roc_auc: 0.95255 | val_0_accuracy: 0.88425 |  0:02:26s\n",
            "epoch 100| loss: 0.22991 | val_0_roc_auc: 0.94213 | val_0_accuracy: 0.86821 |  0:02:27s\n",
            "epoch 101| loss: 0.23636 | val_0_roc_auc: 0.9444  | val_0_accuracy: 0.87537 |  0:02:29s\n",
            "epoch 102| loss: 0.21755 | val_0_roc_auc: 0.95011 | val_0_accuracy: 0.88327 |  0:02:30s\n",
            "epoch 103| loss: 0.19961 | val_0_roc_auc: 0.95224 | val_0_accuracy: 0.88327 |  0:02:32s\n",
            "epoch 104| loss: 0.18703 | val_0_roc_auc: 0.95585 | val_0_accuracy: 0.88968 |  0:02:33s\n",
            "epoch 105| loss: 0.22095 | val_0_roc_auc: 0.93824 | val_0_accuracy: 0.86871 |  0:02:35s\n",
            "epoch 106| loss: 0.25128 | val_0_roc_auc: 0.94762 | val_0_accuracy: 0.8771  |  0:02:36s\n",
            "epoch 107| loss: 0.22178 | val_0_roc_auc: 0.94891 | val_0_accuracy: 0.87389 |  0:02:38s\n",
            "epoch 108| loss: 0.20099 | val_0_roc_auc: 0.95245 | val_0_accuracy: 0.88425 |  0:02:39s\n",
            "epoch 109| loss: 0.18808 | val_0_roc_auc: 0.95671 | val_0_accuracy: 0.89388 |  0:02:40s\n",
            "epoch 110| loss: 0.2175  | val_0_roc_auc: 0.94662 | val_0_accuracy: 0.87414 |  0:02:42s\n",
            "epoch 111| loss: 0.23414 | val_0_roc_auc: 0.94271 | val_0_accuracy: 0.86945 |  0:02:43s\n",
            "epoch 112| loss: 0.22319 | val_0_roc_auc: 0.95277 | val_0_accuracy: 0.88598 |  0:02:45s\n",
            "epoch 113| loss: 0.19136 | val_0_roc_auc: 0.95526 | val_0_accuracy: 0.88425 |  0:02:46s\n",
            "epoch 114| loss: 0.18096 | val_0_roc_auc: 0.95736 | val_0_accuracy: 0.89289 |  0:02:48s\n",
            "epoch 115| loss: 0.21861 | val_0_roc_auc: 0.94332 | val_0_accuracy: 0.87192 |  0:02:49s\n",
            "epoch 116| loss: 0.23228 | val_0_roc_auc: 0.94167 | val_0_accuracy: 0.87043 |  0:02:51s\n",
            "epoch 117| loss: 0.21886 | val_0_roc_auc: 0.94796 | val_0_accuracy: 0.87858 |  0:02:52s\n",
            "epoch 118| loss: 0.19339 | val_0_roc_auc: 0.95456 | val_0_accuracy: 0.88894 |  0:02:54s\n",
            "epoch 119| loss: 0.17502 | val_0_roc_auc: 0.95673 | val_0_accuracy: 0.89339 |  0:02:55s\n",
            "epoch 120| loss: 0.21847 | val_0_roc_auc: 0.94544 | val_0_accuracy: 0.87438 |  0:02:56s\n",
            "epoch 121| loss: 0.22249 | val_0_roc_auc: 0.95033 | val_0_accuracy: 0.87636 |  0:02:58s\n",
            "epoch 122| loss: 0.20748 | val_0_roc_auc: 0.95188 | val_0_accuracy: 0.88401 |  0:02:59s\n",
            "epoch 123| loss: 0.18928 | val_0_roc_auc: 0.95722 | val_0_accuracy: 0.89042 |  0:03:01s\n",
            "epoch 124| loss: 0.17004 | val_0_roc_auc: 0.96068 | val_0_accuracy: 0.89388 |  0:03:02s\n",
            "epoch 125| loss: 0.22412 | val_0_roc_auc: 0.94563 | val_0_accuracy: 0.87192 |  0:03:04s\n",
            "epoch 126| loss: 0.22484 | val_0_roc_auc: 0.94938 | val_0_accuracy: 0.88228 |  0:03:05s\n",
            "epoch 127| loss: 0.20689 | val_0_roc_auc: 0.95062 | val_0_accuracy: 0.88154 |  0:03:07s\n",
            "epoch 128| loss: 0.18342 | val_0_roc_auc: 0.95744 | val_0_accuracy: 0.89215 |  0:03:08s\n",
            "epoch 129| loss: 0.17306 | val_0_roc_auc: 0.96002 | val_0_accuracy: 0.89906 |  0:03:10s\n",
            "epoch 130| loss: 0.21107 | val_0_roc_auc: 0.94387 | val_0_accuracy: 0.87512 |  0:03:11s\n",
            "epoch 131| loss: 0.22124 | val_0_roc_auc: 0.94903 | val_0_accuracy: 0.87784 |  0:03:13s\n",
            "epoch 132| loss: 0.20308 | val_0_roc_auc: 0.95157 | val_0_accuracy: 0.87858 |  0:03:14s\n",
            "epoch 133| loss: 0.18046 | val_0_roc_auc: 0.95843 | val_0_accuracy: 0.89141 |  0:03:15s\n",
            "epoch 134| loss: 0.16407 | val_0_roc_auc: 0.96195 | val_0_accuracy: 0.89906 |  0:03:17s\n",
            "epoch 135| loss: 0.21083 | val_0_roc_auc: 0.95023 | val_0_accuracy: 0.88277 |  0:03:18s\n",
            "epoch 136| loss: 0.2185  | val_0_roc_auc: 0.95215 | val_0_accuracy: 0.88327 |  0:03:20s\n",
            "epoch 137| loss: 0.19772 | val_0_roc_auc: 0.95562 | val_0_accuracy: 0.88672 |  0:03:21s\n",
            "epoch 138| loss: 0.17478 | val_0_roc_auc: 0.95923 | val_0_accuracy: 0.89536 |  0:03:23s\n",
            "epoch 139| loss: 0.1633  | val_0_roc_auc: 0.96225 | val_0_accuracy: 0.90252 |  0:03:24s\n",
            "epoch 140| loss: 0.21349 | val_0_roc_auc: 0.9458  | val_0_accuracy: 0.87512 |  0:03:26s\n",
            "epoch 141| loss: 0.21663 | val_0_roc_auc: 0.94818 | val_0_accuracy: 0.87907 |  0:03:27s\n",
            "epoch 142| loss: 0.19945 | val_0_roc_auc: 0.95197 | val_0_accuracy: 0.88623 |  0:03:29s\n",
            "epoch 143| loss: 0.17697 | val_0_roc_auc: 0.95816 | val_0_accuracy: 0.89363 |  0:03:30s\n",
            "epoch 144| loss: 0.16121 | val_0_roc_auc: 0.96006 | val_0_accuracy: 0.90104 |  0:03:32s\n",
            "epoch 145| loss: 0.20417 | val_0_roc_auc: 0.948   | val_0_accuracy: 0.87216 |  0:03:34s\n",
            "epoch 146| loss: 0.21365 | val_0_roc_auc: 0.9494  | val_0_accuracy: 0.87833 |  0:03:35s\n",
            "epoch 147| loss: 0.19749 | val_0_roc_auc: 0.95725 | val_0_accuracy: 0.88771 |  0:03:37s\n",
            "epoch 148| loss: 0.16861 | val_0_roc_auc: 0.95976 | val_0_accuracy: 0.89437 |  0:03:38s\n",
            "epoch 149| loss: 0.15562 | val_0_roc_auc: 0.96178 | val_0_accuracy: 0.8998  |  0:03:40s\n",
            "epoch 150| loss: 0.20372 | val_0_roc_auc: 0.94914 | val_0_accuracy: 0.87611 |  0:03:41s\n",
            "epoch 151| loss: 0.2148  | val_0_roc_auc: 0.94837 | val_0_accuracy: 0.87858 |  0:03:43s\n",
            "epoch 152| loss: 0.19337 | val_0_roc_auc: 0.9549  | val_0_accuracy: 0.88697 |  0:03:44s\n",
            "epoch 153| loss: 0.16979 | val_0_roc_auc: 0.96112 | val_0_accuracy: 0.89783 |  0:03:45s\n",
            "epoch 154| loss: 0.15778 | val_0_roc_auc: 0.96411 | val_0_accuracy: 0.90326 |  0:03:47s\n",
            "epoch 155| loss: 0.19429 | val_0_roc_auc: 0.94987 | val_0_accuracy: 0.885   |  0:03:48s\n",
            "epoch 156| loss: 0.21251 | val_0_roc_auc: 0.95049 | val_0_accuracy: 0.88697 |  0:03:50s\n",
            "epoch 157| loss: 0.18608 | val_0_roc_auc: 0.95459 | val_0_accuracy: 0.8882  |  0:03:51s\n",
            "epoch 158| loss: 0.1674  | val_0_roc_auc: 0.96003 | val_0_accuracy: 0.90054 |  0:03:53s\n",
            "epoch 159| loss: 0.15334 | val_0_roc_auc: 0.96238 | val_0_accuracy: 0.90054 |  0:03:54s\n",
            "epoch 160| loss: 0.20041 | val_0_roc_auc: 0.94826 | val_0_accuracy: 0.87883 |  0:03:56s\n",
            "epoch 161| loss: 0.21351 | val_0_roc_auc: 0.95414 | val_0_accuracy: 0.89166 |  0:03:57s\n",
            "epoch 162| loss: 0.18787 | val_0_roc_auc: 0.95601 | val_0_accuracy: 0.8887  |  0:03:59s\n",
            "epoch 163| loss: 0.17185 | val_0_roc_auc: 0.96298 | val_0_accuracy: 0.90276 |  0:04:00s\n",
            "epoch 164| loss: 0.1531  | val_0_roc_auc: 0.96366 | val_0_accuracy: 0.90573 |  0:04:01s\n",
            "epoch 165| loss: 0.19562 | val_0_roc_auc: 0.94616 | val_0_accuracy: 0.87142 |  0:04:03s\n",
            "epoch 166| loss: 0.20326 | val_0_roc_auc: 0.95615 | val_0_accuracy: 0.885   |  0:04:04s\n",
            "epoch 167| loss: 0.18776 | val_0_roc_auc: 0.9579  | val_0_accuracy: 0.89684 |  0:04:06s\n",
            "epoch 168| loss: 0.16942 | val_0_roc_auc: 0.96173 | val_0_accuracy: 0.89931 |  0:04:07s\n",
            "epoch 169| loss: 0.14688 | val_0_roc_auc: 0.96471 | val_0_accuracy: 0.90523 |  0:04:09s\n",
            "epoch 170| loss: 0.19139 | val_0_roc_auc: 0.94785 | val_0_accuracy: 0.87488 |  0:04:10s\n",
            "epoch 171| loss: 0.20782 | val_0_roc_auc: 0.95306 | val_0_accuracy: 0.88425 |  0:04:12s\n",
            "epoch 172| loss: 0.17989 | val_0_roc_auc: 0.95882 | val_0_accuracy: 0.89536 |  0:04:13s\n",
            "epoch 173| loss: 0.16016 | val_0_roc_auc: 0.96276 | val_0_accuracy: 0.89832 |  0:04:15s\n",
            "epoch 174| loss: 0.1456  | val_0_roc_auc: 0.96446 | val_0_accuracy: 0.90449 |  0:04:16s\n",
            "epoch 175| loss: 0.19493 | val_0_roc_auc: 0.94582 | val_0_accuracy: 0.88055 |  0:04:18s\n",
            "epoch 176| loss: 0.20615 | val_0_roc_auc: 0.95542 | val_0_accuracy: 0.89339 |  0:04:19s\n",
            "epoch 177| loss: 0.1822  | val_0_roc_auc: 0.95863 | val_0_accuracy: 0.89659 |  0:04:20s\n",
            "epoch 178| loss: 0.16181 | val_0_roc_auc: 0.96239 | val_0_accuracy: 0.90079 |  0:04:22s\n",
            "epoch 179| loss: 0.14483 | val_0_roc_auc: 0.96401 | val_0_accuracy: 0.90474 |  0:04:23s\n",
            "epoch 180| loss: 0.18008 | val_0_roc_auc: 0.9495  | val_0_accuracy: 0.88277 |  0:04:25s\n",
            "epoch 181| loss: 0.19127 | val_0_roc_auc: 0.95426 | val_0_accuracy: 0.88771 |  0:04:26s\n",
            "epoch 182| loss: 0.17878 | val_0_roc_auc: 0.96062 | val_0_accuracy: 0.89931 |  0:04:28s\n",
            "epoch 183| loss: 0.15978 | val_0_roc_auc: 0.96513 | val_0_accuracy: 0.90499 |  0:04:29s\n",
            "epoch 184| loss: 0.142   | val_0_roc_auc: 0.96713 | val_0_accuracy: 0.90671 |  0:04:30s\n",
            "epoch 185| loss: 0.17853 | val_0_roc_auc: 0.95096 | val_0_accuracy: 0.87734 |  0:04:32s\n",
            "epoch 186| loss: 0.20239 | val_0_roc_auc: 0.95463 | val_0_accuracy: 0.89067 |  0:04:33s\n",
            "epoch 187| loss: 0.17727 | val_0_roc_auc: 0.95536 | val_0_accuracy: 0.8887  |  0:04:35s\n",
            "epoch 188| loss: 0.15952 | val_0_roc_auc: 0.96257 | val_0_accuracy: 0.90326 |  0:04:36s\n",
            "epoch 189| loss: 0.14248 | val_0_roc_auc: 0.96599 | val_0_accuracy: 0.90449 |  0:04:38s\n",
            "epoch 190| loss: 0.18075 | val_0_roc_auc: 0.95021 | val_0_accuracy: 0.88746 |  0:04:39s\n",
            "epoch 191| loss: 0.19427 | val_0_roc_auc: 0.95397 | val_0_accuracy: 0.88697 |  0:04:41s\n",
            "epoch 192| loss: 0.16739 | val_0_roc_auc: 0.95843 | val_0_accuracy: 0.89684 |  0:04:42s\n",
            "epoch 193| loss: 0.1528  | val_0_roc_auc: 0.96277 | val_0_accuracy: 0.90202 |  0:04:44s\n",
            "epoch 194| loss: 0.14179 | val_0_roc_auc: 0.96707 | val_0_accuracy: 0.90893 |  0:04:45s\n",
            "epoch 195| loss: 0.19037 | val_0_roc_auc: 0.95103 | val_0_accuracy: 0.88351 |  0:04:46s\n",
            "epoch 196| loss: 0.18608 | val_0_roc_auc: 0.95516 | val_0_accuracy: 0.88697 |  0:04:48s\n",
            "epoch 197| loss: 0.16645 | val_0_roc_auc: 0.96137 | val_0_accuracy: 0.8998  |  0:04:49s\n",
            "epoch 198| loss: 0.14956 | val_0_roc_auc: 0.96311 | val_0_accuracy: 0.90301 |  0:04:51s\n",
            "epoch 199| loss: 0.13611 | val_0_roc_auc: 0.96679 | val_0_accuracy: 0.90696 |  0:04:52s\n",
            "epoch 200| loss: 0.17329 | val_0_roc_auc: 0.95151 | val_0_accuracy: 0.88129 |  0:04:54s\n",
            "epoch 201| loss: 0.18256 | val_0_roc_auc: 0.95432 | val_0_accuracy: 0.88598 |  0:04:55s\n",
            "epoch 202| loss: 0.1664  | val_0_roc_auc: 0.96047 | val_0_accuracy: 0.89339 |  0:04:57s\n",
            "epoch 203| loss: 0.15178 | val_0_roc_auc: 0.96436 | val_0_accuracy: 0.90375 |  0:04:58s\n",
            "epoch 204| loss: 0.1338  | val_0_roc_auc: 0.96665 | val_0_accuracy: 0.90869 |  0:05:00s\n",
            "epoch 205| loss: 0.16977 | val_0_roc_auc: 0.94805 | val_0_accuracy: 0.87932 |  0:05:01s\n",
            "epoch 206| loss: 0.18728 | val_0_roc_auc: 0.95154 | val_0_accuracy: 0.88524 |  0:05:02s\n",
            "epoch 207| loss: 0.17006 | val_0_roc_auc: 0.96047 | val_0_accuracy: 0.89882 |  0:05:04s\n",
            "epoch 208| loss: 0.14783 | val_0_roc_auc: 0.96597 | val_0_accuracy: 0.90597 |  0:05:05s\n",
            "epoch 209| loss: 0.12876 | val_0_roc_auc: 0.96717 | val_0_accuracy: 0.90745 |  0:05:07s\n",
            "epoch 210| loss: 0.17369 | val_0_roc_auc: 0.95657 | val_0_accuracy: 0.88376 |  0:05:08s\n",
            "epoch 211| loss: 0.18653 | val_0_roc_auc: 0.95202 | val_0_accuracy: 0.88796 |  0:05:10s\n",
            "epoch 212| loss: 0.17057 | val_0_roc_auc: 0.9634  | val_0_accuracy: 0.90079 |  0:05:11s\n",
            "epoch 213| loss: 0.14644 | val_0_roc_auc: 0.96629 | val_0_accuracy: 0.90844 |  0:05:13s\n",
            "epoch 214| loss: 0.13228 | val_0_roc_auc: 0.96924 | val_0_accuracy: 0.91412 |  0:05:15s\n",
            "epoch 215| loss: 0.16249 | val_0_roc_auc: 0.95139 | val_0_accuracy: 0.88376 |  0:05:16s\n",
            "epoch 216| loss: 0.19331 | val_0_roc_auc: 0.95624 | val_0_accuracy: 0.8924  |  0:05:18s\n",
            "epoch 217| loss: 0.16943 | val_0_roc_auc: 0.96113 | val_0_accuracy: 0.90079 |  0:05:19s\n",
            "epoch 218| loss: 0.14434 | val_0_roc_auc: 0.96772 | val_0_accuracy: 0.90918 |  0:05:21s\n",
            "epoch 219| loss: 0.13297 | val_0_roc_auc: 0.96831 | val_0_accuracy: 0.90943 |  0:05:22s\n",
            "epoch 220| loss: 0.16979 | val_0_roc_auc: 0.95642 | val_0_accuracy: 0.89116 |  0:05:24s\n",
            "epoch 221| loss: 0.1872  | val_0_roc_auc: 0.95802 | val_0_accuracy: 0.8961  |  0:05:25s\n",
            "epoch 222| loss: 0.16544 | val_0_roc_auc: 0.96169 | val_0_accuracy: 0.9035  |  0:05:27s\n",
            "epoch 223| loss: 0.14214 | val_0_roc_auc: 0.96572 | val_0_accuracy: 0.91041 |  0:05:28s\n",
            "epoch 224| loss: 0.1259  | val_0_roc_auc: 0.96886 | val_0_accuracy: 0.91338 |  0:05:29s\n",
            "epoch 225| loss: 0.1627  | val_0_roc_auc: 0.95931 | val_0_accuracy: 0.89684 |  0:05:31s\n",
            "epoch 226| loss: 0.17405 | val_0_roc_auc: 0.95484 | val_0_accuracy: 0.88351 |  0:05:32s\n",
            "epoch 227| loss: 0.15855 | val_0_roc_auc: 0.95991 | val_0_accuracy: 0.90054 |  0:05:34s\n",
            "epoch 228| loss: 0.13496 | val_0_roc_auc: 0.965   | val_0_accuracy: 0.90326 |  0:05:35s\n",
            "epoch 229| loss: 0.1281  | val_0_roc_auc: 0.96699 | val_0_accuracy: 0.90844 |  0:05:37s\n",
            "epoch 230| loss: 0.16286 | val_0_roc_auc: 0.95498 | val_0_accuracy: 0.89141 |  0:05:38s\n",
            "epoch 231| loss: 0.18104 | val_0_roc_auc: 0.95685 | val_0_accuracy: 0.89561 |  0:05:40s\n",
            "epoch 232| loss: 0.16168 | val_0_roc_auc: 0.96289 | val_0_accuracy: 0.90178 |  0:05:41s\n",
            "epoch 233| loss: 0.14066 | val_0_roc_auc: 0.96641 | val_0_accuracy: 0.90967 |  0:05:42s\n",
            "epoch 234| loss: 0.12793 | val_0_roc_auc: 0.96925 | val_0_accuracy: 0.91436 |  0:05:44s\n",
            "epoch 235| loss: 0.16364 | val_0_roc_auc: 0.9565  | val_0_accuracy: 0.89511 |  0:05:45s\n",
            "epoch 236| loss: 0.17173 | val_0_roc_auc: 0.96028 | val_0_accuracy: 0.89733 |  0:05:47s\n",
            "epoch 237| loss: 0.15566 | val_0_roc_auc: 0.96466 | val_0_accuracy: 0.90597 |  0:05:48s\n",
            "epoch 238| loss: 0.13845 | val_0_roc_auc: 0.96676 | val_0_accuracy: 0.90795 |  0:05:50s\n",
            "epoch 239| loss: 0.12182 | val_0_roc_auc: 0.96947 | val_0_accuracy: 0.91239 |  0:05:51s\n",
            "epoch 240| loss: 0.17521 | val_0_roc_auc: 0.94944 | val_0_accuracy: 0.88031 |  0:05:53s\n",
            "epoch 241| loss: 0.17525 | val_0_roc_auc: 0.95944 | val_0_accuracy: 0.8961  |  0:05:54s\n",
            "epoch 242| loss: 0.15294 | val_0_roc_auc: 0.96216 | val_0_accuracy: 0.90375 |  0:05:56s\n",
            "epoch 243| loss: 0.13715 | val_0_roc_auc: 0.96596 | val_0_accuracy: 0.90992 |  0:05:57s\n",
            "epoch 244| loss: 0.12416 | val_0_roc_auc: 0.96944 | val_0_accuracy: 0.91066 |  0:05:58s\n",
            "epoch 245| loss: 0.16001 | val_0_roc_auc: 0.95807 | val_0_accuracy: 0.89289 |  0:06:00s\n",
            "epoch 246| loss: 0.17515 | val_0_roc_auc: 0.95969 | val_0_accuracy: 0.89684 |  0:06:01s\n",
            "epoch 247| loss: 0.15204 | val_0_roc_auc: 0.96394 | val_0_accuracy: 0.90844 |  0:06:03s\n",
            "epoch 248| loss: 0.13484 | val_0_roc_auc: 0.96958 | val_0_accuracy: 0.90893 |  0:06:04s\n",
            "epoch 249| loss: 0.12128 | val_0_roc_auc: 0.97046 | val_0_accuracy: 0.91362 |  0:06:06s\n",
            "epoch 250| loss: 0.16315 | val_0_roc_auc: 0.95727 | val_0_accuracy: 0.89437 |  0:06:07s\n",
            "epoch 251| loss: 0.1779  | val_0_roc_auc: 0.9638  | val_0_accuracy: 0.90202 |  0:06:09s\n",
            "epoch 252| loss: 0.15113 | val_0_roc_auc: 0.96435 | val_0_accuracy: 0.90202 |  0:06:10s\n",
            "epoch 253| loss: 0.13597 | val_0_roc_auc: 0.96812 | val_0_accuracy: 0.91412 |  0:06:11s\n",
            "epoch 254| loss: 0.11432 | val_0_roc_auc: 0.97039 | val_0_accuracy: 0.91313 |  0:06:13s\n",
            "epoch 255| loss: 0.1635  | val_0_roc_auc: 0.95929 | val_0_accuracy: 0.8998  |  0:06:14s\n",
            "epoch 256| loss: 0.17356 | val_0_roc_auc: 0.95805 | val_0_accuracy: 0.89191 |  0:06:16s\n",
            "epoch 257| loss: 0.14869 | val_0_roc_auc: 0.96781 | val_0_accuracy: 0.90819 |  0:06:17s\n",
            "epoch 258| loss: 0.13141 | val_0_roc_auc: 0.96823 | val_0_accuracy: 0.90597 |  0:06:19s\n",
            "epoch 259| loss: 0.11595 | val_0_roc_auc: 0.97136 | val_0_accuracy: 0.91165 |  0:06:20s\n",
            "epoch 260| loss: 0.15633 | val_0_roc_auc: 0.95575 | val_0_accuracy: 0.89166 |  0:06:21s\n",
            "epoch 261| loss: 0.16873 | val_0_roc_auc: 0.95824 | val_0_accuracy: 0.89141 |  0:06:23s\n",
            "epoch 262| loss: 0.14835 | val_0_roc_auc: 0.96013 | val_0_accuracy: 0.9003  |  0:06:24s\n",
            "epoch 263| loss: 0.13318 | val_0_roc_auc: 0.96555 | val_0_accuracy: 0.90721 |  0:06:26s\n",
            "epoch 264| loss: 0.11717 | val_0_roc_auc: 0.96876 | val_0_accuracy: 0.91264 |  0:06:27s\n",
            "epoch 265| loss: 0.15419 | val_0_roc_auc: 0.95745 | val_0_accuracy: 0.88993 |  0:06:29s\n",
            "epoch 266| loss: 0.16856 | val_0_roc_auc: 0.95944 | val_0_accuracy: 0.89709 |  0:06:30s\n",
            "epoch 267| loss: 0.15255 | val_0_roc_auc: 0.96357 | val_0_accuracy: 0.90079 |  0:06:32s\n",
            "epoch 268| loss: 0.13381 | val_0_roc_auc: 0.96639 | val_0_accuracy: 0.91066 |  0:06:33s\n",
            "epoch 269| loss: 0.11649 | val_0_roc_auc: 0.96947 | val_0_accuracy: 0.91017 |  0:06:35s\n",
            "epoch 270| loss: 0.15398 | val_0_roc_auc: 0.94983 | val_0_accuracy: 0.88327 |  0:06:36s\n",
            "epoch 271| loss: 0.16633 | val_0_roc_auc: 0.96339 | val_0_accuracy: 0.90128 |  0:06:38s\n",
            "epoch 272| loss: 0.14983 | val_0_roc_auc: 0.96259 | val_0_accuracy: 0.8998  |  0:06:39s\n",
            "epoch 273| loss: 0.12879 | val_0_roc_auc: 0.96589 | val_0_accuracy: 0.91091 |  0:06:40s\n",
            "epoch 274| loss: 0.11267 | val_0_roc_auc: 0.96924 | val_0_accuracy: 0.91535 |  0:06:42s\n",
            "epoch 275| loss: 0.14816 | val_0_roc_auc: 0.96162 | val_0_accuracy: 0.904   |  0:06:43s\n",
            "epoch 276| loss: 0.16823 | val_0_roc_auc: 0.96237 | val_0_accuracy: 0.90474 |  0:06:45s\n",
            "epoch 277| loss: 0.14932 | val_0_roc_auc: 0.9654  | val_0_accuracy: 0.90375 |  0:06:46s\n",
            "epoch 278| loss: 0.12626 | val_0_roc_auc: 0.96904 | val_0_accuracy: 0.90967 |  0:06:48s\n",
            "epoch 279| loss: 0.11164 | val_0_roc_auc: 0.97106 | val_0_accuracy: 0.91313 |  0:06:49s\n",
            "epoch 280| loss: 0.15293 | val_0_roc_auc: 0.96157 | val_0_accuracy: 0.90005 |  0:06:51s\n",
            "epoch 281| loss: 0.16543 | val_0_roc_auc: 0.95718 | val_0_accuracy: 0.8961  |  0:06:52s\n",
            "epoch 282| loss: 0.15286 | val_0_roc_auc: 0.96419 | val_0_accuracy: 0.90449 |  0:06:54s\n",
            "epoch 283| loss: 0.12651 | val_0_roc_auc: 0.96893 | val_0_accuracy: 0.91091 |  0:06:55s\n",
            "epoch 284| loss: 0.10909 | val_0_roc_auc: 0.97007 | val_0_accuracy: 0.91807 |  0:06:57s\n",
            "epoch 285| loss: 0.15331 | val_0_roc_auc: 0.95909 | val_0_accuracy: 0.89635 |  0:06:59s\n",
            "epoch 286| loss: 0.16313 | val_0_roc_auc: 0.96191 | val_0_accuracy: 0.90301 |  0:07:00s\n",
            "epoch 287| loss: 0.14695 | val_0_roc_auc: 0.96493 | val_0_accuracy: 0.90918 |  0:07:02s\n",
            "epoch 288| loss: 0.12469 | val_0_roc_auc: 0.9661  | val_0_accuracy: 0.90795 |  0:07:03s\n",
            "epoch 289| loss: 0.11017 | val_0_roc_auc: 0.9685  | val_0_accuracy: 0.9151  |  0:07:05s\n",
            "epoch 290| loss: 0.15195 | val_0_roc_auc: 0.95821 | val_0_accuracy: 0.89363 |  0:07:06s\n",
            "epoch 291| loss: 0.15858 | val_0_roc_auc: 0.96187 | val_0_accuracy: 0.89882 |  0:07:08s\n",
            "epoch 292| loss: 0.14894 | val_0_roc_auc: 0.96583 | val_0_accuracy: 0.90844 |  0:07:09s\n",
            "epoch 293| loss: 0.12554 | val_0_roc_auc: 0.9674  | val_0_accuracy: 0.91165 |  0:07:11s\n",
            "epoch 294| loss: 0.11235 | val_0_roc_auc: 0.97105 | val_0_accuracy: 0.91338 |  0:07:12s\n",
            "epoch 295| loss: 0.14445 | val_0_roc_auc: 0.95716 | val_0_accuracy: 0.89116 |  0:07:13s\n",
            "epoch 296| loss: 0.16118 | val_0_roc_auc: 0.96368 | val_0_accuracy: 0.90597 |  0:07:15s\n",
            "epoch 297| loss: 0.14699 | val_0_roc_auc: 0.96646 | val_0_accuracy: 0.90622 |  0:07:16s\n",
            "epoch 298| loss: 0.12407 | val_0_roc_auc: 0.96744 | val_0_accuracy: 0.90992 |  0:07:18s\n",
            "epoch 299| loss: 0.10668 | val_0_roc_auc: 0.9719  | val_0_accuracy: 0.91609 |  0:07:19s\n",
            "epoch 300| loss: 0.14732 | val_0_roc_auc: 0.96289 | val_0_accuracy: 0.89931 |  0:07:20s\n",
            "epoch 301| loss: 0.16459 | val_0_roc_auc: 0.96104 | val_0_accuracy: 0.90005 |  0:07:22s\n",
            "epoch 302| loss: 0.14211 | val_0_roc_auc: 0.96836 | val_0_accuracy: 0.91313 |  0:07:23s\n",
            "epoch 303| loss: 0.11939 | val_0_roc_auc: 0.97029 | val_0_accuracy: 0.91486 |  0:07:25s\n",
            "epoch 304| loss: 0.10663 | val_0_roc_auc: 0.9715  | val_0_accuracy: 0.91955 |  0:07:26s\n",
            "epoch 305| loss: 0.1399  | val_0_roc_auc: 0.96255 | val_0_accuracy: 0.90054 |  0:07:28s\n",
            "epoch 306| loss: 0.15083 | val_0_roc_auc: 0.95732 | val_0_accuracy: 0.8887  |  0:07:29s\n",
            "epoch 307| loss: 0.13682 | val_0_roc_auc: 0.96832 | val_0_accuracy: 0.91338 |  0:07:31s\n",
            "epoch 308| loss: 0.11855 | val_0_roc_auc: 0.9708  | val_0_accuracy: 0.91856 |  0:07:32s\n",
            "epoch 309| loss: 0.10487 | val_0_roc_auc: 0.97193 | val_0_accuracy: 0.92029 |  0:07:34s\n",
            "epoch 310| loss: 0.14251 | val_0_roc_auc: 0.95871 | val_0_accuracy: 0.89783 |  0:07:35s\n",
            "epoch 311| loss: 0.16189 | val_0_roc_auc: 0.96136 | val_0_accuracy: 0.89684 |  0:07:36s\n",
            "epoch 312| loss: 0.14129 | val_0_roc_auc: 0.96712 | val_0_accuracy: 0.91041 |  0:07:38s\n",
            "epoch 313| loss: 0.11748 | val_0_roc_auc: 0.9703  | val_0_accuracy: 0.91979 |  0:07:39s\n",
            "epoch 314| loss: 0.10704 | val_0_roc_auc: 0.97143 | val_0_accuracy: 0.92226 |  0:07:41s\n",
            "epoch 315| loss: 0.1433  | val_0_roc_auc: 0.95737 | val_0_accuracy: 0.89141 |  0:07:42s\n",
            "epoch 316| loss: 0.16147 | val_0_roc_auc: 0.95976 | val_0_accuracy: 0.89956 |  0:07:44s\n",
            "epoch 317| loss: 0.14049 | val_0_roc_auc: 0.96803 | val_0_accuracy: 0.90943 |  0:07:45s\n",
            "epoch 318| loss: 0.1182  | val_0_roc_auc: 0.96987 | val_0_accuracy: 0.9151  |  0:07:47s\n",
            "epoch 319| loss: 0.10685 | val_0_roc_auc: 0.97161 | val_0_accuracy: 0.92226 |  0:07:48s\n",
            "epoch 320| loss: 0.13054 | val_0_roc_auc: 0.95641 | val_0_accuracy: 0.89783 |  0:07:50s\n",
            "epoch 321| loss: 0.1509  | val_0_roc_auc: 0.96101 | val_0_accuracy: 0.9003  |  0:07:51s\n",
            "epoch 322| loss: 0.13578 | val_0_roc_auc: 0.96599 | val_0_accuracy: 0.91115 |  0:07:52s\n",
            "epoch 323| loss: 0.11409 | val_0_roc_auc: 0.9702  | val_0_accuracy: 0.91881 |  0:07:54s\n",
            "epoch 324| loss: 0.10763 | val_0_roc_auc: 0.9714  | val_0_accuracy: 0.91807 |  0:07:55s\n",
            "epoch 325| loss: 0.14364 | val_0_roc_auc: 0.95977 | val_0_accuracy: 0.89857 |  0:07:57s\n",
            "epoch 326| loss: 0.15431 | val_0_roc_auc: 0.96682 | val_0_accuracy: 0.90424 |  0:07:58s\n",
            "epoch 327| loss: 0.13734 | val_0_roc_auc: 0.96405 | val_0_accuracy: 0.90128 |  0:08:00s\n",
            "epoch 328| loss: 0.11922 | val_0_roc_auc: 0.97004 | val_0_accuracy: 0.91313 |  0:08:01s\n",
            "epoch 329| loss: 0.09988 | val_0_roc_auc: 0.97247 | val_0_accuracy: 0.92177 |  0:08:03s\n",
            "epoch 330| loss: 0.13191 | val_0_roc_auc: 0.95899 | val_0_accuracy: 0.89906 |  0:08:04s\n",
            "epoch 331| loss: 0.15133 | val_0_roc_auc: 0.96332 | val_0_accuracy: 0.90178 |  0:08:06s\n",
            "epoch 332| loss: 0.13599 | val_0_roc_auc: 0.96869 | val_0_accuracy: 0.91239 |  0:08:07s\n",
            "epoch 333| loss: 0.11406 | val_0_roc_auc: 0.97186 | val_0_accuracy: 0.91807 |  0:08:08s\n",
            "epoch 334| loss: 0.09929 | val_0_roc_auc: 0.97332 | val_0_accuracy: 0.91856 |  0:08:10s\n",
            "epoch 335| loss: 0.15615 | val_0_roc_auc: 0.95969 | val_0_accuracy: 0.90227 |  0:08:11s\n",
            "epoch 336| loss: 0.1495  | val_0_roc_auc: 0.96338 | val_0_accuracy: 0.90499 |  0:08:13s\n",
            "epoch 337| loss: 0.13148 | val_0_roc_auc: 0.96543 | val_0_accuracy: 0.91486 |  0:08:14s\n",
            "epoch 338| loss: 0.11463 | val_0_roc_auc: 0.97085 | val_0_accuracy: 0.91708 |  0:08:16s\n",
            "epoch 339| loss: 0.10002 | val_0_roc_auc: 0.97328 | val_0_accuracy: 0.92572 |  0:08:17s\n",
            "epoch 340| loss: 0.13349 | val_0_roc_auc: 0.9613  | val_0_accuracy: 0.90523 |  0:08:19s\n",
            "epoch 341| loss: 0.14203 | val_0_roc_auc: 0.96313 | val_0_accuracy: 0.9003  |  0:08:20s\n",
            "epoch 342| loss: 0.13086 | val_0_roc_auc: 0.96722 | val_0_accuracy: 0.91584 |  0:08:22s\n",
            "epoch 343| loss: 0.11545 | val_0_roc_auc: 0.97028 | val_0_accuracy: 0.91708 |  0:08:23s\n",
            "epoch 344| loss: 0.09913 | val_0_roc_auc: 0.9703  | val_0_accuracy: 0.92078 |  0:08:25s\n",
            "epoch 345| loss: 0.13361 | val_0_roc_auc: 0.96357 | val_0_accuracy: 0.90499 |  0:08:26s\n",
            "epoch 346| loss: 0.1454  | val_0_roc_auc: 0.96077 | val_0_accuracy: 0.89561 |  0:08:27s\n",
            "epoch 347| loss: 0.13698 | val_0_roc_auc: 0.96676 | val_0_accuracy: 0.91066 |  0:08:29s\n",
            "epoch 348| loss: 0.11461 | val_0_roc_auc: 0.97109 | val_0_accuracy: 0.91881 |  0:08:30s\n",
            "epoch 349| loss: 0.1012  | val_0_roc_auc: 0.97291 | val_0_accuracy: 0.92522 |  0:08:32s\n",
            "epoch 350| loss: 0.13021 | val_0_roc_auc: 0.9629  | val_0_accuracy: 0.90597 |  0:08:33s\n",
            "epoch 351| loss: 0.14879 | val_0_roc_auc: 0.96244 | val_0_accuracy: 0.90375 |  0:08:35s\n",
            "epoch 352| loss: 0.13146 | val_0_roc_auc: 0.97042 | val_0_accuracy: 0.91634 |  0:08:36s\n",
            "epoch 353| loss: 0.11299 | val_0_roc_auc: 0.97172 | val_0_accuracy: 0.9151  |  0:08:38s\n",
            "epoch 354| loss: 0.09617 | val_0_roc_auc: 0.97354 | val_0_accuracy: 0.92201 |  0:08:40s\n",
            "epoch 355| loss: 0.1357  | val_0_roc_auc: 0.9635  | val_0_accuracy: 0.90054 |  0:08:41s\n",
            "epoch 356| loss: 0.14531 | val_0_roc_auc: 0.96727 | val_0_accuracy: 0.90301 |  0:08:43s\n",
            "epoch 357| loss: 0.12822 | val_0_roc_auc: 0.96957 | val_0_accuracy: 0.91017 |  0:08:44s\n",
            "epoch 358| loss: 0.11004 | val_0_roc_auc: 0.97242 | val_0_accuracy: 0.91683 |  0:08:46s\n",
            "epoch 359| loss: 0.09657 | val_0_roc_auc: 0.97424 | val_0_accuracy: 0.92029 |  0:08:47s\n",
            "epoch 360| loss: 0.13194 | val_0_roc_auc: 0.9579  | val_0_accuracy: 0.89832 |  0:08:49s\n",
            "epoch 361| loss: 0.14902 | val_0_roc_auc: 0.96256 | val_0_accuracy: 0.90276 |  0:08:50s\n",
            "epoch 362| loss: 0.12948 | val_0_roc_auc: 0.9693  | val_0_accuracy: 0.91436 |  0:08:52s\n",
            "epoch 363| loss: 0.10865 | val_0_roc_auc: 0.97132 | val_0_accuracy: 0.92177 |  0:08:53s\n",
            "epoch 364| loss: 0.09846 | val_0_roc_auc: 0.97384 | val_0_accuracy: 0.92596 |  0:08:54s\n",
            "epoch 365| loss: 0.13183 | val_0_roc_auc: 0.96292 | val_0_accuracy: 0.8998  |  0:08:56s\n",
            "epoch 366| loss: 0.14856 | val_0_roc_auc: 0.96222 | val_0_accuracy: 0.90005 |  0:08:57s\n",
            "epoch 367| loss: 0.12397 | val_0_roc_auc: 0.96742 | val_0_accuracy: 0.91313 |  0:08:59s\n",
            "epoch 368| loss: 0.11008 | val_0_roc_auc: 0.97063 | val_0_accuracy: 0.91634 |  0:09:00s\n",
            "epoch 369| loss: 0.09433 | val_0_roc_auc: 0.97259 | val_0_accuracy: 0.92251 |  0:09:02s\n",
            "epoch 370| loss: 0.13489 | val_0_roc_auc: 0.96194 | val_0_accuracy: 0.90301 |  0:09:03s\n",
            "epoch 371| loss: 0.141   | val_0_roc_auc: 0.96424 | val_0_accuracy: 0.90918 |  0:09:05s\n",
            "epoch 372| loss: 0.12516 | val_0_roc_auc: 0.96976 | val_0_accuracy: 0.91584 |  0:09:06s\n",
            "epoch 373| loss: 0.10761 | val_0_roc_auc: 0.97074 | val_0_accuracy: 0.92053 |  0:09:08s\n",
            "epoch 374| loss: 0.0933  | val_0_roc_auc: 0.97285 | val_0_accuracy: 0.92448 |  0:09:09s\n",
            "epoch 375| loss: 0.13727 | val_0_roc_auc: 0.96154 | val_0_accuracy: 0.89783 |  0:09:11s\n",
            "epoch 376| loss: 0.15145 | val_0_roc_auc: 0.96657 | val_0_accuracy: 0.9077  |  0:09:12s\n",
            "epoch 377| loss: 0.12706 | val_0_roc_auc: 0.96623 | val_0_accuracy: 0.91239 |  0:09:13s\n",
            "epoch 378| loss: 0.10712 | val_0_roc_auc: 0.97071 | val_0_accuracy: 0.91658 |  0:09:15s\n",
            "epoch 379| loss: 0.09443 | val_0_roc_auc: 0.97228 | val_0_accuracy: 0.92201 |  0:09:16s\n",
            "epoch 380| loss: 0.12265 | val_0_roc_auc: 0.95734 | val_0_accuracy: 0.89635 |  0:09:18s\n",
            "epoch 381| loss: 0.1409  | val_0_roc_auc: 0.96097 | val_0_accuracy: 0.90992 |  0:09:19s\n",
            "epoch 382| loss: 0.12833 | val_0_roc_auc: 0.96669 | val_0_accuracy: 0.91609 |  0:09:21s\n",
            "epoch 383| loss: 0.11422 | val_0_roc_auc: 0.96902 | val_0_accuracy: 0.91881 |  0:09:22s\n",
            "epoch 384| loss: 0.09657 | val_0_roc_auc: 0.97138 | val_0_accuracy: 0.92498 |  0:09:24s\n",
            "epoch 385| loss: 0.12215 | val_0_roc_auc: 0.96138 | val_0_accuracy: 0.90671 |  0:09:25s\n",
            "epoch 386| loss: 0.14611 | val_0_roc_auc: 0.96188 | val_0_accuracy: 0.90449 |  0:09:27s\n",
            "epoch 387| loss: 0.1325  | val_0_roc_auc: 0.9659  | val_0_accuracy: 0.91708 |  0:09:28s\n",
            "epoch 388| loss: 0.10641 | val_0_roc_auc: 0.96982 | val_0_accuracy: 0.92078 |  0:09:29s\n",
            "epoch 389| loss: 0.09454 | val_0_roc_auc: 0.97168 | val_0_accuracy: 0.92349 |  0:09:31s\n",
            "epoch 390| loss: 0.12394 | val_0_roc_auc: 0.96083 | val_0_accuracy: 0.90819 |  0:09:32s\n",
            "epoch 391| loss: 0.13777 | val_0_roc_auc: 0.96117 | val_0_accuracy: 0.91091 |  0:09:34s\n",
            "epoch 392| loss: 0.12129 | val_0_roc_auc: 0.9674  | val_0_accuracy: 0.91807 |  0:09:35s\n",
            "epoch 393| loss: 0.10118 | val_0_roc_auc: 0.96941 | val_0_accuracy: 0.92275 |  0:09:37s\n",
            "epoch 394| loss: 0.09428 | val_0_roc_auc: 0.97107 | val_0_accuracy: 0.9267  |  0:09:38s\n",
            "epoch 395| loss: 0.12306 | val_0_roc_auc: 0.95839 | val_0_accuracy: 0.89783 |  0:09:40s\n",
            "epoch 396| loss: 0.14698 | val_0_roc_auc: 0.96484 | val_0_accuracy: 0.90893 |  0:09:41s\n",
            "epoch 397| loss: 0.12105 | val_0_roc_auc: 0.96602 | val_0_accuracy: 0.91436 |  0:09:42s\n",
            "epoch 398| loss: 0.10453 | val_0_roc_auc: 0.97128 | val_0_accuracy: 0.92275 |  0:09:44s\n",
            "epoch 399| loss: 0.09183 | val_0_roc_auc: 0.97126 | val_0_accuracy: 0.92325 |  0:09:45s\n",
            "epoch 400| loss: 0.12398 | val_0_roc_auc: 0.96529 | val_0_accuracy: 0.90745 |  0:09:47s\n",
            "epoch 401| loss: 0.13572 | val_0_roc_auc: 0.96134 | val_0_accuracy: 0.89906 |  0:09:48s\n",
            "epoch 402| loss: 0.12789 | val_0_roc_auc: 0.96827 | val_0_accuracy: 0.91288 |  0:09:50s\n",
            "epoch 403| loss: 0.10257 | val_0_roc_auc: 0.96867 | val_0_accuracy: 0.91732 |  0:09:51s\n",
            "epoch 404| loss: 0.09218 | val_0_roc_auc: 0.97082 | val_0_accuracy: 0.923   |  0:09:53s\n",
            "epoch 405| loss: 0.11779 | val_0_roc_auc: 0.96103 | val_0_accuracy: 0.90573 |  0:09:54s\n",
            "epoch 406| loss: 0.135   | val_0_roc_auc: 0.96512 | val_0_accuracy: 0.90745 |  0:09:56s\n",
            "epoch 407| loss: 0.11206 | val_0_roc_auc: 0.96784 | val_0_accuracy: 0.91412 |  0:09:57s\n",
            "epoch 408| loss: 0.10344 | val_0_roc_auc: 0.97081 | val_0_accuracy: 0.91979 |  0:09:59s\n",
            "epoch 409| loss: 0.09001 | val_0_roc_auc: 0.97242 | val_0_accuracy: 0.92349 |  0:10:00s\n",
            "epoch 410| loss: 0.12328 | val_0_roc_auc: 0.95902 | val_0_accuracy: 0.89413 |  0:10:02s\n",
            "epoch 411| loss: 0.14706 | val_0_roc_auc: 0.96627 | val_0_accuracy: 0.91288 |  0:10:03s\n",
            "epoch 412| loss: 0.12013 | val_0_roc_auc: 0.96611 | val_0_accuracy: 0.91535 |  0:10:04s\n",
            "epoch 413| loss: 0.10279 | val_0_roc_auc: 0.97022 | val_0_accuracy: 0.9267  |  0:10:06s\n",
            "epoch 414| loss: 0.09139 | val_0_roc_auc: 0.97224 | val_0_accuracy: 0.92596 |  0:10:07s\n",
            "epoch 415| loss: 0.1151  | val_0_roc_auc: 0.95956 | val_0_accuracy: 0.89882 |  0:10:09s\n",
            "epoch 416| loss: 0.13649 | val_0_roc_auc: 0.96287 | val_0_accuracy: 0.91214 |  0:10:10s\n",
            "epoch 417| loss: 0.12039 | val_0_roc_auc: 0.96646 | val_0_accuracy: 0.91338 |  0:10:12s\n",
            "epoch 418| loss: 0.09996 | val_0_roc_auc: 0.96845 | val_0_accuracy: 0.91313 |  0:10:13s\n",
            "epoch 419| loss: 0.09055 | val_0_roc_auc: 0.97136 | val_0_accuracy: 0.92078 |  0:10:15s\n",
            "epoch 420| loss: 0.12264 | val_0_roc_auc: 0.96375 | val_0_accuracy: 0.90819 |  0:10:16s\n",
            "epoch 421| loss: 0.14347 | val_0_roc_auc: 0.96238 | val_0_accuracy: 0.90622 |  0:10:17s\n",
            "epoch 422| loss: 0.12065 | val_0_roc_auc: 0.96652 | val_0_accuracy: 0.91412 |  0:10:19s\n",
            "epoch 423| loss: 0.10058 | val_0_roc_auc: 0.96985 | val_0_accuracy: 0.92275 |  0:10:20s\n",
            "epoch 424| loss: 0.0897  | val_0_roc_auc: 0.97141 | val_0_accuracy: 0.92374 |  0:10:22s\n",
            "epoch 425| loss: 0.12903 | val_0_roc_auc: 0.95808 | val_0_accuracy: 0.90005 |  0:10:24s\n",
            "epoch 426| loss: 0.13756 | val_0_roc_auc: 0.96256 | val_0_accuracy: 0.91165 |  0:10:26s\n",
            "epoch 427| loss: 0.12331 | val_0_roc_auc: 0.96547 | val_0_accuracy: 0.91264 |  0:10:27s\n",
            "epoch 428| loss: 0.10451 | val_0_roc_auc: 0.96864 | val_0_accuracy: 0.92152 |  0:10:28s\n",
            "epoch 429| loss: 0.08715 | val_0_roc_auc: 0.97027 | val_0_accuracy: 0.92325 |  0:10:30s\n",
            "epoch 430| loss: 0.11446 | val_0_roc_auc: 0.96452 | val_0_accuracy: 0.90844 |  0:10:31s\n",
            "epoch 431| loss: 0.12935 | val_0_roc_auc: 0.96509 | val_0_accuracy: 0.91091 |  0:10:33s\n",
            "epoch 432| loss: 0.11489 | val_0_roc_auc: 0.96593 | val_0_accuracy: 0.91387 |  0:10:34s\n",
            "epoch 433| loss: 0.09854 | val_0_roc_auc: 0.96863 | val_0_accuracy: 0.91979 |  0:10:36s\n",
            "epoch 434| loss: 0.08742 | val_0_roc_auc: 0.97056 | val_0_accuracy: 0.92596 |  0:10:37s\n",
            "epoch 435| loss: 0.12291 | val_0_roc_auc: 0.96131 | val_0_accuracy: 0.90424 |  0:10:39s\n",
            "epoch 436| loss: 0.12914 | val_0_roc_auc: 0.9641  | val_0_accuracy: 0.90449 |  0:10:40s\n",
            "epoch 437| loss: 0.11838 | val_0_roc_auc: 0.96678 | val_0_accuracy: 0.91264 |  0:10:42s\n",
            "epoch 438| loss: 0.09735 | val_0_roc_auc: 0.9723  | val_0_accuracy: 0.923   |  0:10:43s\n",
            "epoch 439| loss: 0.08565 | val_0_roc_auc: 0.97362 | val_0_accuracy: 0.92522 |  0:10:44s\n",
            "epoch 440| loss: 0.12188 | val_0_roc_auc: 0.95966 | val_0_accuracy: 0.90424 |  0:10:46s\n",
            "epoch 441| loss: 0.13506 | val_0_roc_auc: 0.96267 | val_0_accuracy: 0.91288 |  0:10:47s\n",
            "epoch 442| loss: 0.11692 | val_0_roc_auc: 0.96828 | val_0_accuracy: 0.91955 |  0:10:49s\n",
            "epoch 443| loss: 0.09726 | val_0_roc_auc: 0.96988 | val_0_accuracy: 0.92646 |  0:10:50s\n",
            "epoch 444| loss: 0.08211 | val_0_roc_auc: 0.97129 | val_0_accuracy: 0.92818 |  0:10:52s\n",
            "epoch 445| loss: 0.11497 | val_0_roc_auc: 0.9605  | val_0_accuracy: 0.90499 |  0:10:53s\n",
            "epoch 446| loss: 0.12645 | val_0_roc_auc: 0.96481 | val_0_accuracy: 0.91634 |  0:10:55s\n",
            "epoch 447| loss: 0.11039 | val_0_roc_auc: 0.96618 | val_0_accuracy: 0.91313 |  0:10:56s\n",
            "epoch 448| loss: 0.09937 | val_0_roc_auc: 0.96858 | val_0_accuracy: 0.91732 |  0:10:58s\n",
            "epoch 449| loss: 0.08702 | val_0_roc_auc: 0.97175 | val_0_accuracy: 0.92818 |  0:10:59s\n",
            "epoch 450| loss: 0.12126 | val_0_roc_auc: 0.96222 | val_0_accuracy: 0.90967 |  0:11:00s\n",
            "epoch 451| loss: 0.13631 | val_0_roc_auc: 0.96128 | val_0_accuracy: 0.90523 |  0:11:02s\n",
            "epoch 452| loss: 0.11924 | val_0_roc_auc: 0.96665 | val_0_accuracy: 0.91881 |  0:11:03s\n",
            "epoch 453| loss: 0.09895 | val_0_roc_auc: 0.97073 | val_0_accuracy: 0.92349 |  0:11:05s\n",
            "epoch 454| loss: 0.08673 | val_0_roc_auc: 0.97057 | val_0_accuracy: 0.92201 |  0:11:06s\n",
            "epoch 455| loss: 0.10588 | val_0_roc_auc: 0.95867 | val_0_accuracy: 0.9035  |  0:11:08s\n",
            "epoch 456| loss: 0.13783 | val_0_roc_auc: 0.96144 | val_0_accuracy: 0.90301 |  0:11:09s\n",
            "epoch 457| loss: 0.11569 | val_0_roc_auc: 0.96848 | val_0_accuracy: 0.92103 |  0:11:11s\n",
            "epoch 458| loss: 0.0953  | val_0_roc_auc: 0.97064 | val_0_accuracy: 0.92275 |  0:11:12s\n",
            "epoch 459| loss: 0.08441 | val_0_roc_auc: 0.97222 | val_0_accuracy: 0.92991 |  0:11:14s\n",
            "epoch 460| loss: 0.117   | val_0_roc_auc: 0.9629  | val_0_accuracy: 0.91041 |  0:11:15s\n",
            "epoch 461| loss: 0.12922 | val_0_roc_auc: 0.96358 | val_0_accuracy: 0.90647 |  0:11:16s\n",
            "epoch 462| loss: 0.11522 | val_0_roc_auc: 0.96803 | val_0_accuracy: 0.91807 |  0:11:18s\n",
            "epoch 463| loss: 0.09548 | val_0_roc_auc: 0.97001 | val_0_accuracy: 0.92547 |  0:11:19s\n",
            "epoch 464| loss: 0.08345 | val_0_roc_auc: 0.9718  | val_0_accuracy: 0.92695 |  0:11:21s\n",
            "epoch 465| loss: 0.11286 | val_0_roc_auc: 0.96349 | val_0_accuracy: 0.90918 |  0:11:22s\n",
            "epoch 466| loss: 0.12349 | val_0_roc_auc: 0.96515 | val_0_accuracy: 0.91239 |  0:11:24s\n",
            "epoch 467| loss: 0.11498 | val_0_roc_auc: 0.96675 | val_0_accuracy: 0.91609 |  0:11:25s\n",
            "epoch 468| loss: 0.09874 | val_0_roc_auc: 0.96901 | val_0_accuracy: 0.92177 |  0:11:27s\n",
            "epoch 469| loss: 0.08111 | val_0_roc_auc: 0.97258 | val_0_accuracy: 0.92843 |  0:11:28s\n",
            "epoch 470| loss: 0.12405 | val_0_roc_auc: 0.96009 | val_0_accuracy: 0.90869 |  0:11:30s\n",
            "epoch 471| loss: 0.13764 | val_0_roc_auc: 0.95971 | val_0_accuracy: 0.90301 |  0:11:31s\n",
            "epoch 472| loss: 0.11696 | val_0_roc_auc: 0.9683  | val_0_accuracy: 0.91905 |  0:11:32s\n",
            "epoch 473| loss: 0.09392 | val_0_roc_auc: 0.96983 | val_0_accuracy: 0.92078 |  0:11:34s\n",
            "epoch 474| loss: 0.08178 | val_0_roc_auc: 0.97045 | val_0_accuracy: 0.92522 |  0:11:35s\n",
            "epoch 475| loss: 0.11181 | val_0_roc_auc: 0.96198 | val_0_accuracy: 0.90005 |  0:11:37s\n",
            "epoch 476| loss: 0.13888 | val_0_roc_auc: 0.96523 | val_0_accuracy: 0.91288 |  0:11:38s\n",
            "epoch 477| loss: 0.11068 | val_0_roc_auc: 0.96837 | val_0_accuracy: 0.9151  |  0:11:40s\n",
            "epoch 478| loss: 0.0937  | val_0_roc_auc: 0.97034 | val_0_accuracy: 0.92251 |  0:11:41s\n",
            "epoch 479| loss: 0.08261 | val_0_roc_auc: 0.97167 | val_0_accuracy: 0.92522 |  0:11:43s\n",
            "epoch 480| loss: 0.11871 | val_0_roc_auc: 0.96228 | val_0_accuracy: 0.90745 |  0:11:44s\n",
            "epoch 481| loss: 0.12943 | val_0_roc_auc: 0.9626  | val_0_accuracy: 0.90967 |  0:11:46s\n",
            "epoch 482| loss: 0.11759 | val_0_roc_auc: 0.96853 | val_0_accuracy: 0.91165 |  0:11:47s\n",
            "epoch 483| loss: 0.09603 | val_0_roc_auc: 0.97104 | val_0_accuracy: 0.92226 |  0:11:48s\n",
            "epoch 484| loss: 0.0815  | val_0_roc_auc: 0.97287 | val_0_accuracy: 0.92374 |  0:11:50s\n",
            "epoch 485| loss: 0.12014 | val_0_roc_auc: 0.96287 | val_0_accuracy: 0.90671 |  0:11:51s\n",
            "epoch 486| loss: 0.12485 | val_0_roc_auc: 0.96596 | val_0_accuracy: 0.91362 |  0:11:53s\n",
            "epoch 487| loss: 0.10805 | val_0_roc_auc: 0.96872 | val_0_accuracy: 0.91881 |  0:11:54s\n",
            "epoch 488| loss: 0.09657 | val_0_roc_auc: 0.97363 | val_0_accuracy: 0.92572 |  0:11:56s\n",
            "epoch 489| loss: 0.08284 | val_0_roc_auc: 0.97504 | val_0_accuracy: 0.9272  |  0:11:57s\n",
            "epoch 490| loss: 0.11918 | val_0_roc_auc: 0.96428 | val_0_accuracy: 0.91436 |  0:11:59s\n",
            "epoch 491| loss: 0.12977 | val_0_roc_auc: 0.96566 | val_0_accuracy: 0.90696 |  0:12:00s\n",
            "epoch 492| loss: 0.10914 | val_0_roc_auc: 0.96954 | val_0_accuracy: 0.92596 |  0:12:02s\n",
            "epoch 493| loss: 0.09297 | val_0_roc_auc: 0.97299 | val_0_accuracy: 0.92991 |  0:12:04s\n",
            "epoch 494| loss: 0.07978 | val_0_roc_auc: 0.9735  | val_0_accuracy: 0.92966 |  0:12:05s\n",
            "epoch 495| loss: 0.10711 | val_0_roc_auc: 0.96187 | val_0_accuracy: 0.9077  |  0:12:07s\n",
            "epoch 496| loss: 0.12201 | val_0_roc_auc: 0.96342 | val_0_accuracy: 0.9114  |  0:12:08s\n",
            "epoch 497| loss: 0.10799 | val_0_roc_auc: 0.96688 | val_0_accuracy: 0.91683 |  0:12:10s\n",
            "epoch 498| loss: 0.09622 | val_0_roc_auc: 0.97105 | val_0_accuracy: 0.92078 |  0:12:11s\n",
            "epoch 499| loss: 0.08113 | val_0_roc_auc: 0.97235 | val_0_accuracy: 0.92399 |  0:12:13s\n",
            "Stop training because you reached max_epochs = 500 with best_epoch = 459 and best_val_0_accuracy = 0.92991\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KFold 5 of 5\n",
            "epoch 0  | loss: 0.77744 | val_0_roc_auc: 0.72564 | val_0_accuracy: 0.65934 |  0:00:01s\n",
            "epoch 1  | loss: 0.60641 | val_0_roc_auc: 0.77136 | val_0_accuracy: 0.69341 |  0:00:02s\n",
            "epoch 2  | loss: 0.56581 | val_0_roc_auc: 0.79784 | val_0_accuracy: 0.7176  |  0:00:04s\n",
            "epoch 3  | loss: 0.54579 | val_0_roc_auc: 0.80705 | val_0_accuracy: 0.72649 |  0:00:05s\n",
            "epoch 4  | loss: 0.53473 | val_0_roc_auc: 0.81252 | val_0_accuracy: 0.73513 |  0:00:07s\n",
            "epoch 5  | loss: 0.52505 | val_0_roc_auc: 0.82361 | val_0_accuracy: 0.75019 |  0:00:08s\n",
            "epoch 6  | loss: 0.50409 | val_0_roc_auc: 0.83497 | val_0_accuracy: 0.75241 |  0:00:10s\n",
            "epoch 7  | loss: 0.49124 | val_0_roc_auc: 0.83953 | val_0_accuracy: 0.75907 |  0:00:11s\n",
            "epoch 8  | loss: 0.48037 | val_0_roc_auc: 0.84564 | val_0_accuracy: 0.7645  |  0:00:13s\n",
            "epoch 9  | loss: 0.4702  | val_0_roc_auc: 0.8476  | val_0_accuracy: 0.76302 |  0:00:14s\n",
            "epoch 10 | loss: 0.47074 | val_0_roc_auc: 0.84914 | val_0_accuracy: 0.76401 |  0:00:15s\n",
            "epoch 11 | loss: 0.45855 | val_0_roc_auc: 0.85338 | val_0_accuracy: 0.7724  |  0:00:17s\n",
            "epoch 12 | loss: 0.45465 | val_0_roc_auc: 0.85936 | val_0_accuracy: 0.77487 |  0:00:18s\n",
            "epoch 13 | loss: 0.44203 | val_0_roc_auc: 0.86343 | val_0_accuracy: 0.77833 |  0:00:20s\n",
            "epoch 14 | loss: 0.4385  | val_0_roc_auc: 0.86393 | val_0_accuracy: 0.77833 |  0:00:21s\n",
            "epoch 15 | loss: 0.44201 | val_0_roc_auc: 0.86511 | val_0_accuracy: 0.7766  |  0:00:23s\n",
            "epoch 16 | loss: 0.43164 | val_0_roc_auc: 0.87033 | val_0_accuracy: 0.78302 |  0:00:24s\n",
            "epoch 17 | loss: 0.42581 | val_0_roc_auc: 0.87657 | val_0_accuracy: 0.78598 |  0:00:26s\n",
            "epoch 18 | loss: 0.41618 | val_0_roc_auc: 0.87858 | val_0_accuracy: 0.7882  |  0:00:27s\n",
            "epoch 19 | loss: 0.40616 | val_0_roc_auc: 0.87992 | val_0_accuracy: 0.7924  |  0:00:28s\n",
            "epoch 20 | loss: 0.40656 | val_0_roc_auc: 0.87975 | val_0_accuracy: 0.79585 |  0:00:30s\n",
            "epoch 21 | loss: 0.4066  | val_0_roc_auc: 0.88372 | val_0_accuracy: 0.79832 |  0:00:31s\n",
            "epoch 22 | loss: 0.3966  | val_0_roc_auc: 0.88635 | val_0_accuracy: 0.79783 |  0:00:33s\n",
            "epoch 23 | loss: 0.38841 | val_0_roc_auc: 0.89085 | val_0_accuracy: 0.80597 |  0:00:34s\n",
            "epoch 24 | loss: 0.37895 | val_0_roc_auc: 0.89312 | val_0_accuracy: 0.81066 |  0:00:36s\n",
            "epoch 25 | loss: 0.39064 | val_0_roc_auc: 0.88969 | val_0_accuracy: 0.79659 |  0:00:37s\n",
            "epoch 26 | loss: 0.38332 | val_0_roc_auc: 0.89958 | val_0_accuracy: 0.81634 |  0:00:39s\n",
            "epoch 27 | loss: 0.37788 | val_0_roc_auc: 0.90217 | val_0_accuracy: 0.81733 |  0:00:40s\n",
            "epoch 28 | loss: 0.3642  | val_0_roc_auc: 0.90445 | val_0_accuracy: 0.82251 |  0:00:41s\n",
            "epoch 29 | loss: 0.35003 | val_0_roc_auc: 0.90656 | val_0_accuracy: 0.82671 |  0:00:43s\n",
            "epoch 30 | loss: 0.36479 | val_0_roc_auc: 0.90246 | val_0_accuracy: 0.82597 |  0:00:44s\n",
            "epoch 31 | loss: 0.3593  | val_0_roc_auc: 0.90446 | val_0_accuracy: 0.82449 |  0:00:46s\n",
            "epoch 32 | loss: 0.35459 | val_0_roc_auc: 0.91021 | val_0_accuracy: 0.8314  |  0:00:47s\n",
            "epoch 33 | loss: 0.33935 | val_0_roc_auc: 0.91072 | val_0_accuracy: 0.82893 |  0:00:49s\n",
            "epoch 34 | loss: 0.33156 | val_0_roc_auc: 0.91679 | val_0_accuracy: 0.83584 |  0:00:50s\n",
            "epoch 35 | loss: 0.3497  | val_0_roc_auc: 0.9125  | val_0_accuracy: 0.83017 |  0:00:52s\n",
            "epoch 36 | loss: 0.34645 | val_0_roc_auc: 0.91305 | val_0_accuracy: 0.82942 |  0:00:53s\n",
            "epoch 37 | loss: 0.33449 | val_0_roc_auc: 0.91639 | val_0_accuracy: 0.83757 |  0:00:55s\n",
            "epoch 38 | loss: 0.3251  | val_0_roc_auc: 0.91953 | val_0_accuracy: 0.83856 |  0:00:56s\n",
            "epoch 39 | loss: 0.31156 | val_0_roc_auc: 0.92216 | val_0_accuracy: 0.84424 |  0:00:57s\n",
            "epoch 40 | loss: 0.32832 | val_0_roc_auc: 0.90932 | val_0_accuracy: 0.82942 |  0:00:59s\n",
            "epoch 41 | loss: 0.32393 | val_0_roc_auc: 0.91461 | val_0_accuracy: 0.83584 |  0:01:00s\n",
            "epoch 42 | loss: 0.31976 | val_0_roc_auc: 0.9159  | val_0_accuracy: 0.83387 |  0:01:02s\n",
            "epoch 43 | loss: 0.30408 | val_0_roc_auc: 0.92462 | val_0_accuracy: 0.84893 |  0:01:03s\n",
            "epoch 44 | loss: 0.28955 | val_0_roc_auc: 0.9272  | val_0_accuracy: 0.84868 |  0:01:05s\n",
            "epoch 45 | loss: 0.31394 | val_0_roc_auc: 0.91193 | val_0_accuracy: 0.83239 |  0:01:06s\n",
            "epoch 46 | loss: 0.31457 | val_0_roc_auc: 0.92545 | val_0_accuracy: 0.85016 |  0:01:07s\n",
            "epoch 47 | loss: 0.30107 | val_0_roc_auc: 0.92611 | val_0_accuracy: 0.84819 |  0:01:09s\n",
            "epoch 48 | loss: 0.28477 | val_0_roc_auc: 0.9312  | val_0_accuracy: 0.85831 |  0:01:10s\n",
            "epoch 49 | loss: 0.27022 | val_0_roc_auc: 0.93488 | val_0_accuracy: 0.86374 |  0:01:12s\n",
            "epoch 50 | loss: 0.30289 | val_0_roc_auc: 0.92088 | val_0_accuracy: 0.84251 |  0:01:13s\n",
            "epoch 51 | loss: 0.30363 | val_0_roc_auc: 0.92232 | val_0_accuracy: 0.84917 |  0:01:15s\n",
            "epoch 52 | loss: 0.29595 | val_0_roc_auc: 0.92767 | val_0_accuracy: 0.84967 |  0:01:16s\n",
            "epoch 53 | loss: 0.27464 | val_0_roc_auc: 0.93519 | val_0_accuracy: 0.86522 |  0:01:18s\n",
            "epoch 54 | loss: 0.25719 | val_0_roc_auc: 0.93567 | val_0_accuracy: 0.86571 |  0:01:19s\n",
            "epoch 55 | loss: 0.28003 | val_0_roc_auc: 0.92551 | val_0_accuracy: 0.84819 |  0:01:20s\n",
            "epoch 56 | loss: 0.28963 | val_0_roc_auc: 0.92978 | val_0_accuracy: 0.85386 |  0:01:22s\n",
            "epoch 57 | loss: 0.27993 | val_0_roc_auc: 0.93224 | val_0_accuracy: 0.85979 |  0:01:23s\n",
            "epoch 58 | loss: 0.25916 | val_0_roc_auc: 0.93603 | val_0_accuracy: 0.86374 |  0:01:25s\n",
            "epoch 59 | loss: 0.24373 | val_0_roc_auc: 0.94157 | val_0_accuracy: 0.87608 |  0:01:26s\n",
            "epoch 60 | loss: 0.27897 | val_0_roc_auc: 0.92354 | val_0_accuracy: 0.84991 |  0:01:28s\n",
            "epoch 61 | loss: 0.27717 | val_0_roc_auc: 0.92721 | val_0_accuracy: 0.84794 |  0:01:29s\n",
            "epoch 62 | loss: 0.26404 | val_0_roc_auc: 0.93698 | val_0_accuracy: 0.85806 |  0:01:31s\n",
            "epoch 63 | loss: 0.24535 | val_0_roc_auc: 0.94157 | val_0_accuracy: 0.87559 |  0:01:32s\n",
            "epoch 64 | loss: 0.23134 | val_0_roc_auc: 0.94368 | val_0_accuracy: 0.87657 |  0:01:34s\n",
            "epoch 65 | loss: 0.26669 | val_0_roc_auc: 0.92808 | val_0_accuracy: 0.85584 |  0:01:36s\n",
            "epoch 66 | loss: 0.26772 | val_0_roc_auc: 0.93376 | val_0_accuracy: 0.86547 |  0:01:37s\n",
            "epoch 67 | loss: 0.25089 | val_0_roc_auc: 0.93931 | val_0_accuracy: 0.86941 |  0:01:39s\n",
            "epoch 68 | loss: 0.23529 | val_0_roc_auc: 0.94258 | val_0_accuracy: 0.87435 |  0:01:40s\n",
            "epoch 69 | loss: 0.21944 | val_0_roc_auc: 0.94448 | val_0_accuracy: 0.87929 |  0:01:42s\n",
            "epoch 70 | loss: 0.25188 | val_0_roc_auc: 0.93573 | val_0_accuracy: 0.86398 |  0:01:43s\n",
            "epoch 71 | loss: 0.26687 | val_0_roc_auc: 0.93774 | val_0_accuracy: 0.86324 |  0:01:44s\n",
            "epoch 72 | loss: 0.24717 | val_0_roc_auc: 0.94325 | val_0_accuracy: 0.87509 |  0:01:46s\n",
            "epoch 73 | loss: 0.22393 | val_0_roc_auc: 0.9456  | val_0_accuracy: 0.87904 |  0:01:47s\n",
            "epoch 74 | loss: 0.20822 | val_0_roc_auc: 0.9488  | val_0_accuracy: 0.88818 |  0:01:49s\n",
            "epoch 75 | loss: 0.25327 | val_0_roc_auc: 0.93541 | val_0_accuracy: 0.86695 |  0:01:50s\n",
            "epoch 76 | loss: 0.25558 | val_0_roc_auc: 0.93679 | val_0_accuracy: 0.86818 |  0:01:52s\n",
            "epoch 77 | loss: 0.24227 | val_0_roc_auc: 0.94028 | val_0_accuracy: 0.87287 |  0:01:53s\n",
            "epoch 78 | loss: 0.21834 | val_0_roc_auc: 0.94878 | val_0_accuracy: 0.88398 |  0:01:55s\n",
            "epoch 79 | loss: 0.20535 | val_0_roc_auc: 0.9504  | val_0_accuracy: 0.88719 |  0:01:56s\n",
            "epoch 80 | loss: 0.25174 | val_0_roc_auc: 0.93619 | val_0_accuracy: 0.85929 |  0:01:58s\n",
            "epoch 81 | loss: 0.25112 | val_0_roc_auc: 0.93474 | val_0_accuracy: 0.87336 |  0:01:59s\n",
            "epoch 82 | loss: 0.23205 | val_0_roc_auc: 0.94472 | val_0_accuracy: 0.87559 |  0:02:00s\n",
            "epoch 83 | loss: 0.21709 | val_0_roc_auc: 0.94844 | val_0_accuracy: 0.88299 |  0:02:02s\n",
            "epoch 84 | loss: 0.19592 | val_0_roc_auc: 0.95153 | val_0_accuracy: 0.8862  |  0:02:03s\n",
            "epoch 85 | loss: 0.24211 | val_0_roc_auc: 0.93678 | val_0_accuracy: 0.85855 |  0:02:05s\n",
            "epoch 86 | loss: 0.2364  | val_0_roc_auc: 0.93701 | val_0_accuracy: 0.86769 |  0:02:06s\n",
            "epoch 87 | loss: 0.22534 | val_0_roc_auc: 0.94112 | val_0_accuracy: 0.86892 |  0:02:08s\n",
            "epoch 88 | loss: 0.20593 | val_0_roc_auc: 0.94872 | val_0_accuracy: 0.88595 |  0:02:09s\n",
            "epoch 89 | loss: 0.19313 | val_0_roc_auc: 0.95147 | val_0_accuracy: 0.88744 |  0:02:11s\n",
            "epoch 90 | loss: 0.23274 | val_0_roc_auc: 0.93467 | val_0_accuracy: 0.86596 |  0:02:12s\n",
            "epoch 91 | loss: 0.23474 | val_0_roc_auc: 0.93588 | val_0_accuracy: 0.86596 |  0:02:13s\n",
            "epoch 92 | loss: 0.21862 | val_0_roc_auc: 0.94388 | val_0_accuracy: 0.88028 |  0:02:15s\n",
            "epoch 93 | loss: 0.20143 | val_0_roc_auc: 0.95065 | val_0_accuracy: 0.88941 |  0:02:16s\n",
            "epoch 94 | loss: 0.18691 | val_0_roc_auc: 0.95208 | val_0_accuracy: 0.8899  |  0:02:18s\n",
            "epoch 95 | loss: 0.22501 | val_0_roc_auc: 0.93803 | val_0_accuracy: 0.86818 |  0:02:19s\n",
            "epoch 96 | loss: 0.23817 | val_0_roc_auc: 0.94337 | val_0_accuracy: 0.87361 |  0:02:21s\n",
            "epoch 97 | loss: 0.22171 | val_0_roc_auc: 0.94608 | val_0_accuracy: 0.88225 |  0:02:22s\n",
            "epoch 98 | loss: 0.19847 | val_0_roc_auc: 0.9494  | val_0_accuracy: 0.88423 |  0:02:24s\n",
            "epoch 99 | loss: 0.18332 | val_0_roc_auc: 0.95362 | val_0_accuracy: 0.8904  |  0:02:25s\n",
            "epoch 100| loss: 0.21809 | val_0_roc_auc: 0.94294 | val_0_accuracy: 0.87336 |  0:02:27s\n",
            "epoch 101| loss: 0.22598 | val_0_roc_auc: 0.93846 | val_0_accuracy: 0.87411 |  0:02:28s\n",
            "epoch 102| loss: 0.21451 | val_0_roc_auc: 0.94284 | val_0_accuracy: 0.87336 |  0:02:29s\n",
            "epoch 103| loss: 0.20053 | val_0_roc_auc: 0.9491  | val_0_accuracy: 0.88497 |  0:02:31s\n",
            "epoch 104| loss: 0.17595 | val_0_roc_auc: 0.95238 | val_0_accuracy: 0.89435 |  0:02:32s\n",
            "epoch 105| loss: 0.23002 | val_0_roc_auc: 0.93475 | val_0_accuracy: 0.86645 |  0:02:34s\n",
            "epoch 106| loss: 0.22547 | val_0_roc_auc: 0.94394 | val_0_accuracy: 0.87682 |  0:02:35s\n",
            "epoch 107| loss: 0.21265 | val_0_roc_auc: 0.94536 | val_0_accuracy: 0.87312 |  0:02:37s\n",
            "epoch 108| loss: 0.19144 | val_0_roc_auc: 0.9518  | val_0_accuracy: 0.88719 |  0:02:38s\n",
            "epoch 109| loss: 0.17362 | val_0_roc_auc: 0.9539  | val_0_accuracy: 0.88867 |  0:02:40s\n",
            "epoch 110| loss: 0.21726 | val_0_roc_auc: 0.94081 | val_0_accuracy: 0.87386 |  0:02:41s\n",
            "epoch 111| loss: 0.21955 | val_0_roc_auc: 0.94336 | val_0_accuracy: 0.87583 |  0:02:42s\n",
            "epoch 112| loss: 0.20528 | val_0_roc_auc: 0.94647 | val_0_accuracy: 0.88299 |  0:02:44s\n",
            "epoch 113| loss: 0.18492 | val_0_roc_auc: 0.95127 | val_0_accuracy: 0.89213 |  0:02:45s\n",
            "epoch 114| loss: 0.17117 | val_0_roc_auc: 0.95521 | val_0_accuracy: 0.89608 |  0:02:47s\n",
            "epoch 115| loss: 0.21969 | val_0_roc_auc: 0.94094 | val_0_accuracy: 0.88003 |  0:02:48s\n",
            "epoch 116| loss: 0.22573 | val_0_roc_auc: 0.94528 | val_0_accuracy: 0.88028 |  0:02:50s\n",
            "epoch 117| loss: 0.2063  | val_0_roc_auc: 0.94977 | val_0_accuracy: 0.88744 |  0:02:51s\n",
            "epoch 118| loss: 0.17897 | val_0_roc_auc: 0.95312 | val_0_accuracy: 0.89262 |  0:02:53s\n",
            "epoch 119| loss: 0.16494 | val_0_roc_auc: 0.95537 | val_0_accuracy: 0.89854 |  0:02:54s\n",
            "epoch 120| loss: 0.20863 | val_0_roc_auc: 0.94288 | val_0_accuracy: 0.87435 |  0:02:56s\n",
            "epoch 121| loss: 0.22573 | val_0_roc_auc: 0.94969 | val_0_accuracy: 0.88398 |  0:02:57s\n",
            "epoch 122| loss: 0.2041  | val_0_roc_auc: 0.95057 | val_0_accuracy: 0.88497 |  0:02:58s\n",
            "epoch 123| loss: 0.18288 | val_0_roc_auc: 0.95446 | val_0_accuracy: 0.89657 |  0:03:00s\n",
            "epoch 124| loss: 0.16322 | val_0_roc_auc: 0.95674 | val_0_accuracy: 0.89953 |  0:03:01s\n",
            "epoch 125| loss: 0.20764 | val_0_roc_auc: 0.94162 | val_0_accuracy: 0.87559 |  0:03:03s\n",
            "epoch 126| loss: 0.21536 | val_0_roc_auc: 0.94626 | val_0_accuracy: 0.88176 |  0:03:04s\n",
            "epoch 127| loss: 0.19277 | val_0_roc_auc: 0.94951 | val_0_accuracy: 0.88818 |  0:03:06s\n",
            "epoch 128| loss: 0.1808  | val_0_roc_auc: 0.95334 | val_0_accuracy: 0.89632 |  0:03:07s\n",
            "epoch 129| loss: 0.16171 | val_0_roc_auc: 0.95867 | val_0_accuracy: 0.90299 |  0:03:09s\n",
            "epoch 130| loss: 0.2128  | val_0_roc_auc: 0.94167 | val_0_accuracy: 0.87386 |  0:03:10s\n",
            "epoch 131| loss: 0.21412 | val_0_roc_auc: 0.94704 | val_0_accuracy: 0.88028 |  0:03:12s\n",
            "epoch 132| loss: 0.19831 | val_0_roc_auc: 0.94873 | val_0_accuracy: 0.89015 |  0:03:14s\n",
            "epoch 133| loss: 0.17545 | val_0_roc_auc: 0.95635 | val_0_accuracy: 0.89805 |  0:03:15s\n",
            "epoch 134| loss: 0.15606 | val_0_roc_auc: 0.95756 | val_0_accuracy: 0.90101 |  0:03:17s\n",
            "epoch 135| loss: 0.20139 | val_0_roc_auc: 0.94123 | val_0_accuracy: 0.87164 |  0:03:18s\n",
            "epoch 136| loss: 0.2067  | val_0_roc_auc: 0.94437 | val_0_accuracy: 0.88497 |  0:03:20s\n",
            "epoch 137| loss: 0.19399 | val_0_roc_auc: 0.95186 | val_0_accuracy: 0.89682 |  0:03:21s\n",
            "epoch 138| loss: 0.16741 | val_0_roc_auc: 0.95562 | val_0_accuracy: 0.89632 |  0:03:22s\n",
            "epoch 139| loss: 0.15416 | val_0_roc_auc: 0.95894 | val_0_accuracy: 0.902   |  0:03:24s\n",
            "epoch 140| loss: 0.20283 | val_0_roc_auc: 0.94326 | val_0_accuracy: 0.87781 |  0:03:25s\n",
            "epoch 141| loss: 0.20952 | val_0_roc_auc: 0.94673 | val_0_accuracy: 0.8862  |  0:03:27s\n",
            "epoch 142| loss: 0.1913  | val_0_roc_auc: 0.94942 | val_0_accuracy: 0.88941 |  0:03:28s\n",
            "epoch 143| loss: 0.16952 | val_0_roc_auc: 0.95465 | val_0_accuracy: 0.8978  |  0:03:30s\n",
            "epoch 144| loss: 0.15771 | val_0_roc_auc: 0.95821 | val_0_accuracy: 0.90249 |  0:03:31s\n",
            "epoch 145| loss: 0.19812 | val_0_roc_auc: 0.94553 | val_0_accuracy: 0.87855 |  0:03:33s\n",
            "epoch 146| loss: 0.20153 | val_0_roc_auc: 0.94694 | val_0_accuracy: 0.88571 |  0:03:34s\n",
            "epoch 147| loss: 0.19395 | val_0_roc_auc: 0.95154 | val_0_accuracy: 0.88842 |  0:03:35s\n",
            "epoch 148| loss: 0.17508 | val_0_roc_auc: 0.95551 | val_0_accuracy: 0.90052 |  0:03:37s\n",
            "epoch 149| loss: 0.1528  | val_0_roc_auc: 0.95851 | val_0_accuracy: 0.90743 |  0:03:38s\n",
            "epoch 150| loss: 0.19831 | val_0_roc_auc: 0.94864 | val_0_accuracy: 0.88818 |  0:03:40s\n",
            "epoch 151| loss: 0.2025  | val_0_roc_auc: 0.95256 | val_0_accuracy: 0.89163 |  0:03:41s\n",
            "epoch 152| loss: 0.18718 | val_0_roc_auc: 0.95312 | val_0_accuracy: 0.8941  |  0:03:43s\n",
            "epoch 153| loss: 0.16489 | val_0_roc_auc: 0.95843 | val_0_accuracy: 0.90323 |  0:03:44s\n",
            "epoch 154| loss: 0.14706 | val_0_roc_auc: 0.96053 | val_0_accuracy: 0.90792 |  0:03:46s\n",
            "epoch 155| loss: 0.19626 | val_0_roc_auc: 0.94858 | val_0_accuracy: 0.88299 |  0:03:47s\n",
            "epoch 156| loss: 0.20085 | val_0_roc_auc: 0.94945 | val_0_accuracy: 0.88472 |  0:03:49s\n",
            "epoch 157| loss: 0.18208 | val_0_roc_auc: 0.95837 | val_0_accuracy: 0.89706 |  0:03:50s\n",
            "epoch 158| loss: 0.15782 | val_0_roc_auc: 0.96105 | val_0_accuracy: 0.90817 |  0:03:51s\n",
            "epoch 159| loss: 0.14203 | val_0_roc_auc: 0.96196 | val_0_accuracy: 0.91558 |  0:03:53s\n",
            "epoch 160| loss: 0.19581 | val_0_roc_auc: 0.94799 | val_0_accuracy: 0.87509 |  0:03:54s\n",
            "epoch 161| loss: 0.20263 | val_0_roc_auc: 0.94567 | val_0_accuracy: 0.8825  |  0:03:56s\n",
            "epoch 162| loss: 0.17963 | val_0_roc_auc: 0.95551 | val_0_accuracy: 0.90175 |  0:03:57s\n",
            "epoch 163| loss: 0.15943 | val_0_roc_auc: 0.95617 | val_0_accuracy: 0.90225 |  0:03:59s\n",
            "epoch 164| loss: 0.14192 | val_0_roc_auc: 0.95964 | val_0_accuracy: 0.90842 |  0:04:00s\n",
            "epoch 165| loss: 0.1856  | val_0_roc_auc: 0.94473 | val_0_accuracy: 0.87929 |  0:04:02s\n",
            "epoch 166| loss: 0.20405 | val_0_roc_auc: 0.95136 | val_0_accuracy: 0.88645 |  0:04:03s\n",
            "epoch 167| loss: 0.17611 | val_0_roc_auc: 0.95458 | val_0_accuracy: 0.89385 |  0:04:04s\n",
            "epoch 168| loss: 0.16084 | val_0_roc_auc: 0.95975 | val_0_accuracy: 0.90348 |  0:04:06s\n",
            "epoch 169| loss: 0.1394  | val_0_roc_auc: 0.96063 | val_0_accuracy: 0.91064 |  0:04:07s\n",
            "epoch 170| loss: 0.18188 | val_0_roc_auc: 0.9457  | val_0_accuracy: 0.88571 |  0:04:09s\n",
            "epoch 171| loss: 0.20142 | val_0_roc_auc: 0.95276 | val_0_accuracy: 0.88694 |  0:04:10s\n",
            "epoch 172| loss: 0.18098 | val_0_roc_auc: 0.95431 | val_0_accuracy: 0.8983  |  0:04:12s\n",
            "epoch 173| loss: 0.15324 | val_0_roc_auc: 0.95953 | val_0_accuracy: 0.90546 |  0:04:13s\n",
            "epoch 174| loss: 0.14584 | val_0_roc_auc: 0.961   | val_0_accuracy: 0.90792 |  0:04:15s\n",
            "epoch 175| loss: 0.18407 | val_0_roc_auc: 0.95029 | val_0_accuracy: 0.88447 |  0:04:16s\n",
            "epoch 176| loss: 0.19083 | val_0_roc_auc: 0.94948 | val_0_accuracy: 0.89237 |  0:04:17s\n",
            "epoch 177| loss: 0.1704  | val_0_roc_auc: 0.95568 | val_0_accuracy: 0.90052 |  0:04:19s\n",
            "epoch 178| loss: 0.1535  | val_0_roc_auc: 0.95993 | val_0_accuracy: 0.90718 |  0:04:20s\n",
            "epoch 179| loss: 0.14012 | val_0_roc_auc: 0.96141 | val_0_accuracy: 0.90743 |  0:04:22s\n",
            "epoch 180| loss: 0.1761  | val_0_roc_auc: 0.94382 | val_0_accuracy: 0.88275 |  0:04:23s\n",
            "epoch 181| loss: 0.19374 | val_0_roc_auc: 0.94664 | val_0_accuracy: 0.88546 |  0:04:25s\n",
            "epoch 182| loss: 0.17185 | val_0_roc_auc: 0.95645 | val_0_accuracy: 0.90496 |  0:04:26s\n",
            "epoch 183| loss: 0.14655 | val_0_roc_auc: 0.95973 | val_0_accuracy: 0.90718 |  0:04:27s\n",
            "epoch 184| loss: 0.13799 | val_0_roc_auc: 0.96181 | val_0_accuracy: 0.91064 |  0:04:29s\n",
            "epoch 185| loss: 0.18269 | val_0_roc_auc: 0.95034 | val_0_accuracy: 0.89114 |  0:04:30s\n",
            "epoch 186| loss: 0.18625 | val_0_roc_auc: 0.95164 | val_0_accuracy: 0.89064 |  0:04:32s\n",
            "epoch 187| loss: 0.16696 | val_0_roc_auc: 0.95239 | val_0_accuracy: 0.89583 |  0:04:33s\n",
            "epoch 188| loss: 0.14906 | val_0_roc_auc: 0.95792 | val_0_accuracy: 0.9057  |  0:04:35s\n",
            "epoch 189| loss: 0.13388 | val_0_roc_auc: 0.95912 | val_0_accuracy: 0.90866 |  0:04:36s\n",
            "epoch 190| loss: 0.18161 | val_0_roc_auc: 0.94541 | val_0_accuracy: 0.88176 |  0:04:38s\n",
            "epoch 191| loss: 0.18985 | val_0_roc_auc: 0.94803 | val_0_accuracy: 0.88669 |  0:04:39s\n",
            "epoch 192| loss: 0.16826 | val_0_roc_auc: 0.9541  | val_0_accuracy: 0.89854 |  0:04:41s\n",
            "epoch 193| loss: 0.14277 | val_0_roc_auc: 0.95971 | val_0_accuracy: 0.90496 |  0:04:42s\n",
            "epoch 194| loss: 0.13326 | val_0_roc_auc: 0.96117 | val_0_accuracy: 0.91286 |  0:04:44s\n",
            "epoch 195| loss: 0.17019 | val_0_roc_auc: 0.95294 | val_0_accuracy: 0.8941  |  0:04:45s\n",
            "epoch 196| loss: 0.1873  | val_0_roc_auc: 0.95002 | val_0_accuracy: 0.89632 |  0:04:46s\n",
            "epoch 197| loss: 0.16851 | val_0_roc_auc: 0.95331 | val_0_accuracy: 0.89484 |  0:04:48s\n",
            "epoch 198| loss: 0.14306 | val_0_roc_auc: 0.95967 | val_0_accuracy: 0.91163 |  0:04:49s\n",
            "epoch 199| loss: 0.13208 | val_0_roc_auc: 0.96177 | val_0_accuracy: 0.91163 |  0:04:51s\n",
            "epoch 200| loss: 0.16622 | val_0_roc_auc: 0.9473  | val_0_accuracy: 0.89138 |  0:04:52s\n",
            "epoch 201| loss: 0.18903 | val_0_roc_auc: 0.95114 | val_0_accuracy: 0.89459 |  0:04:54s\n",
            "epoch 202| loss: 0.16698 | val_0_roc_auc: 0.95412 | val_0_accuracy: 0.89435 |  0:04:55s\n",
            "epoch 203| loss: 0.14351 | val_0_roc_auc: 0.95934 | val_0_accuracy: 0.90348 |  0:04:57s\n",
            "epoch 204| loss: 0.12632 | val_0_roc_auc: 0.96254 | val_0_accuracy: 0.91261 |  0:04:58s\n",
            "epoch 205| loss: 0.16793 | val_0_roc_auc: 0.94831 | val_0_accuracy: 0.88373 |  0:04:59s\n",
            "epoch 206| loss: 0.18574 | val_0_roc_auc: 0.9539  | val_0_accuracy: 0.8899  |  0:05:01s\n",
            "epoch 207| loss: 0.16078 | val_0_roc_auc: 0.9563  | val_0_accuracy: 0.90052 |  0:05:02s\n",
            "epoch 208| loss: 0.13942 | val_0_roc_auc: 0.96068 | val_0_accuracy: 0.90743 |  0:05:04s\n",
            "epoch 209| loss: 0.1247  | val_0_roc_auc: 0.96256 | val_0_accuracy: 0.9141  |  0:05:05s\n",
            "\n",
            "Early stopping occurred at epoch 209 with best_epoch = 159 and best_val_0_accuracy = 0.91558\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy: \" + str(round(mean(acc_score_tabnet),4)) + \" +- \"+ str(round(std(acc_score_tabnet),4)))\n",
        "print(\"ROC-AUC: \" + str(round(mean(auc_score_tabnet),4)) + \" +- \" + str(round(std(auc_score_tabnet),4)))\n",
        "print(\"F1-Score: \" + str(round(mean(f1_tabnet),4)) +\" +- \"+ str(round(std(f1_tabnet),4)))\n",
        "\n",
        "\n",
        "yhat = tabnet.predict(np.array(X_test)).round()\n",
        "print(classification_report(y_test, yhat))\n",
        "cm = confusion_matrix(yhat, y_test,labels=[0,1])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
        "disp.plot()\n",
        "RocCurveDisplay.from_predictions(y_test, yhat)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "id": "FH2J4aQ8XjBI",
        "outputId": "944f73ea-bccd-4a5b-856a-7ee4c1b39d80"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9222 +- 0.0078\n",
            "ROC-AUC: 0.9248 +- 0.0082\n",
            "F1-Score: 0.9367 +- 0.0063\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.84      0.88      2024\n",
            "           1       0.90      0.95      0.93      3041\n",
            "\n",
            "    accuracy                           0.91      5065\n",
            "   macro avg       0.91      0.90      0.90      5065\n",
            "weighted avg       0.91      0.91      0.91      5065\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# feature_importances\n",
        "feature_imp = pd.DataFrame(sorted(zip(tabnet.feature_importances_,X_train.columns)), columns=['Value','Feature'])\n",
        "plt.figure(figsize=(20, 10))\n",
        "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n",
        "plt.title('TabNet Features (avg over folds)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "p6rbzcNzXkRq",
        "outputId": "f22b4fb6-5e44-4280-8d27-02c3bea3658b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "free_gpu_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yiytY0tXlq-",
        "outputId": "b7adc901-2e8c-4c92-87c8-ecaf22cc6d2b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial GPU Usage\n",
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 |  0% |  6% |\n",
            "GPU Usage after emptying the cache\n",
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 |  2% |  1% |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meta_dnn_train = pd.concat([pd.DataFrame(meta_train_cnn1d, columns=['1DCNN']),\n",
        "                       pd.DataFrame(meta_train_tabnet, columns=['TabNet'])], axis = 1)"
      ],
      "metadata": {
        "id": "tD5gU4tSDomy"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta_dnn_test = pd.concat([pd.DataFrame(meta_test_cnn1d, columns=['1DCNN']),\n",
        "                       pd.DataFrame(meta_test_tabnet, columns=['TabNet'])], axis = 1)"
      ],
      "metadata": {
        "id": "hLa8sRp1Drjf"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta_dnn_train.to_pickle('./drive/MyDrive/save/taiwan/meta_dnn_train.pkl')\n",
        "meta_dnn_test.to_pickle('./drive/MyDrive/save/taiwan/meta_dnn_test.pkl')"
      ],
      "metadata": {
        "id": "LlmFH6ACDte6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################  LSTM ###################"
      ],
      "metadata": {
        "id": "LR2q7F5DX9Cm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define LSTM model\n",
        "def create_model_lstm(input_shape):\n",
        "    # Model Building\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=100,input_shape=(input_shape, 1),return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(100, return_sequences=False))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "X_train_lstm = np.array(X_train).reshape(-1, X_train.shape[1], 1)\n",
        "X_test_lstm = np.array(X_test).reshape(-1, X_test.shape[1], 1)\n",
        "# LSTM model\n",
        "lstm = create_model_lstm(input_shape = X_train_lstm.shape[1]) "
      ],
      "metadata": {
        "id": "i74aRRcJYB4e"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=n_fold, shuffle = False)\n",
        "acc_score_lstm = []\n",
        "auc_score_lstm = []\n",
        "f1_lstm = []\n",
        "meta_train_lstm = []\n",
        "meta_test_lstm = []\n",
        "i = 1\n",
        "for train_index, valid_index in kf.split(X_train_lstm, y_train):\n",
        "    print('KFold {} of {}'.format(i,kf.n_splits))\n",
        "    train_X, val_X = X_train_lstm[train_index], X_train_lstm[valid_index]\n",
        "    train_y, val_y = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
        "    lstm.fit(train_X, train_y, validation_data=(val_X, val_y), epochs = 500, \n",
        "            batch_size = 256, \n",
        "            callbacks = [EarlyStop(30), ModelCheckpointFull('./lstm.h5')],\n",
        "            verbose = 1)\n",
        "    ####meta\n",
        "    meta_train_lstm = np.append(meta_train_lstm, lstm.predict(val_X))\n",
        "    \n",
        "    if len(meta_test_lstm) == 0:\n",
        "        meta_test_lstm = lstm.predict(X_test_lstm)\n",
        "    else:\n",
        "        meta_test_lstm = np.add(meta_test_lstm, lstm.predict(X_test_lstm))\n",
        "    #####\n",
        "    yhat = lstm.predict(X_test_lstm).round()\n",
        "    acc_score_lstm.append(accuracy_score(yhat,y_test))\n",
        "    auc_score_lstm.append(roc_auc_score(yhat,y_test))\n",
        "    f1_lstm.append(f1_score(yhat,y_test))\n",
        "    i += 1\n",
        "meta_test_lstm = np.divide(meta_test_lstm, n_fold)"
      ],
      "metadata": {
        "id": "fHPG4VZlYEn5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d364d5d-3a44-40aa-f7dd-7f69325d9154"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KFold 1 of 5\n",
            "Epoch 1/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.6097 - accuracy: 0.6590\n",
            "Epoch 1: val_accuracy improved from -inf to 0.71051, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 15s 54ms/step - loss: 0.6097 - accuracy: 0.6590 - val_loss: 0.5521 - val_accuracy: 0.7105\n",
            "Epoch 2/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.5295 - accuracy: 0.7173\n",
            "Epoch 2: val_accuracy improved from 0.71051 to 0.71101, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.5298 - accuracy: 0.7169 - val_loss: 0.5212 - val_accuracy: 0.7110\n",
            "Epoch 3/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.5242 - accuracy: 0.7210\n",
            "Epoch 3: val_accuracy improved from 0.71101 to 0.73322, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.5242 - accuracy: 0.7210 - val_loss: 0.5071 - val_accuracy: 0.7332\n",
            "Epoch 4/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.5134 - accuracy: 0.7272\n",
            "Epoch 4: val_accuracy did not improve from 0.73322\n",
            "64/64 [==============================] - 2s 26ms/step - loss: 0.5134 - accuracy: 0.7272 - val_loss: 0.5091 - val_accuracy: 0.7127\n",
            "Epoch 5/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.5121 - accuracy: 0.7269\n",
            "Epoch 5: val_accuracy improved from 0.73322 to 0.73667, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 2s 27ms/step - loss: 0.5117 - accuracy: 0.7270 - val_loss: 0.4989 - val_accuracy: 0.7367\n",
            "Epoch 6/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.5075 - accuracy: 0.7276\n",
            "Epoch 6: val_accuracy improved from 0.73667 to 0.73791, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 21ms/step - loss: 0.5074 - accuracy: 0.7277 - val_loss: 0.4928 - val_accuracy: 0.7379\n",
            "Epoch 7/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.5073 - accuracy: 0.7263\n",
            "Epoch 7: val_accuracy improved from 0.73791 to 0.74161, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 20ms/step - loss: 0.5070 - accuracy: 0.7265 - val_loss: 0.4958 - val_accuracy: 0.7416\n",
            "Epoch 8/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.5024 - accuracy: 0.7287\n",
            "Epoch 8: val_accuracy did not improve from 0.74161\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.5027 - accuracy: 0.7280 - val_loss: 0.4894 - val_accuracy: 0.7406\n",
            "Epoch 9/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.5028 - accuracy: 0.7266\n",
            "Epoch 9: val_accuracy did not improve from 0.74161\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.5028 - accuracy: 0.7266 - val_loss: 0.4940 - val_accuracy: 0.7364\n",
            "Epoch 10/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4989 - accuracy: 0.7306\n",
            "Epoch 10: val_accuracy improved from 0.74161 to 0.74260, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.4990 - accuracy: 0.7306 - val_loss: 0.5049 - val_accuracy: 0.7426\n",
            "Epoch 11/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.5029 - accuracy: 0.7284\n",
            "Epoch 11: val_accuracy did not improve from 0.74260\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.5025 - accuracy: 0.7280 - val_loss: 0.5036 - val_accuracy: 0.7159\n",
            "Epoch 12/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4996 - accuracy: 0.7325\n",
            "Epoch 12: val_accuracy did not improve from 0.74260\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4996 - accuracy: 0.7329 - val_loss: 0.4910 - val_accuracy: 0.7404\n",
            "Epoch 13/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4997 - accuracy: 0.7330\n",
            "Epoch 13: val_accuracy did not improve from 0.74260\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4988 - accuracy: 0.7337 - val_loss: 0.4958 - val_accuracy: 0.7377\n",
            "Epoch 14/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4991 - accuracy: 0.7307\n",
            "Epoch 14: val_accuracy improved from 0.74260 to 0.74482, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.5003 - accuracy: 0.7299 - val_loss: 0.4874 - val_accuracy: 0.7448\n",
            "Epoch 15/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4949 - accuracy: 0.7342\n",
            "Epoch 15: val_accuracy did not improve from 0.74482\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.4935 - accuracy: 0.7352 - val_loss: 0.4863 - val_accuracy: 0.7409\n",
            "Epoch 16/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4923 - accuracy: 0.7352\n",
            "Epoch 16: val_accuracy improved from 0.74482 to 0.74654, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4915 - accuracy: 0.7362 - val_loss: 0.4816 - val_accuracy: 0.7465\n",
            "Epoch 17/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.4901 - accuracy: 0.7419\n",
            "Epoch 17: val_accuracy did not improve from 0.74654\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4906 - accuracy: 0.7418 - val_loss: 0.4863 - val_accuracy: 0.7340\n",
            "Epoch 18/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4915 - accuracy: 0.7366\n",
            "Epoch 18: val_accuracy improved from 0.74654 to 0.74729, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4914 - accuracy: 0.7375 - val_loss: 0.4829 - val_accuracy: 0.7473\n",
            "Epoch 19/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4913 - accuracy: 0.7371\n",
            "Epoch 19: val_accuracy did not improve from 0.74729\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4930 - accuracy: 0.7359 - val_loss: 0.4811 - val_accuracy: 0.7428\n",
            "Epoch 20/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4903 - accuracy: 0.7389\n",
            "Epoch 20: val_accuracy did not improve from 0.74729\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.4891 - accuracy: 0.7396 - val_loss: 0.4795 - val_accuracy: 0.7473\n",
            "Epoch 21/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4833 - accuracy: 0.7423\n",
            "Epoch 21: val_accuracy did not improve from 0.74729\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4829 - accuracy: 0.7424 - val_loss: 0.4815 - val_accuracy: 0.7409\n",
            "Epoch 22/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4853 - accuracy: 0.7365\n",
            "Epoch 22: val_accuracy did not improve from 0.74729\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4852 - accuracy: 0.7363 - val_loss: 0.4849 - val_accuracy: 0.7391\n",
            "Epoch 23/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4837 - accuracy: 0.7413\n",
            "Epoch 23: val_accuracy did not improve from 0.74729\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4838 - accuracy: 0.7413 - val_loss: 0.4814 - val_accuracy: 0.7384\n",
            "Epoch 24/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4859 - accuracy: 0.7423\n",
            "Epoch 24: val_accuracy did not improve from 0.74729\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4861 - accuracy: 0.7417 - val_loss: 0.4807 - val_accuracy: 0.7431\n",
            "Epoch 25/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4789 - accuracy: 0.7442\n",
            "Epoch 25: val_accuracy did not improve from 0.74729\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4783 - accuracy: 0.7447 - val_loss: 0.4782 - val_accuracy: 0.7441\n",
            "Epoch 26/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.4764 - accuracy: 0.7467\n",
            "Epoch 26: val_accuracy improved from 0.74729 to 0.75197, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4769 - accuracy: 0.7463 - val_loss: 0.4775 - val_accuracy: 0.7520\n",
            "Epoch 27/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4763 - accuracy: 0.7479\n",
            "Epoch 27: val_accuracy did not improve from 0.75197\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.4756 - accuracy: 0.7486 - val_loss: 0.4755 - val_accuracy: 0.7468\n",
            "Epoch 28/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4765 - accuracy: 0.7474\n",
            "Epoch 28: val_accuracy did not improve from 0.75197\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4757 - accuracy: 0.7475 - val_loss: 0.4737 - val_accuracy: 0.7478\n",
            "Epoch 29/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4731 - accuracy: 0.7452\n",
            "Epoch 29: val_accuracy did not improve from 0.75197\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4725 - accuracy: 0.7459 - val_loss: 0.4753 - val_accuracy: 0.7409\n",
            "Epoch 30/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4735 - accuracy: 0.7466\n",
            "Epoch 30: val_accuracy did not improve from 0.75197\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.4742 - accuracy: 0.7457 - val_loss: 0.4754 - val_accuracy: 0.7473\n",
            "Epoch 31/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4696 - accuracy: 0.7511\n",
            "Epoch 31: val_accuracy did not improve from 0.75197\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.4692 - accuracy: 0.7514 - val_loss: 0.4707 - val_accuracy: 0.7438\n",
            "Epoch 32/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4708 - accuracy: 0.7469\n",
            "Epoch 32: val_accuracy did not improve from 0.75197\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.4695 - accuracy: 0.7477 - val_loss: 0.4823 - val_accuracy: 0.7463\n",
            "Epoch 33/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4681 - accuracy: 0.7500\n",
            "Epoch 33: val_accuracy improved from 0.75197 to 0.75395, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4688 - accuracy: 0.7496 - val_loss: 0.4700 - val_accuracy: 0.7539\n",
            "Epoch 34/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4663 - accuracy: 0.7488\n",
            "Epoch 34: val_accuracy did not improve from 0.75395\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4656 - accuracy: 0.7498 - val_loss: 0.4640 - val_accuracy: 0.7515\n",
            "Epoch 35/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.4644 - accuracy: 0.7567\n",
            "Epoch 35: val_accuracy did not improve from 0.75395\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4652 - accuracy: 0.7559 - val_loss: 0.4656 - val_accuracy: 0.7507\n",
            "Epoch 36/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.4568 - accuracy: 0.7573\n",
            "Epoch 36: val_accuracy did not improve from 0.75395\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4579 - accuracy: 0.7566 - val_loss: 0.4799 - val_accuracy: 0.7515\n",
            "Epoch 37/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4604 - accuracy: 0.7546\n",
            "Epoch 37: val_accuracy improved from 0.75395 to 0.76012, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.4613 - accuracy: 0.7543 - val_loss: 0.4575 - val_accuracy: 0.7601\n",
            "Epoch 38/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4559 - accuracy: 0.7602\n",
            "Epoch 38: val_accuracy improved from 0.76012 to 0.76382, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4561 - accuracy: 0.7601 - val_loss: 0.4571 - val_accuracy: 0.7638\n",
            "Epoch 39/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4523 - accuracy: 0.7620\n",
            "Epoch 39: val_accuracy did not improve from 0.76382\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4520 - accuracy: 0.7626 - val_loss: 0.4564 - val_accuracy: 0.7567\n",
            "Epoch 40/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4496 - accuracy: 0.7649\n",
            "Epoch 40: val_accuracy did not improve from 0.76382\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4499 - accuracy: 0.7647 - val_loss: 0.4550 - val_accuracy: 0.7586\n",
            "Epoch 41/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4494 - accuracy: 0.7625\n",
            "Epoch 41: val_accuracy did not improve from 0.76382\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4497 - accuracy: 0.7623 - val_loss: 0.4526 - val_accuracy: 0.7601\n",
            "Epoch 42/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4390 - accuracy: 0.7736\n",
            "Epoch 42: val_accuracy did not improve from 0.76382\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4401 - accuracy: 0.7724 - val_loss: 0.4669 - val_accuracy: 0.7535\n",
            "Epoch 43/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4472 - accuracy: 0.7664\n",
            "Epoch 43: val_accuracy did not improve from 0.76382\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4473 - accuracy: 0.7674 - val_loss: 0.4577 - val_accuracy: 0.7559\n",
            "Epoch 44/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4428 - accuracy: 0.7686\n",
            "Epoch 44: val_accuracy did not improve from 0.76382\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4424 - accuracy: 0.7687 - val_loss: 0.4491 - val_accuracy: 0.7584\n",
            "Epoch 45/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4453 - accuracy: 0.7697\n",
            "Epoch 45: val_accuracy did not improve from 0.76382\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4454 - accuracy: 0.7688 - val_loss: 0.4460 - val_accuracy: 0.7621\n",
            "Epoch 46/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4450 - accuracy: 0.7707\n",
            "Epoch 46: val_accuracy improved from 0.76382 to 0.76703, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4436 - accuracy: 0.7713 - val_loss: 0.4499 - val_accuracy: 0.7670\n",
            "Epoch 47/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4299 - accuracy: 0.7776\n",
            "Epoch 47: val_accuracy did not improve from 0.76703\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4304 - accuracy: 0.7779 - val_loss: 0.4636 - val_accuracy: 0.7596\n",
            "Epoch 48/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4350 - accuracy: 0.7733\n",
            "Epoch 48: val_accuracy did not improve from 0.76703\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.4347 - accuracy: 0.7740 - val_loss: 0.4552 - val_accuracy: 0.7532\n",
            "Epoch 49/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4269 - accuracy: 0.7810\n",
            "Epoch 49: val_accuracy did not improve from 0.76703\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4273 - accuracy: 0.7802 - val_loss: 0.4430 - val_accuracy: 0.7668\n",
            "Epoch 50/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4262 - accuracy: 0.7787\n",
            "Epoch 50: val_accuracy improved from 0.76703 to 0.77122, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4270 - accuracy: 0.7785 - val_loss: 0.4320 - val_accuracy: 0.7712\n",
            "Epoch 51/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4238 - accuracy: 0.7848\n",
            "Epoch 51: val_accuracy did not improve from 0.77122\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4228 - accuracy: 0.7854 - val_loss: 0.4615 - val_accuracy: 0.7567\n",
            "Epoch 52/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4263 - accuracy: 0.7822\n",
            "Epoch 52: val_accuracy did not improve from 0.77122\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4264 - accuracy: 0.7814 - val_loss: 0.4409 - val_accuracy: 0.7710\n",
            "Epoch 53/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4253 - accuracy: 0.7845\n",
            "Epoch 53: val_accuracy improved from 0.77122 to 0.77591, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.4251 - accuracy: 0.7840 - val_loss: 0.4348 - val_accuracy: 0.7759\n",
            "Epoch 54/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4205 - accuracy: 0.7845\n",
            "Epoch 54: val_accuracy did not improve from 0.77591\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4202 - accuracy: 0.7844 - val_loss: 0.4530 - val_accuracy: 0.7559\n",
            "Epoch 55/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4200 - accuracy: 0.7878\n",
            "Epoch 55: val_accuracy did not improve from 0.77591\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4201 - accuracy: 0.7873 - val_loss: 0.4349 - val_accuracy: 0.7757\n",
            "Epoch 56/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4235 - accuracy: 0.7894\n",
            "Epoch 56: val_accuracy improved from 0.77591 to 0.77789, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4231 - accuracy: 0.7897 - val_loss: 0.4373 - val_accuracy: 0.7779\n",
            "Epoch 57/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4147 - accuracy: 0.7887\n",
            "Epoch 57: val_accuracy did not improve from 0.77789\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4158 - accuracy: 0.7884 - val_loss: 0.4433 - val_accuracy: 0.7697\n",
            "Epoch 58/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4066 - accuracy: 0.7951\n",
            "Epoch 58: val_accuracy did not improve from 0.77789\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4065 - accuracy: 0.7945 - val_loss: 0.4430 - val_accuracy: 0.7722\n",
            "Epoch 59/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4037 - accuracy: 0.7995\n",
            "Epoch 59: val_accuracy improved from 0.77789 to 0.77937, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.4035 - accuracy: 0.7993 - val_loss: 0.4339 - val_accuracy: 0.7794\n",
            "Epoch 60/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4013 - accuracy: 0.7995\n",
            "Epoch 60: val_accuracy did not improve from 0.77937\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4024 - accuracy: 0.7985 - val_loss: 0.4285 - val_accuracy: 0.7771\n",
            "Epoch 61/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.3987 - accuracy: 0.7991\n",
            "Epoch 61: val_accuracy improved from 0.77937 to 0.78233, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.3987 - accuracy: 0.7991 - val_loss: 0.4304 - val_accuracy: 0.7823\n",
            "Epoch 62/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4002 - accuracy: 0.8007\n",
            "Epoch 62: val_accuracy did not improve from 0.78233\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.4005 - accuracy: 0.8009 - val_loss: 0.4354 - val_accuracy: 0.7818\n",
            "Epoch 63/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.3923 - accuracy: 0.8017\n",
            "Epoch 63: val_accuracy did not improve from 0.78233\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.3930 - accuracy: 0.8014 - val_loss: 0.4255 - val_accuracy: 0.7769\n",
            "Epoch 64/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.3885 - accuracy: 0.8080\n",
            "Epoch 64: val_accuracy improved from 0.78233 to 0.78702, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.3883 - accuracy: 0.8076 - val_loss: 0.4228 - val_accuracy: 0.7870\n",
            "Epoch 65/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.3812 - accuracy: 0.8105\n",
            "Epoch 65: val_accuracy improved from 0.78702 to 0.78899, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.3810 - accuracy: 0.8107 - val_loss: 0.4203 - val_accuracy: 0.7890\n",
            "Epoch 66/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.3802 - accuracy: 0.8111\n",
            "Epoch 66: val_accuracy did not improve from 0.78899\n",
            "64/64 [==============================] - 1s 20ms/step - loss: 0.3802 - accuracy: 0.8112 - val_loss: 0.4265 - val_accuracy: 0.7836\n",
            "Epoch 67/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.3823 - accuracy: 0.8133\n",
            "Epoch 67: val_accuracy improved from 0.78899 to 0.79640, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 20ms/step - loss: 0.3823 - accuracy: 0.8133 - val_loss: 0.4154 - val_accuracy: 0.7964\n",
            "Epoch 68/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.3733 - accuracy: 0.8180\n",
            "Epoch 68: val_accuracy improved from 0.79640 to 0.79763, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.3729 - accuracy: 0.8187 - val_loss: 0.4104 - val_accuracy: 0.7976\n",
            "Epoch 69/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.3704 - accuracy: 0.8179\n",
            "Epoch 69: val_accuracy did not improve from 0.79763\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.3704 - accuracy: 0.8179 - val_loss: 0.4146 - val_accuracy: 0.7954\n",
            "Epoch 70/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.3695 - accuracy: 0.8168\n",
            "Epoch 70: val_accuracy did not improve from 0.79763\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.3708 - accuracy: 0.8169 - val_loss: 0.4136 - val_accuracy: 0.7922\n",
            "Epoch 71/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.3712 - accuracy: 0.8181\n",
            "Epoch 71: val_accuracy did not improve from 0.79763\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.3717 - accuracy: 0.8179 - val_loss: 0.4150 - val_accuracy: 0.7957\n",
            "Epoch 72/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.3657 - accuracy: 0.8208\n",
            "Epoch 72: val_accuracy improved from 0.79763 to 0.80355, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.3661 - accuracy: 0.8203 - val_loss: 0.4046 - val_accuracy: 0.8036\n",
            "Epoch 73/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.3568 - accuracy: 0.8268\n",
            "Epoch 73: val_accuracy did not improve from 0.80355\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.3579 - accuracy: 0.8269 - val_loss: 0.4167 - val_accuracy: 0.7942\n",
            "Epoch 74/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.3568 - accuracy: 0.8274\n",
            "Epoch 74: val_accuracy improved from 0.80355 to 0.80701, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.3574 - accuracy: 0.8267 - val_loss: 0.4004 - val_accuracy: 0.8070\n",
            "Epoch 75/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.3523 - accuracy: 0.8265\n",
            "Epoch 75: val_accuracy did not improve from 0.80701\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.3524 - accuracy: 0.8267 - val_loss: 0.3976 - val_accuracy: 0.8011\n",
            "Epoch 76/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.3505 - accuracy: 0.8276\n",
            "Epoch 76: val_accuracy did not improve from 0.80701\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.3502 - accuracy: 0.8282 - val_loss: 0.4062 - val_accuracy: 0.8028\n",
            "Epoch 77/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.3529 - accuracy: 0.8309\n",
            "Epoch 77: val_accuracy did not improve from 0.80701\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.3541 - accuracy: 0.8303 - val_loss: 0.4136 - val_accuracy: 0.7952\n",
            "Epoch 78/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.3490 - accuracy: 0.8322\n",
            "Epoch 78: val_accuracy did not improve from 0.80701\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.3487 - accuracy: 0.8325 - val_loss: 0.4143 - val_accuracy: 0.8031\n",
            "Epoch 79/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.3477 - accuracy: 0.8333\n",
            "Epoch 79: val_accuracy did not improve from 0.80701\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.3477 - accuracy: 0.8333 - val_loss: 0.3997 - val_accuracy: 0.8011\n",
            "Epoch 80/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.3357 - accuracy: 0.8388\n",
            "Epoch 80: val_accuracy did not improve from 0.80701\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.3373 - accuracy: 0.8379 - val_loss: 0.4054 - val_accuracy: 0.8048\n",
            "Epoch 81/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.3433 - accuracy: 0.8333\n",
            "Epoch 81: val_accuracy did not improve from 0.80701\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.3439 - accuracy: 0.8333 - val_loss: 0.4022 - val_accuracy: 0.8055\n",
            "Epoch 82/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.3457 - accuracy: 0.8313\n",
            "Epoch 82: val_accuracy did not improve from 0.80701\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.3476 - accuracy: 0.8303 - val_loss: 0.3974 - val_accuracy: 0.8018\n",
            "Epoch 83/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.3252 - accuracy: 0.8452\n",
            "Epoch 83: val_accuracy improved from 0.80701 to 0.81022, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.3256 - accuracy: 0.8451 - val_loss: 0.3948 - val_accuracy: 0.8102\n",
            "Epoch 84/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.3234 - accuracy: 0.8461\n",
            "Epoch 84: val_accuracy improved from 0.81022 to 0.81491, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.3229 - accuracy: 0.8462 - val_loss: 0.3854 - val_accuracy: 0.8149\n",
            "Epoch 85/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.3141 - accuracy: 0.8514\n",
            "Epoch 85: val_accuracy did not improve from 0.81491\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.3145 - accuracy: 0.8511 - val_loss: 0.4055 - val_accuracy: 0.8033\n",
            "Epoch 86/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.3203 - accuracy: 0.8463\n",
            "Epoch 86: val_accuracy did not improve from 0.81491\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.3207 - accuracy: 0.8457 - val_loss: 0.3831 - val_accuracy: 0.8137\n",
            "Epoch 87/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.3159 - accuracy: 0.8489\n",
            "Epoch 87: val_accuracy improved from 0.81491 to 0.81565, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.3157 - accuracy: 0.8491 - val_loss: 0.3949 - val_accuracy: 0.8156\n",
            "Epoch 88/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.3162 - accuracy: 0.8494\n",
            "Epoch 88: val_accuracy improved from 0.81565 to 0.82157, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 23ms/step - loss: 0.3162 - accuracy: 0.8494 - val_loss: 0.3814 - val_accuracy: 0.8216\n",
            "Epoch 89/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.3188 - accuracy: 0.8484\n",
            "Epoch 89: val_accuracy did not improve from 0.82157\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.3184 - accuracy: 0.8488 - val_loss: 0.3937 - val_accuracy: 0.8154\n",
            "Epoch 90/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.3068 - accuracy: 0.8553\n",
            "Epoch 90: val_accuracy did not improve from 0.82157\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.3082 - accuracy: 0.8546 - val_loss: 0.3950 - val_accuracy: 0.8149\n",
            "Epoch 91/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.3161 - accuracy: 0.8514\n",
            "Epoch 91: val_accuracy did not improve from 0.82157\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.3154 - accuracy: 0.8512 - val_loss: 0.3947 - val_accuracy: 0.8127\n",
            "Epoch 92/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.3099 - accuracy: 0.8539\n",
            "Epoch 92: val_accuracy improved from 0.82157 to 0.82577, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.3110 - accuracy: 0.8530 - val_loss: 0.3756 - val_accuracy: 0.8258\n",
            "Epoch 93/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.3125 - accuracy: 0.8541\n",
            "Epoch 93: val_accuracy did not improve from 0.82577\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.3132 - accuracy: 0.8538 - val_loss: 0.3904 - val_accuracy: 0.8198\n",
            "Epoch 94/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.3100 - accuracy: 0.8553\n",
            "Epoch 94: val_accuracy did not improve from 0.82577\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.3098 - accuracy: 0.8556 - val_loss: 0.4005 - val_accuracy: 0.8092\n",
            "Epoch 95/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2969 - accuracy: 0.8602\n",
            "Epoch 95: val_accuracy did not improve from 0.82577\n",
            "64/64 [==============================] - 1s 15ms/step - loss: 0.2965 - accuracy: 0.8609 - val_loss: 0.3913 - val_accuracy: 0.8213\n",
            "Epoch 96/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2926 - accuracy: 0.8640\n",
            "Epoch 96: val_accuracy did not improve from 0.82577\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2938 - accuracy: 0.8636 - val_loss: 0.3978 - val_accuracy: 0.8201\n",
            "Epoch 97/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2905 - accuracy: 0.8623\n",
            "Epoch 97: val_accuracy did not improve from 0.82577\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2924 - accuracy: 0.8615 - val_loss: 0.3887 - val_accuracy: 0.8193\n",
            "Epoch 98/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2851 - accuracy: 0.8683\n",
            "Epoch 98: val_accuracy improved from 0.82577 to 0.83193, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2854 - accuracy: 0.8685 - val_loss: 0.3977 - val_accuracy: 0.8319\n",
            "Epoch 99/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2886 - accuracy: 0.8640\n",
            "Epoch 99: val_accuracy did not improve from 0.83193\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.2889 - accuracy: 0.8638 - val_loss: 0.3785 - val_accuracy: 0.8263\n",
            "Epoch 100/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2941 - accuracy: 0.8593\n",
            "Epoch 100: val_accuracy did not improve from 0.83193\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2937 - accuracy: 0.8593 - val_loss: 0.3981 - val_accuracy: 0.8142\n",
            "Epoch 101/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.2882 - accuracy: 0.8670\n",
            "Epoch 101: val_accuracy did not improve from 0.83193\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2882 - accuracy: 0.8670 - val_loss: 0.3915 - val_accuracy: 0.8240\n",
            "Epoch 102/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2834 - accuracy: 0.8714\n",
            "Epoch 102: val_accuracy did not improve from 0.83193\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2825 - accuracy: 0.8723 - val_loss: 0.3868 - val_accuracy: 0.8272\n",
            "Epoch 103/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2804 - accuracy: 0.8710\n",
            "Epoch 103: val_accuracy did not improve from 0.83193\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2806 - accuracy: 0.8702 - val_loss: 0.3829 - val_accuracy: 0.8277\n",
            "Epoch 104/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2897 - accuracy: 0.8655\n",
            "Epoch 104: val_accuracy did not improve from 0.83193\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2915 - accuracy: 0.8642 - val_loss: 0.4014 - val_accuracy: 0.8174\n",
            "Epoch 105/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2809 - accuracy: 0.8697\n",
            "Epoch 105: val_accuracy improved from 0.83193 to 0.83317, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2798 - accuracy: 0.8705 - val_loss: 0.3727 - val_accuracy: 0.8332\n",
            "Epoch 106/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2725 - accuracy: 0.8769\n",
            "Epoch 106: val_accuracy did not improve from 0.83317\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2717 - accuracy: 0.8768 - val_loss: 0.3791 - val_accuracy: 0.8322\n",
            "Epoch 107/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2641 - accuracy: 0.8792\n",
            "Epoch 107: val_accuracy did not improve from 0.83317\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2636 - accuracy: 0.8789 - val_loss: 0.3756 - val_accuracy: 0.8292\n",
            "Epoch 108/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.2552 - accuracy: 0.8840\n",
            "Epoch 108: val_accuracy improved from 0.83317 to 0.83736, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2552 - accuracy: 0.8840 - val_loss: 0.3887 - val_accuracy: 0.8374\n",
            "Epoch 109/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2647 - accuracy: 0.8790\n",
            "Epoch 109: val_accuracy did not improve from 0.83736\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2653 - accuracy: 0.8786 - val_loss: 0.3811 - val_accuracy: 0.8277\n",
            "Epoch 110/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2826 - accuracy: 0.8655\n",
            "Epoch 110: val_accuracy did not improve from 0.83736\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.2810 - accuracy: 0.8667 - val_loss: 0.3844 - val_accuracy: 0.8270\n",
            "Epoch 111/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2603 - accuracy: 0.8797\n",
            "Epoch 111: val_accuracy did not improve from 0.83736\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.2609 - accuracy: 0.8793 - val_loss: 0.3798 - val_accuracy: 0.8369\n",
            "Epoch 112/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.2494 - accuracy: 0.8884\n",
            "Epoch 112: val_accuracy improved from 0.83736 to 0.84008, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2494 - accuracy: 0.8884 - val_loss: 0.3803 - val_accuracy: 0.8401\n",
            "Epoch 113/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.2715 - accuracy: 0.8760\n",
            "Epoch 113: val_accuracy did not improve from 0.84008\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2715 - accuracy: 0.8760 - val_loss: 0.3926 - val_accuracy: 0.8265\n",
            "Epoch 114/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2697 - accuracy: 0.8781\n",
            "Epoch 114: val_accuracy did not improve from 0.84008\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.2694 - accuracy: 0.8786 - val_loss: 0.3740 - val_accuracy: 0.8359\n",
            "Epoch 115/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2424 - accuracy: 0.8909\n",
            "Epoch 115: val_accuracy improved from 0.84008 to 0.84501, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2432 - accuracy: 0.8905 - val_loss: 0.3704 - val_accuracy: 0.8450\n",
            "Epoch 116/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2515 - accuracy: 0.8877\n",
            "Epoch 116: val_accuracy did not improve from 0.84501\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2509 - accuracy: 0.8880 - val_loss: 0.3804 - val_accuracy: 0.8406\n",
            "Epoch 117/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2409 - accuracy: 0.8934\n",
            "Epoch 117: val_accuracy did not improve from 0.84501\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2419 - accuracy: 0.8932 - val_loss: 0.3674 - val_accuracy: 0.8411\n",
            "Epoch 118/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.2441 - accuracy: 0.8919\n",
            "Epoch 118: val_accuracy improved from 0.84501 to 0.84699, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2441 - accuracy: 0.8919 - val_loss: 0.3812 - val_accuracy: 0.8470\n",
            "Epoch 119/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2382 - accuracy: 0.8947\n",
            "Epoch 119: val_accuracy did not improve from 0.84699\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.2391 - accuracy: 0.8944 - val_loss: 0.3760 - val_accuracy: 0.8430\n",
            "Epoch 120/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2305 - accuracy: 0.8998\n",
            "Epoch 120: val_accuracy improved from 0.84699 to 0.84798, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.2305 - accuracy: 0.8992 - val_loss: 0.3647 - val_accuracy: 0.8480\n",
            "Epoch 121/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2429 - accuracy: 0.8916\n",
            "Epoch 121: val_accuracy did not improve from 0.84798\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2427 - accuracy: 0.8920 - val_loss: 0.3687 - val_accuracy: 0.8462\n",
            "Epoch 122/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.2578 - accuracy: 0.8863\n",
            "Epoch 122: val_accuracy did not improve from 0.84798\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2578 - accuracy: 0.8863 - val_loss: 0.3896 - val_accuracy: 0.8406\n",
            "Epoch 123/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.2485 - accuracy: 0.8881\n",
            "Epoch 123: val_accuracy did not improve from 0.84798\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2485 - accuracy: 0.8881 - val_loss: 0.3708 - val_accuracy: 0.8455\n",
            "Epoch 124/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2533 - accuracy: 0.8848\n",
            "Epoch 124: val_accuracy did not improve from 0.84798\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2529 - accuracy: 0.8850 - val_loss: 0.3660 - val_accuracy: 0.8428\n",
            "Epoch 125/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2240 - accuracy: 0.9032\n",
            "Epoch 125: val_accuracy did not improve from 0.84798\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2236 - accuracy: 0.9035 - val_loss: 0.3913 - val_accuracy: 0.8453\n",
            "Epoch 126/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.2239 - accuracy: 0.9015\n",
            "Epoch 126: val_accuracy did not improve from 0.84798\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2239 - accuracy: 0.9015 - val_loss: 0.3618 - val_accuracy: 0.8455\n",
            "Epoch 127/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.2318 - accuracy: 0.8980\n",
            "Epoch 127: val_accuracy did not improve from 0.84798\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.2318 - accuracy: 0.8979 - val_loss: 0.3755 - val_accuracy: 0.8450\n",
            "Epoch 128/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.2309 - accuracy: 0.8974\n",
            "Epoch 128: val_accuracy did not improve from 0.84798\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.2309 - accuracy: 0.8974 - val_loss: 0.3762 - val_accuracy: 0.8470\n",
            "Epoch 129/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.2296 - accuracy: 0.8990\n",
            "Epoch 129: val_accuracy improved from 0.84798 to 0.85143, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.2296 - accuracy: 0.8990 - val_loss: 0.3776 - val_accuracy: 0.8514\n",
            "Epoch 130/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.2138 - accuracy: 0.9033\n",
            "Epoch 130: val_accuracy did not improve from 0.85143\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.2139 - accuracy: 0.9035 - val_loss: 0.3719 - val_accuracy: 0.8433\n",
            "Epoch 131/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2265 - accuracy: 0.8994\n",
            "Epoch 131: val_accuracy did not improve from 0.85143\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2278 - accuracy: 0.8985 - val_loss: 0.4020 - val_accuracy: 0.8369\n",
            "Epoch 132/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2249 - accuracy: 0.8997\n",
            "Epoch 132: val_accuracy did not improve from 0.85143\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2246 - accuracy: 0.8999 - val_loss: 0.3656 - val_accuracy: 0.8480\n",
            "Epoch 133/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2110 - accuracy: 0.9076\n",
            "Epoch 133: val_accuracy did not improve from 0.85143\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2111 - accuracy: 0.9076 - val_loss: 0.4044 - val_accuracy: 0.8396\n",
            "Epoch 134/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2272 - accuracy: 0.9023\n",
            "Epoch 134: val_accuracy did not improve from 0.85143\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.2275 - accuracy: 0.9020 - val_loss: 0.3707 - val_accuracy: 0.8477\n",
            "Epoch 135/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2477 - accuracy: 0.8897\n",
            "Epoch 135: val_accuracy improved from 0.85143 to 0.85390, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2469 - accuracy: 0.8900 - val_loss: 0.3535 - val_accuracy: 0.8539\n",
            "Epoch 136/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.2046 - accuracy: 0.9107\n",
            "Epoch 136: val_accuracy did not improve from 0.85390\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.2046 - accuracy: 0.9107 - val_loss: 0.3941 - val_accuracy: 0.8482\n",
            "Epoch 137/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2081 - accuracy: 0.9101\n",
            "Epoch 137: val_accuracy did not improve from 0.85390\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.2072 - accuracy: 0.9108 - val_loss: 0.3872 - val_accuracy: 0.8440\n",
            "Epoch 138/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2213 - accuracy: 0.9032\n",
            "Epoch 138: val_accuracy improved from 0.85390 to 0.86056, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2208 - accuracy: 0.9033 - val_loss: 0.3595 - val_accuracy: 0.8606\n",
            "Epoch 139/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2017 - accuracy: 0.9130\n",
            "Epoch 139: val_accuracy did not improve from 0.86056\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2032 - accuracy: 0.9128 - val_loss: 0.3975 - val_accuracy: 0.8425\n",
            "Epoch 140/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.2092 - accuracy: 0.9086\n",
            "Epoch 140: val_accuracy improved from 0.86056 to 0.86180, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2092 - accuracy: 0.9086 - val_loss: 0.3567 - val_accuracy: 0.8618\n",
            "Epoch 141/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1941 - accuracy: 0.9154\n",
            "Epoch 141: val_accuracy improved from 0.86180 to 0.86254, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1940 - accuracy: 0.9155 - val_loss: 0.3667 - val_accuracy: 0.8625\n",
            "Epoch 142/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1917 - accuracy: 0.9184\n",
            "Epoch 142: val_accuracy did not improve from 0.86254\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1917 - accuracy: 0.9184 - val_loss: 0.3871 - val_accuracy: 0.8541\n",
            "Epoch 143/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1884 - accuracy: 0.9182\n",
            "Epoch 143: val_accuracy did not improve from 0.86254\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1884 - accuracy: 0.9182 - val_loss: 0.4019 - val_accuracy: 0.8497\n",
            "Epoch 144/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1962 - accuracy: 0.9152\n",
            "Epoch 144: val_accuracy did not improve from 0.86254\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1962 - accuracy: 0.9152 - val_loss: 0.3702 - val_accuracy: 0.8539\n",
            "Epoch 145/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1848 - accuracy: 0.9197\n",
            "Epoch 145: val_accuracy improved from 0.86254 to 0.86278, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1850 - accuracy: 0.9197 - val_loss: 0.3617 - val_accuracy: 0.8628\n",
            "Epoch 146/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1934 - accuracy: 0.9167\n",
            "Epoch 146: val_accuracy did not improve from 0.86278\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1942 - accuracy: 0.9161 - val_loss: 0.4188 - val_accuracy: 0.8485\n",
            "Epoch 147/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2289 - accuracy: 0.9002\n",
            "Epoch 147: val_accuracy did not improve from 0.86278\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2280 - accuracy: 0.9004 - val_loss: 0.3776 - val_accuracy: 0.8564\n",
            "Epoch 148/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1952 - accuracy: 0.9176\n",
            "Epoch 148: val_accuracy did not improve from 0.86278\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1952 - accuracy: 0.9178 - val_loss: 0.3734 - val_accuracy: 0.8559\n",
            "Epoch 149/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1765 - accuracy: 0.9257\n",
            "Epoch 149: val_accuracy did not improve from 0.86278\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1768 - accuracy: 0.9255 - val_loss: 0.3828 - val_accuracy: 0.8524\n",
            "Epoch 150/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2026 - accuracy: 0.9133\n",
            "Epoch 150: val_accuracy did not improve from 0.86278\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2022 - accuracy: 0.9133 - val_loss: 0.3874 - val_accuracy: 0.8482\n",
            "Epoch 151/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.2048 - accuracy: 0.9101\n",
            "Epoch 151: val_accuracy did not improve from 0.86278\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.2044 - accuracy: 0.9103 - val_loss: 0.3642 - val_accuracy: 0.8608\n",
            "Epoch 152/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1834 - accuracy: 0.9223\n",
            "Epoch 152: val_accuracy did not improve from 0.86278\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1862 - accuracy: 0.9207 - val_loss: 0.3938 - val_accuracy: 0.8448\n",
            "Epoch 153/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1941 - accuracy: 0.9153\n",
            "Epoch 153: val_accuracy did not improve from 0.86278\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1932 - accuracy: 0.9154 - val_loss: 0.3701 - val_accuracy: 0.8593\n",
            "Epoch 154/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1861 - accuracy: 0.9192\n",
            "Epoch 154: val_accuracy improved from 0.86278 to 0.86402, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1844 - accuracy: 0.9198 - val_loss: 0.3644 - val_accuracy: 0.8640\n",
            "Epoch 155/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1842 - accuracy: 0.9210\n",
            "Epoch 155: val_accuracy improved from 0.86402 to 0.86451, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1837 - accuracy: 0.9210 - val_loss: 0.3624 - val_accuracy: 0.8645\n",
            "Epoch 156/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1697 - accuracy: 0.9279\n",
            "Epoch 156: val_accuracy did not improve from 0.86451\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1697 - accuracy: 0.9279 - val_loss: 0.3739 - val_accuracy: 0.8603\n",
            "Epoch 157/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1788 - accuracy: 0.9274\n",
            "Epoch 157: val_accuracy did not improve from 0.86451\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1802 - accuracy: 0.9267 - val_loss: 0.3708 - val_accuracy: 0.8574\n",
            "Epoch 158/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1823 - accuracy: 0.9241\n",
            "Epoch 158: val_accuracy improved from 0.86451 to 0.86895, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1828 - accuracy: 0.9239 - val_loss: 0.3623 - val_accuracy: 0.8690\n",
            "Epoch 159/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1687 - accuracy: 0.9289\n",
            "Epoch 159: val_accuracy did not improve from 0.86895\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1687 - accuracy: 0.9289 - val_loss: 0.3697 - val_accuracy: 0.8657\n",
            "Epoch 160/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1764 - accuracy: 0.9271\n",
            "Epoch 160: val_accuracy did not improve from 0.86895\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1767 - accuracy: 0.9268 - val_loss: 0.3772 - val_accuracy: 0.8601\n",
            "Epoch 161/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.2005 - accuracy: 0.9160\n",
            "Epoch 161: val_accuracy did not improve from 0.86895\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2005 - accuracy: 0.9161 - val_loss: 0.3492 - val_accuracy: 0.8625\n",
            "Epoch 162/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1696 - accuracy: 0.9269\n",
            "Epoch 162: val_accuracy did not improve from 0.86895\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1696 - accuracy: 0.9269 - val_loss: 0.3716 - val_accuracy: 0.8672\n",
            "Epoch 163/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1868 - accuracy: 0.9213\n",
            "Epoch 163: val_accuracy did not improve from 0.86895\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1871 - accuracy: 0.9215 - val_loss: 0.3745 - val_accuracy: 0.8682\n",
            "Epoch 164/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1780 - accuracy: 0.9271\n",
            "Epoch 164: val_accuracy did not improve from 0.86895\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1778 - accuracy: 0.9273 - val_loss: 0.3766 - val_accuracy: 0.8591\n",
            "Epoch 165/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1838 - accuracy: 0.9224\n",
            "Epoch 165: val_accuracy did not improve from 0.86895\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1838 - accuracy: 0.9224 - val_loss: 0.3745 - val_accuracy: 0.8640\n",
            "Epoch 166/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1647 - accuracy: 0.9324\n",
            "Epoch 166: val_accuracy did not improve from 0.86895\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1654 - accuracy: 0.9320 - val_loss: 0.3899 - val_accuracy: 0.8635\n",
            "Epoch 167/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1683 - accuracy: 0.9289\n",
            "Epoch 167: val_accuracy did not improve from 0.86895\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1683 - accuracy: 0.9289 - val_loss: 0.3706 - val_accuracy: 0.8645\n",
            "Epoch 168/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1680 - accuracy: 0.9292\n",
            "Epoch 168: val_accuracy did not improve from 0.86895\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1680 - accuracy: 0.9292 - val_loss: 0.3694 - val_accuracy: 0.8620\n",
            "Epoch 169/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1663 - accuracy: 0.9314\n",
            "Epoch 169: val_accuracy did not improve from 0.86895\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1665 - accuracy: 0.9318 - val_loss: 0.3870 - val_accuracy: 0.8655\n",
            "Epoch 170/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1544 - accuracy: 0.9330\n",
            "Epoch 170: val_accuracy did not improve from 0.86895\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1552 - accuracy: 0.9328 - val_loss: 0.3646 - val_accuracy: 0.8685\n",
            "Epoch 171/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1594 - accuracy: 0.9331\n",
            "Epoch 171: val_accuracy did not improve from 0.86895\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1595 - accuracy: 0.9332 - val_loss: 0.3879 - val_accuracy: 0.8655\n",
            "Epoch 172/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1714 - accuracy: 0.9272\n",
            "Epoch 172: val_accuracy improved from 0.86895 to 0.87043, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1721 - accuracy: 0.9269 - val_loss: 0.3660 - val_accuracy: 0.8704\n",
            "Epoch 173/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1546 - accuracy: 0.9361\n",
            "Epoch 173: val_accuracy did not improve from 0.87043\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1546 - accuracy: 0.9361 - val_loss: 0.3896 - val_accuracy: 0.8620\n",
            "Epoch 174/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1634 - accuracy: 0.9319\n",
            "Epoch 174: val_accuracy did not improve from 0.87043\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1633 - accuracy: 0.9318 - val_loss: 0.4112 - val_accuracy: 0.8502\n",
            "Epoch 175/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1885 - accuracy: 0.9229\n",
            "Epoch 175: val_accuracy did not improve from 0.87043\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.1885 - accuracy: 0.9229 - val_loss: 0.3746 - val_accuracy: 0.8653\n",
            "Epoch 176/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1734 - accuracy: 0.9263\n",
            "Epoch 176: val_accuracy did not improve from 0.87043\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1734 - accuracy: 0.9263 - val_loss: 0.3576 - val_accuracy: 0.8692\n",
            "Epoch 177/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1529 - accuracy: 0.9352\n",
            "Epoch 177: val_accuracy did not improve from 0.87043\n",
            "64/64 [==============================] - 1s 21ms/step - loss: 0.1529 - accuracy: 0.9352 - val_loss: 0.3775 - val_accuracy: 0.8662\n",
            "Epoch 178/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1587 - accuracy: 0.9321\n",
            "Epoch 178: val_accuracy did not improve from 0.87043\n",
            "64/64 [==============================] - 2s 25ms/step - loss: 0.1592 - accuracy: 0.9320 - val_loss: 0.3763 - val_accuracy: 0.8680\n",
            "Epoch 179/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.1618 - accuracy: 0.9316\n",
            "Epoch 179: val_accuracy did not improve from 0.87043\n",
            "64/64 [==============================] - 1s 22ms/step - loss: 0.1624 - accuracy: 0.9314 - val_loss: 0.3924 - val_accuracy: 0.8628\n",
            "Epoch 180/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.1571 - accuracy: 0.9363\n",
            "Epoch 180: val_accuracy did not improve from 0.87043\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.1568 - accuracy: 0.9366 - val_loss: 0.3698 - val_accuracy: 0.8682\n",
            "Epoch 181/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1378 - accuracy: 0.9438\n",
            "Epoch 181: val_accuracy improved from 0.87043 to 0.87241, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1394 - accuracy: 0.9427 - val_loss: 0.3779 - val_accuracy: 0.8724\n",
            "Epoch 182/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1394 - accuracy: 0.9422\n",
            "Epoch 182: val_accuracy did not improve from 0.87241\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1403 - accuracy: 0.9419 - val_loss: 0.3857 - val_accuracy: 0.8625\n",
            "Epoch 183/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1584 - accuracy: 0.9365\n",
            "Epoch 183: val_accuracy did not improve from 0.87241\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1588 - accuracy: 0.9363 - val_loss: 0.3835 - val_accuracy: 0.8707\n",
            "Epoch 184/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1515 - accuracy: 0.9383\n",
            "Epoch 184: val_accuracy improved from 0.87241 to 0.87438, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1515 - accuracy: 0.9383 - val_loss: 0.3767 - val_accuracy: 0.8744\n",
            "Epoch 185/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1503 - accuracy: 0.9376\n",
            "Epoch 185: val_accuracy did not improve from 0.87438\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1510 - accuracy: 0.9373 - val_loss: 0.3815 - val_accuracy: 0.8675\n",
            "Epoch 186/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1968 - accuracy: 0.9160\n",
            "Epoch 186: val_accuracy did not improve from 0.87438\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1969 - accuracy: 0.9160 - val_loss: 0.3576 - val_accuracy: 0.8615\n",
            "Epoch 187/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1683 - accuracy: 0.9298\n",
            "Epoch 187: val_accuracy did not improve from 0.87438\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1678 - accuracy: 0.9297 - val_loss: 0.3554 - val_accuracy: 0.8719\n",
            "Epoch 188/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1501 - accuracy: 0.9371\n",
            "Epoch 188: val_accuracy improved from 0.87438 to 0.87636, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1501 - accuracy: 0.9371 - val_loss: 0.3659 - val_accuracy: 0.8764\n",
            "Epoch 189/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1315 - accuracy: 0.9451\n",
            "Epoch 189: val_accuracy did not improve from 0.87636\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1317 - accuracy: 0.9454 - val_loss: 0.3868 - val_accuracy: 0.8727\n",
            "Epoch 190/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1291 - accuracy: 0.9458\n",
            "Epoch 190: val_accuracy did not improve from 0.87636\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1291 - accuracy: 0.9458 - val_loss: 0.3879 - val_accuracy: 0.8704\n",
            "Epoch 191/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1348 - accuracy: 0.9434\n",
            "Epoch 191: val_accuracy did not improve from 0.87636\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1348 - accuracy: 0.9434 - val_loss: 0.3895 - val_accuracy: 0.8623\n",
            "Epoch 192/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1391 - accuracy: 0.9426\n",
            "Epoch 192: val_accuracy did not improve from 0.87636\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1391 - accuracy: 0.9426 - val_loss: 0.3978 - val_accuracy: 0.8719\n",
            "Epoch 193/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1435 - accuracy: 0.9379\n",
            "Epoch 193: val_accuracy did not improve from 0.87636\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1435 - accuracy: 0.9379 - val_loss: 0.3710 - val_accuracy: 0.8734\n",
            "Epoch 194/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1375 - accuracy: 0.9426\n",
            "Epoch 194: val_accuracy did not improve from 0.87636\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1375 - accuracy: 0.9426 - val_loss: 0.3983 - val_accuracy: 0.8623\n",
            "Epoch 195/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1434 - accuracy: 0.9407\n",
            "Epoch 195: val_accuracy did not improve from 0.87636\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1434 - accuracy: 0.9407 - val_loss: 0.3716 - val_accuracy: 0.8729\n",
            "Epoch 196/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1363 - accuracy: 0.9418\n",
            "Epoch 196: val_accuracy did not improve from 0.87636\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1363 - accuracy: 0.9418 - val_loss: 0.3774 - val_accuracy: 0.8759\n",
            "Epoch 197/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1323 - accuracy: 0.9450\n",
            "Epoch 197: val_accuracy did not improve from 0.87636\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.1324 - accuracy: 0.9450 - val_loss: 0.4139 - val_accuracy: 0.8640\n",
            "Epoch 198/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1834 - accuracy: 0.9258\n",
            "Epoch 198: val_accuracy did not improve from 0.87636\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1834 - accuracy: 0.9258 - val_loss: 0.3692 - val_accuracy: 0.8719\n",
            "Epoch 199/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.1480 - accuracy: 0.9389\n",
            "Epoch 199: val_accuracy did not improve from 0.87636\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1476 - accuracy: 0.9393 - val_loss: 0.3789 - val_accuracy: 0.8714\n",
            "Epoch 200/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.1505 - accuracy: 0.9389\n",
            "Epoch 200: val_accuracy did not improve from 0.87636\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1500 - accuracy: 0.9392 - val_loss: 0.3604 - val_accuracy: 0.8756\n",
            "Epoch 201/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1265 - accuracy: 0.9479\n",
            "Epoch 201: val_accuracy did not improve from 0.87636\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.1267 - accuracy: 0.9478 - val_loss: 0.3823 - val_accuracy: 0.8736\n",
            "Epoch 202/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1385 - accuracy: 0.9420\n",
            "Epoch 202: val_accuracy did not improve from 0.87636\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1396 - accuracy: 0.9414 - val_loss: 0.4022 - val_accuracy: 0.8638\n",
            "Epoch 203/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.1625 - accuracy: 0.9332\n",
            "Epoch 203: val_accuracy did not improve from 0.87636\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1621 - accuracy: 0.9333 - val_loss: 0.3626 - val_accuracy: 0.8744\n",
            "Epoch 204/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1552 - accuracy: 0.9358\n",
            "Epoch 204: val_accuracy did not improve from 0.87636\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1552 - accuracy: 0.9358 - val_loss: 0.3595 - val_accuracy: 0.8751\n",
            "Epoch 205/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1360 - accuracy: 0.9447\n",
            "Epoch 205: val_accuracy did not improve from 0.87636\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1360 - accuracy: 0.9447 - val_loss: 0.3809 - val_accuracy: 0.8709\n",
            "Epoch 206/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1292 - accuracy: 0.9470\n",
            "Epoch 206: val_accuracy improved from 0.87636 to 0.87981, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1294 - accuracy: 0.9470 - val_loss: 0.3771 - val_accuracy: 0.8798\n",
            "Epoch 207/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1200 - accuracy: 0.9524\n",
            "Epoch 207: val_accuracy improved from 0.87981 to 0.88549, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1200 - accuracy: 0.9524 - val_loss: 0.3687 - val_accuracy: 0.8855\n",
            "Epoch 208/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1241 - accuracy: 0.9483\n",
            "Epoch 208: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1248 - accuracy: 0.9482 - val_loss: 0.3775 - val_accuracy: 0.8808\n",
            "Epoch 209/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1361 - accuracy: 0.9437\n",
            "Epoch 209: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1361 - accuracy: 0.9437 - val_loss: 0.3943 - val_accuracy: 0.8694\n",
            "Epoch 210/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1443 - accuracy: 0.9411\n",
            "Epoch 210: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.1443 - accuracy: 0.9411 - val_loss: 0.3725 - val_accuracy: 0.8734\n",
            "Epoch 211/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1125 - accuracy: 0.9544\n",
            "Epoch 211: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1137 - accuracy: 0.9539 - val_loss: 0.4057 - val_accuracy: 0.8729\n",
            "Epoch 212/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1384 - accuracy: 0.9433\n",
            "Epoch 212: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1384 - accuracy: 0.9433 - val_loss: 0.3762 - val_accuracy: 0.8820\n",
            "Epoch 213/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1381 - accuracy: 0.9421\n",
            "Epoch 213: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1381 - accuracy: 0.9421 - val_loss: 0.4425 - val_accuracy: 0.8504\n",
            "Epoch 214/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1569 - accuracy: 0.9352\n",
            "Epoch 214: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1569 - accuracy: 0.9352 - val_loss: 0.3757 - val_accuracy: 0.8749\n",
            "Epoch 215/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1388 - accuracy: 0.9443\n",
            "Epoch 215: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1389 - accuracy: 0.9443 - val_loss: 0.4017 - val_accuracy: 0.8640\n",
            "Epoch 216/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1464 - accuracy: 0.9417\n",
            "Epoch 216: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1471 - accuracy: 0.9416 - val_loss: 0.3826 - val_accuracy: 0.8660\n",
            "Epoch 217/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1335 - accuracy: 0.9457\n",
            "Epoch 217: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1330 - accuracy: 0.9459 - val_loss: 0.3947 - val_accuracy: 0.8692\n",
            "Epoch 218/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1178 - accuracy: 0.9513\n",
            "Epoch 218: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1175 - accuracy: 0.9515 - val_loss: 0.3625 - val_accuracy: 0.8776\n",
            "Epoch 219/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.1394 - accuracy: 0.9443\n",
            "Epoch 219: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1392 - accuracy: 0.9445 - val_loss: 0.3807 - val_accuracy: 0.8761\n",
            "Epoch 220/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1128 - accuracy: 0.9542\n",
            "Epoch 220: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1131 - accuracy: 0.9541 - val_loss: 0.3854 - val_accuracy: 0.8759\n",
            "Epoch 221/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1525 - accuracy: 0.9388\n",
            "Epoch 221: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1525 - accuracy: 0.9388 - val_loss: 0.5408 - val_accuracy: 0.8090\n",
            "Epoch 222/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2363 - accuracy: 0.9014\n",
            "Epoch 222: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2351 - accuracy: 0.9024 - val_loss: 0.3847 - val_accuracy: 0.8628\n",
            "Epoch 223/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1406 - accuracy: 0.9432\n",
            "Epoch 223: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1406 - accuracy: 0.9432 - val_loss: 0.3552 - val_accuracy: 0.8801\n",
            "Epoch 224/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1174 - accuracy: 0.9519\n",
            "Epoch 224: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1177 - accuracy: 0.9517 - val_loss: 0.3580 - val_accuracy: 0.8813\n",
            "Epoch 225/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1111 - accuracy: 0.9549\n",
            "Epoch 225: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.1110 - accuracy: 0.9550 - val_loss: 0.3576 - val_accuracy: 0.8771\n",
            "Epoch 226/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.1209 - accuracy: 0.9517\n",
            "Epoch 226: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1204 - accuracy: 0.9517 - val_loss: 0.3591 - val_accuracy: 0.8818\n",
            "Epoch 227/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1044 - accuracy: 0.9568\n",
            "Epoch 227: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1045 - accuracy: 0.9569 - val_loss: 0.3639 - val_accuracy: 0.8833\n",
            "Epoch 228/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.1012 - accuracy: 0.9597\n",
            "Epoch 228: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1008 - accuracy: 0.9598 - val_loss: 0.3946 - val_accuracy: 0.8771\n",
            "Epoch 229/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1100 - accuracy: 0.9553\n",
            "Epoch 229: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1100 - accuracy: 0.9553 - val_loss: 0.3773 - val_accuracy: 0.8796\n",
            "Epoch 230/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1180 - accuracy: 0.9528\n",
            "Epoch 230: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1174 - accuracy: 0.9530 - val_loss: 0.4053 - val_accuracy: 0.8783\n",
            "Epoch 231/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1115 - accuracy: 0.9566\n",
            "Epoch 231: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1115 - accuracy: 0.9566 - val_loss: 0.3840 - val_accuracy: 0.8838\n",
            "Epoch 232/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1101 - accuracy: 0.9545\n",
            "Epoch 232: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1101 - accuracy: 0.9545 - val_loss: 0.3924 - val_accuracy: 0.8751\n",
            "Epoch 233/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1135 - accuracy: 0.9539\n",
            "Epoch 233: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1135 - accuracy: 0.9539 - val_loss: 0.3915 - val_accuracy: 0.8810\n",
            "Epoch 234/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1066 - accuracy: 0.9565\n",
            "Epoch 234: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1067 - accuracy: 0.9564 - val_loss: 0.3870 - val_accuracy: 0.8813\n",
            "Epoch 235/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1064 - accuracy: 0.9575\n",
            "Epoch 235: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1064 - accuracy: 0.9575 - val_loss: 0.3990 - val_accuracy: 0.8727\n",
            "Epoch 236/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1113 - accuracy: 0.9548\n",
            "Epoch 236: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1112 - accuracy: 0.9548 - val_loss: 0.4144 - val_accuracy: 0.8751\n",
            "Epoch 237/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.1305 - accuracy: 0.9473\n",
            "Epoch 237: val_accuracy did not improve from 0.88549\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1295 - accuracy: 0.9477 - val_loss: 0.3767 - val_accuracy: 0.8796\n",
            "127/127 [==============================] - 1s 5ms/step\n",
            "159/159 [==============================] - 1s 4ms/step\n",
            "159/159 [==============================] - 1s 4ms/step\n",
            "KFold 2 of 5\n",
            "Epoch 1/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1971 - accuracy: 0.9269\n",
            "Epoch 1: val_accuracy improved from -inf to 0.95706, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.1986 - accuracy: 0.9267 - val_loss: 0.1103 - val_accuracy: 0.9571\n",
            "Epoch 2/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1795 - accuracy: 0.9332\n",
            "Epoch 2: val_accuracy did not improve from 0.95706\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1795 - accuracy: 0.9332 - val_loss: 0.1341 - val_accuracy: 0.9447\n",
            "Epoch 3/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1616 - accuracy: 0.9367\n",
            "Epoch 3: val_accuracy improved from 0.95706 to 0.95829, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1618 - accuracy: 0.9365 - val_loss: 0.1050 - val_accuracy: 0.9583\n",
            "Epoch 4/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1658 - accuracy: 0.9367\n",
            "Epoch 4: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1654 - accuracy: 0.9369 - val_loss: 0.1255 - val_accuracy: 0.9479\n",
            "Epoch 5/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1626 - accuracy: 0.9371\n",
            "Epoch 5: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1625 - accuracy: 0.9371 - val_loss: 0.1362 - val_accuracy: 0.9455\n",
            "Epoch 6/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1572 - accuracy: 0.9404\n",
            "Epoch 6: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1572 - accuracy: 0.9404 - val_loss: 0.1260 - val_accuracy: 0.9472\n",
            "Epoch 7/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1449 - accuracy: 0.9424\n",
            "Epoch 7: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1449 - accuracy: 0.9424 - val_loss: 0.1296 - val_accuracy: 0.9484\n",
            "Epoch 8/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1436 - accuracy: 0.9445\n",
            "Epoch 8: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.1442 - accuracy: 0.9443 - val_loss: 0.1356 - val_accuracy: 0.9440\n",
            "Epoch 9/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.2002 - accuracy: 0.9208\n",
            "Epoch 9: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1999 - accuracy: 0.9210 - val_loss: 0.1682 - val_accuracy: 0.9326\n",
            "Epoch 10/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1536 - accuracy: 0.9412\n",
            "Epoch 10: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1536 - accuracy: 0.9412 - val_loss: 0.1323 - val_accuracy: 0.9487\n",
            "Epoch 11/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1289 - accuracy: 0.9492\n",
            "Epoch 11: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1284 - accuracy: 0.9495 - val_loss: 0.1196 - val_accuracy: 0.9538\n",
            "Epoch 12/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9524\n",
            "Epoch 12: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1239 - accuracy: 0.9524 - val_loss: 0.1263 - val_accuracy: 0.9484\n",
            "Epoch 13/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1240 - accuracy: 0.9518\n",
            "Epoch 13: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1242 - accuracy: 0.9517 - val_loss: 0.1333 - val_accuracy: 0.9477\n",
            "Epoch 14/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1664 - accuracy: 0.9352\n",
            "Epoch 14: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1661 - accuracy: 0.9353 - val_loss: 0.1650 - val_accuracy: 0.9304\n",
            "Epoch 15/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1467 - accuracy: 0.9416\n",
            "Epoch 15: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1468 - accuracy: 0.9414 - val_loss: 0.1411 - val_accuracy: 0.9420\n",
            "Epoch 16/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1253 - accuracy: 0.9511\n",
            "Epoch 16: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1250 - accuracy: 0.9514 - val_loss: 0.1448 - val_accuracy: 0.9405\n",
            "Epoch 17/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1309 - accuracy: 0.9486\n",
            "Epoch 17: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1313 - accuracy: 0.9486 - val_loss: 0.1407 - val_accuracy: 0.9469\n",
            "Epoch 18/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1167 - accuracy: 0.9553\n",
            "Epoch 18: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1167 - accuracy: 0.9553 - val_loss: 0.1312 - val_accuracy: 0.9460\n",
            "Epoch 19/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1535 - accuracy: 0.9403\n",
            "Epoch 19: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1533 - accuracy: 0.9402 - val_loss: 0.1414 - val_accuracy: 0.9455\n",
            "Epoch 20/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1199 - accuracy: 0.9535\n",
            "Epoch 20: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.1199 - accuracy: 0.9535 - val_loss: 0.1371 - val_accuracy: 0.9472\n",
            "Epoch 21/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1181 - accuracy: 0.9529\n",
            "Epoch 21: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.1179 - accuracy: 0.9530 - val_loss: 0.1469 - val_accuracy: 0.9400\n",
            "Epoch 22/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1228 - accuracy: 0.9521\n",
            "Epoch 22: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1232 - accuracy: 0.9522 - val_loss: 0.1806 - val_accuracy: 0.9321\n",
            "Epoch 23/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.1445 - accuracy: 0.9461\n",
            "Epoch 23: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1441 - accuracy: 0.9463 - val_loss: 0.1459 - val_accuracy: 0.9442\n",
            "Epoch 24/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1309 - accuracy: 0.9488\n",
            "Epoch 24: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1309 - accuracy: 0.9488 - val_loss: 0.1729 - val_accuracy: 0.9324\n",
            "Epoch 25/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.1644 - accuracy: 0.9352\n",
            "Epoch 25: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1639 - accuracy: 0.9358 - val_loss: 0.1572 - val_accuracy: 0.9385\n",
            "Epoch 26/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1356 - accuracy: 0.9463\n",
            "Epoch 26: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.1354 - accuracy: 0.9464 - val_loss: 0.1496 - val_accuracy: 0.9410\n",
            "Epoch 27/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1082 - accuracy: 0.9583\n",
            "Epoch 27: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1086 - accuracy: 0.9581 - val_loss: 0.1513 - val_accuracy: 0.9356\n",
            "Epoch 28/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1229 - accuracy: 0.9507\n",
            "Epoch 28: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.1229 - accuracy: 0.9507 - val_loss: 0.1543 - val_accuracy: 0.9398\n",
            "Epoch 29/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1039 - accuracy: 0.9591\n",
            "Epoch 29: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1038 - accuracy: 0.9590 - val_loss: 0.1341 - val_accuracy: 0.9452\n",
            "Epoch 30/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1173 - accuracy: 0.9552\n",
            "Epoch 30: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1172 - accuracy: 0.9553 - val_loss: 0.1586 - val_accuracy: 0.9398\n",
            "Epoch 31/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1168 - accuracy: 0.9534\n",
            "Epoch 31: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1168 - accuracy: 0.9534 - val_loss: 0.1518 - val_accuracy: 0.9383\n",
            "Epoch 32/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1085 - accuracy: 0.9578\n",
            "Epoch 32: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1083 - accuracy: 0.9580 - val_loss: 0.1481 - val_accuracy: 0.9432\n",
            "Epoch 33/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.0959 - accuracy: 0.9630\n",
            "Epoch 33: val_accuracy did not improve from 0.95829\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0965 - accuracy: 0.9630 - val_loss: 0.1459 - val_accuracy: 0.9457\n",
            "127/127 [==============================] - 1s 5ms/step\n",
            "159/159 [==============================] - 1s 4ms/step\n",
            "159/159 [==============================] - 1s 4ms/step\n",
            "KFold 3 of 5\n",
            "Epoch 1/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1410 - accuracy: 0.9462\n",
            "Epoch 1: val_accuracy improved from -inf to 0.96372, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1409 - accuracy: 0.9463 - val_loss: 0.0925 - val_accuracy: 0.9637\n",
            "Epoch 2/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1380 - accuracy: 0.9455\n",
            "Epoch 2: val_accuracy improved from 0.96372 to 0.96619, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1381 - accuracy: 0.9456 - val_loss: 0.0952 - val_accuracy: 0.9662\n",
            "Epoch 3/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1211 - accuracy: 0.9528\n",
            "Epoch 3: val_accuracy improved from 0.96619 to 0.96742, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1211 - accuracy: 0.9528 - val_loss: 0.0862 - val_accuracy: 0.9674\n",
            "Epoch 4/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1108 - accuracy: 0.9572\n",
            "Epoch 4: val_accuracy improved from 0.96742 to 0.96964, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1108 - accuracy: 0.9572 - val_loss: 0.0832 - val_accuracy: 0.9696\n",
            "Epoch 5/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1232 - accuracy: 0.9514\n",
            "Epoch 5: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1232 - accuracy: 0.9514 - val_loss: 0.0908 - val_accuracy: 0.9669\n",
            "Epoch 6/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1323 - accuracy: 0.9495\n",
            "Epoch 6: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.1325 - accuracy: 0.9493 - val_loss: 0.1233 - val_accuracy: 0.9538\n",
            "Epoch 7/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1209 - accuracy: 0.9516\n",
            "Epoch 7: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1209 - accuracy: 0.9516 - val_loss: 0.0866 - val_accuracy: 0.9667\n",
            "Epoch 8/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1042 - accuracy: 0.9593\n",
            "Epoch 8: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1042 - accuracy: 0.9593 - val_loss: 0.1022 - val_accuracy: 0.9620\n",
            "Epoch 9/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.1160 - accuracy: 0.9538\n",
            "Epoch 9: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.1159 - accuracy: 0.9537 - val_loss: 0.1269 - val_accuracy: 0.9472\n",
            "Epoch 10/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.2819 - accuracy: 0.8884\n",
            "Epoch 10: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.2806 - accuracy: 0.8890 - val_loss: 0.1913 - val_accuracy: 0.9265\n",
            "Epoch 11/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1821 - accuracy: 0.9263\n",
            "Epoch 11: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1821 - accuracy: 0.9263 - val_loss: 0.1409 - val_accuracy: 0.9415\n",
            "Epoch 12/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1372 - accuracy: 0.9469\n",
            "Epoch 12: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.1372 - accuracy: 0.9469 - val_loss: 0.1397 - val_accuracy: 0.9450\n",
            "Epoch 13/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1281 - accuracy: 0.9500\n",
            "Epoch 13: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1281 - accuracy: 0.9500 - val_loss: 0.1410 - val_accuracy: 0.9460\n",
            "Epoch 14/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1327 - accuracy: 0.9486\n",
            "Epoch 14: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.1326 - accuracy: 0.9486 - val_loss: 0.1314 - val_accuracy: 0.9484\n",
            "Epoch 15/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.1251 - accuracy: 0.9485\n",
            "Epoch 15: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.1247 - accuracy: 0.9487 - val_loss: 0.1143 - val_accuracy: 0.9578\n",
            "Epoch 16/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1265 - accuracy: 0.9511\n",
            "Epoch 16: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1268 - accuracy: 0.9509 - val_loss: 0.1471 - val_accuracy: 0.9427\n",
            "Epoch 17/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1279 - accuracy: 0.9492\n",
            "Epoch 17: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1267 - accuracy: 0.9497 - val_loss: 0.1230 - val_accuracy: 0.9548\n",
            "Epoch 18/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1047 - accuracy: 0.9606\n",
            "Epoch 18: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1048 - accuracy: 0.9606 - val_loss: 0.1075 - val_accuracy: 0.9598\n",
            "Epoch 19/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0959 - accuracy: 0.9632\n",
            "Epoch 19: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0959 - accuracy: 0.9632 - val_loss: 0.1123 - val_accuracy: 0.9585\n",
            "Epoch 20/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.0947 - accuracy: 0.9631\n",
            "Epoch 20: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0946 - accuracy: 0.9632 - val_loss: 0.1199 - val_accuracy: 0.9558\n",
            "Epoch 21/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0924 - accuracy: 0.9643\n",
            "Epoch 21: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.0925 - accuracy: 0.9639 - val_loss: 0.1211 - val_accuracy: 0.9538\n",
            "Epoch 22/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.0932 - accuracy: 0.9631\n",
            "Epoch 22: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0930 - accuracy: 0.9630 - val_loss: 0.1205 - val_accuracy: 0.9526\n",
            "Epoch 23/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.1112 - accuracy: 0.9559\n",
            "Epoch 23: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1110 - accuracy: 0.9559 - val_loss: 0.1156 - val_accuracy: 0.9548\n",
            "Epoch 24/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.0982 - accuracy: 0.9618\n",
            "Epoch 24: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0988 - accuracy: 0.9616 - val_loss: 0.1137 - val_accuracy: 0.9578\n",
            "Epoch 25/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0877 - accuracy: 0.9666\n",
            "Epoch 25: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0892 - accuracy: 0.9658 - val_loss: 0.1104 - val_accuracy: 0.9608\n",
            "Epoch 26/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.0942 - accuracy: 0.9646\n",
            "Epoch 26: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0941 - accuracy: 0.9645 - val_loss: 0.1105 - val_accuracy: 0.9568\n",
            "Epoch 27/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.0984 - accuracy: 0.9607\n",
            "Epoch 27: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0987 - accuracy: 0.9606 - val_loss: 0.1526 - val_accuracy: 0.9437\n",
            "Epoch 28/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1164 - accuracy: 0.9529\n",
            "Epoch 28: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1164 - accuracy: 0.9529 - val_loss: 0.1146 - val_accuracy: 0.9576\n",
            "Epoch 29/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.0896 - accuracy: 0.9649\n",
            "Epoch 29: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0894 - accuracy: 0.9650 - val_loss: 0.1340 - val_accuracy: 0.9531\n",
            "Epoch 30/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0990 - accuracy: 0.9609\n",
            "Epoch 30: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0990 - accuracy: 0.9609 - val_loss: 0.1316 - val_accuracy: 0.9519\n",
            "Epoch 31/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0853 - accuracy: 0.9677\n",
            "Epoch 31: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0853 - accuracy: 0.9677 - val_loss: 0.1305 - val_accuracy: 0.9541\n",
            "Epoch 32/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0919 - accuracy: 0.9646\n",
            "Epoch 32: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.0927 - accuracy: 0.9646 - val_loss: 0.1444 - val_accuracy: 0.9469\n",
            "Epoch 33/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.1058 - accuracy: 0.9585\n",
            "Epoch 33: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.1067 - accuracy: 0.9584 - val_loss: 0.1400 - val_accuracy: 0.9464\n",
            "Epoch 34/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1128 - accuracy: 0.9556\n",
            "Epoch 34: val_accuracy did not improve from 0.96964\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1127 - accuracy: 0.9556 - val_loss: 0.1247 - val_accuracy: 0.9504\n",
            "127/127 [==============================] - 1s 5ms/step\n",
            "159/159 [==============================] - 1s 4ms/step\n",
            "159/159 [==============================] - 1s 4ms/step\n",
            "KFold 4 of 5\n",
            "Epoch 1/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1425 - accuracy: 0.9438\n",
            "Epoch 1: val_accuracy improved from -inf to 0.96816, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1422 - accuracy: 0.9440 - val_loss: 0.0845 - val_accuracy: 0.9682\n",
            "Epoch 2/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1159 - accuracy: 0.9537\n",
            "Epoch 2: val_accuracy improved from 0.96816 to 0.97162, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1157 - accuracy: 0.9538 - val_loss: 0.0780 - val_accuracy: 0.9716\n",
            "Epoch 3/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0978 - accuracy: 0.9606\n",
            "Epoch 3: val_accuracy improved from 0.97162 to 0.97285, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0980 - accuracy: 0.9606 - val_loss: 0.0733 - val_accuracy: 0.9729\n",
            "Epoch 4/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0950 - accuracy: 0.9635\n",
            "Epoch 4: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0950 - accuracy: 0.9635 - val_loss: 0.0754 - val_accuracy: 0.9711\n",
            "Epoch 5/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0977 - accuracy: 0.9637\n",
            "Epoch 5: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0974 - accuracy: 0.9638 - val_loss: 0.0801 - val_accuracy: 0.9692\n",
            "Epoch 6/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1041 - accuracy: 0.9603\n",
            "Epoch 6: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1040 - accuracy: 0.9602 - val_loss: 0.0836 - val_accuracy: 0.9664\n",
            "Epoch 7/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.1219 - accuracy: 0.9530\n",
            "Epoch 7: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1227 - accuracy: 0.9529 - val_loss: 0.1405 - val_accuracy: 0.9440\n",
            "Epoch 8/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1068 - accuracy: 0.9593\n",
            "Epoch 8: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1068 - accuracy: 0.9593 - val_loss: 0.0920 - val_accuracy: 0.9652\n",
            "Epoch 9/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.0888 - accuracy: 0.9656\n",
            "Epoch 9: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.0886 - accuracy: 0.9658 - val_loss: 0.0875 - val_accuracy: 0.9642\n",
            "Epoch 10/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.0858 - accuracy: 0.9660\n",
            "Epoch 10: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.0859 - accuracy: 0.9661 - val_loss: 0.0862 - val_accuracy: 0.9664\n",
            "Epoch 11/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9668\n",
            "Epoch 11: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0826 - accuracy: 0.9668 - val_loss: 0.0871 - val_accuracy: 0.9692\n",
            "Epoch 12/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1025 - accuracy: 0.9621\n",
            "Epoch 12: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.1027 - accuracy: 0.9621 - val_loss: 0.1000 - val_accuracy: 0.9620\n",
            "Epoch 13/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1062 - accuracy: 0.9607\n",
            "Epoch 13: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1062 - accuracy: 0.9607 - val_loss: 0.0918 - val_accuracy: 0.9622\n",
            "Epoch 14/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0992 - accuracy: 0.9614\n",
            "Epoch 14: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0992 - accuracy: 0.9614 - val_loss: 0.1061 - val_accuracy: 0.9605\n",
            "Epoch 15/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1004 - accuracy: 0.9639\n",
            "Epoch 15: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.0995 - accuracy: 0.9643 - val_loss: 0.1020 - val_accuracy: 0.9622\n",
            "Epoch 16/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1053 - accuracy: 0.9615\n",
            "Epoch 16: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1053 - accuracy: 0.9614 - val_loss: 0.1078 - val_accuracy: 0.9590\n",
            "Epoch 17/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0885 - accuracy: 0.9663\n",
            "Epoch 17: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0883 - accuracy: 0.9664 - val_loss: 0.0987 - val_accuracy: 0.9610\n",
            "Epoch 18/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0811 - accuracy: 0.9684\n",
            "Epoch 18: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.0811 - accuracy: 0.9685 - val_loss: 0.0917 - val_accuracy: 0.9657\n",
            "Epoch 19/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.0846 - accuracy: 0.9672\n",
            "Epoch 19: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0853 - accuracy: 0.9671 - val_loss: 0.0914 - val_accuracy: 0.9640\n",
            "Epoch 20/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1023 - accuracy: 0.9605\n",
            "Epoch 20: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1023 - accuracy: 0.9606 - val_loss: 0.1227 - val_accuracy: 0.9526\n",
            "Epoch 21/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.0926 - accuracy: 0.9648\n",
            "Epoch 21: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0935 - accuracy: 0.9647 - val_loss: 0.1121 - val_accuracy: 0.9580\n",
            "Epoch 22/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.0937 - accuracy: 0.9649\n",
            "Epoch 22: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0946 - accuracy: 0.9645 - val_loss: 0.1202 - val_accuracy: 0.9531\n",
            "Epoch 23/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.0933 - accuracy: 0.9646\n",
            "Epoch 23: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0928 - accuracy: 0.9649 - val_loss: 0.1031 - val_accuracy: 0.9625\n",
            "Epoch 24/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.0753 - accuracy: 0.9706\n",
            "Epoch 24: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0753 - accuracy: 0.9709 - val_loss: 0.0965 - val_accuracy: 0.9615\n",
            "Epoch 25/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.0806 - accuracy: 0.9683\n",
            "Epoch 25: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.0811 - accuracy: 0.9681 - val_loss: 0.1158 - val_accuracy: 0.9521\n",
            "Epoch 26/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0834 - accuracy: 0.9674\n",
            "Epoch 26: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0834 - accuracy: 0.9674 - val_loss: 0.1000 - val_accuracy: 0.9622\n",
            "Epoch 27/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0821 - accuracy: 0.9676\n",
            "Epoch 27: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0821 - accuracy: 0.9676 - val_loss: 0.0965 - val_accuracy: 0.9645\n",
            "Epoch 28/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.0777 - accuracy: 0.9701\n",
            "Epoch 28: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0777 - accuracy: 0.9703 - val_loss: 0.1288 - val_accuracy: 0.9499\n",
            "Epoch 29/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.0925 - accuracy: 0.9630\n",
            "Epoch 29: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0926 - accuracy: 0.9630 - val_loss: 0.1057 - val_accuracy: 0.9566\n",
            "Epoch 30/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9700\n",
            "Epoch 30: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0794 - accuracy: 0.9700 - val_loss: 0.1324 - val_accuracy: 0.9504\n",
            "Epoch 31/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.0986 - accuracy: 0.9607\n",
            "Epoch 31: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0987 - accuracy: 0.9606 - val_loss: 0.1196 - val_accuracy: 0.9546\n",
            "Epoch 32/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.0810 - accuracy: 0.9701\n",
            "Epoch 32: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0808 - accuracy: 0.9701 - val_loss: 0.1194 - val_accuracy: 0.9583\n",
            "Epoch 33/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.0800 - accuracy: 0.9689\n",
            "Epoch 33: val_accuracy did not improve from 0.97285\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0802 - accuracy: 0.9688 - val_loss: 0.1249 - val_accuracy: 0.9531\n",
            "127/127 [==============================] - 1s 4ms/step\n",
            "159/159 [==============================] - 1s 4ms/step\n",
            "159/159 [==============================] - 1s 4ms/step\n",
            "KFold 5 of 5\n",
            "Epoch 1/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1195 - accuracy: 0.9551\n",
            "Epoch 1: val_accuracy improved from -inf to 0.96865, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1195 - accuracy: 0.9551 - val_loss: 0.0835 - val_accuracy: 0.9686\n",
            "Epoch 2/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.1204 - accuracy: 0.9529\n",
            "Epoch 2: val_accuracy did not improve from 0.96865\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1206 - accuracy: 0.9529 - val_loss: 0.0912 - val_accuracy: 0.9647\n",
            "Epoch 3/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.1044 - accuracy: 0.9592\n",
            "Epoch 3: val_accuracy improved from 0.96865 to 0.97211, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1046 - accuracy: 0.9590 - val_loss: 0.0725 - val_accuracy: 0.9721\n",
            "Epoch 4/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0789 - accuracy: 0.9709\n",
            "Epoch 4: val_accuracy improved from 0.97211 to 0.97680, saving model to ./lstm.h5\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0791 - accuracy: 0.9709 - val_loss: 0.0599 - val_accuracy: 0.9768\n",
            "Epoch 5/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9714\n",
            "Epoch 5: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.0752 - accuracy: 0.9714 - val_loss: 0.0981 - val_accuracy: 0.9622\n",
            "Epoch 6/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1030 - accuracy: 0.9612\n",
            "Epoch 6: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1030 - accuracy: 0.9612 - val_loss: 0.0734 - val_accuracy: 0.9694\n",
            "Epoch 7/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.0905 - accuracy: 0.9653\n",
            "Epoch 7: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0903 - accuracy: 0.9654 - val_loss: 0.1407 - val_accuracy: 0.9499\n",
            "Epoch 8/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1027 - accuracy: 0.9616\n",
            "Epoch 8: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1027 - accuracy: 0.9616 - val_loss: 0.0748 - val_accuracy: 0.9699\n",
            "Epoch 9/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.0747 - accuracy: 0.9723\n",
            "Epoch 9: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0745 - accuracy: 0.9724 - val_loss: 0.0709 - val_accuracy: 0.9731\n",
            "Epoch 10/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.0759 - accuracy: 0.9714\n",
            "Epoch 10: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.0761 - accuracy: 0.9712 - val_loss: 0.1145 - val_accuracy: 0.9568\n",
            "Epoch 11/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1026 - accuracy: 0.9598\n",
            "Epoch 11: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.1026 - accuracy: 0.9598 - val_loss: 0.0770 - val_accuracy: 0.9726\n",
            "Epoch 12/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0859 - accuracy: 0.9663\n",
            "Epoch 12: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0859 - accuracy: 0.9663 - val_loss: 0.0696 - val_accuracy: 0.9733\n",
            "Epoch 13/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.0677 - accuracy: 0.9754\n",
            "Epoch 13: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0678 - accuracy: 0.9753 - val_loss: 0.0821 - val_accuracy: 0.9716\n",
            "Epoch 14/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.0886 - accuracy: 0.9653\n",
            "Epoch 14: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0884 - accuracy: 0.9654 - val_loss: 0.0759 - val_accuracy: 0.9719\n",
            "Epoch 15/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0803 - accuracy: 0.9695\n",
            "Epoch 15: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.0803 - accuracy: 0.9695 - val_loss: 0.0893 - val_accuracy: 0.9645\n",
            "Epoch 16/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0918 - accuracy: 0.9656\n",
            "Epoch 16: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0916 - accuracy: 0.9658 - val_loss: 0.0796 - val_accuracy: 0.9694\n",
            "Epoch 17/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.0820 - accuracy: 0.9686\n",
            "Epoch 17: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0818 - accuracy: 0.9687 - val_loss: 0.0802 - val_accuracy: 0.9704\n",
            "Epoch 18/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0852 - accuracy: 0.9684\n",
            "Epoch 18: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0854 - accuracy: 0.9683 - val_loss: 0.0963 - val_accuracy: 0.9627\n",
            "Epoch 19/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.0753 - accuracy: 0.9728\n",
            "Epoch 19: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0753 - accuracy: 0.9728 - val_loss: 0.0945 - val_accuracy: 0.9667\n",
            "Epoch 20/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0811 - accuracy: 0.9696\n",
            "Epoch 20: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.0815 - accuracy: 0.9694 - val_loss: 0.0965 - val_accuracy: 0.9640\n",
            "Epoch 21/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.0955 - accuracy: 0.9637\n",
            "Epoch 21: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0952 - accuracy: 0.9639 - val_loss: 0.1064 - val_accuracy: 0.9598\n",
            "Epoch 22/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.0765 - accuracy: 0.9710\n",
            "Epoch 22: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0766 - accuracy: 0.9709 - val_loss: 0.0839 - val_accuracy: 0.9704\n",
            "Epoch 23/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.0745 - accuracy: 0.9717\n",
            "Epoch 23: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0744 - accuracy: 0.9717 - val_loss: 0.0933 - val_accuracy: 0.9649\n",
            "Epoch 24/500\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.0786 - accuracy: 0.9707\n",
            "Epoch 24: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.0786 - accuracy: 0.9707 - val_loss: 0.1023 - val_accuracy: 0.9625\n",
            "Epoch 25/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0714 - accuracy: 0.9730\n",
            "Epoch 25: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.0717 - accuracy: 0.9725 - val_loss: 0.1058 - val_accuracy: 0.9635\n",
            "Epoch 26/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9722\n",
            "Epoch 26: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 16ms/step - loss: 0.0767 - accuracy: 0.9722 - val_loss: 0.1031 - val_accuracy: 0.9640\n",
            "Epoch 27/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9729\n",
            "Epoch 27: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0720 - accuracy: 0.9730 - val_loss: 0.0901 - val_accuracy: 0.9657\n",
            "Epoch 28/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9703\n",
            "Epoch 28: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.0763 - accuracy: 0.9703 - val_loss: 0.1045 - val_accuracy: 0.9600\n",
            "Epoch 29/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.1002 - accuracy: 0.9627\n",
            "Epoch 29: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.1007 - accuracy: 0.9625 - val_loss: 0.1271 - val_accuracy: 0.9563\n",
            "Epoch 30/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.0836 - accuracy: 0.9675\n",
            "Epoch 30: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.0840 - accuracy: 0.9675 - val_loss: 0.1033 - val_accuracy: 0.9612\n",
            "Epoch 31/500\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0840 - accuracy: 0.9680\n",
            "Epoch 31: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 19ms/step - loss: 0.0840 - accuracy: 0.9680 - val_loss: 0.1146 - val_accuracy: 0.9573\n",
            "Epoch 32/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.0827 - accuracy: 0.9699\n",
            "Epoch 32: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 19ms/step - loss: 0.0827 - accuracy: 0.9699 - val_loss: 0.1677 - val_accuracy: 0.9403\n",
            "Epoch 33/500\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.1003 - accuracy: 0.9646\n",
            "Epoch 33: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 19ms/step - loss: 0.1008 - accuracy: 0.9644 - val_loss: 0.1225 - val_accuracy: 0.9563\n",
            "Epoch 34/500\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.0833 - accuracy: 0.9697\n",
            "Epoch 34: val_accuracy did not improve from 0.97680\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.0831 - accuracy: 0.9698 - val_loss: 0.1161 - val_accuracy: 0.9608\n",
            "127/127 [==============================] - 1s 5ms/step\n",
            "159/159 [==============================] - 1s 4ms/step\n",
            "159/159 [==============================] - 1s 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"###LSTM Classifier###\\n\")\n",
        "print(\"Accuracy: \" + str(round(mean(acc_score_lstm),4)) + \" +- \"+ str(round(std(acc_score_lstm),4)))\n",
        "print(\"ROC-AUC: \" + str(round(mean(auc_score_lstm),4)) + \" +- \" + str(round(std(auc_score_lstm),4)))\n",
        "print(\"F1-Score: \" + str(round(mean(f1_lstm),4)) +\" +- \"+ str(round(std(f1_lstm),4)))\n",
        "\n",
        "yhat = lstm.predict(X_test_lstm).round()\n",
        "print(classification_report(y_test, yhat))\n",
        "cm = confusion_matrix(yhat, y_test, labels=[0,1])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
        "disp.plot()\n",
        "RocCurveDisplay.from_predictions(yhat, y_test)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sjGLDBebYHee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        },
        "outputId": "26d0c008-0b52-4497-f7f8-910120c53cfa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###LSTM Classifier###\n",
            "\n",
            "Accuracy: 0.8853 +- 0.01\n",
            "ROC-AUC: 0.8839 +- 0.0091\n",
            "F1-Score: 0.9063 +- 0.0075\n",
            "159/159 [==============================] - 1s 4ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.85      0.87      2024\n",
            "           1       0.90      0.93      0.91      3041\n",
            "\n",
            "    accuracy                           0.90      5065\n",
            "   macro avg       0.89      0.89      0.89      5065\n",
            "weighted avg       0.90      0.90      0.90      5065\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################"
      ],
      "metadata": {
        "id": "voWGTVEAYAI6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta_train_lstm = pd.DataFrame(meta_train_lstm, columns=['LSTM'])"
      ],
      "metadata": {
        "id": "ppxNynHOXqsf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta_test_ltsm = pd.DataFrame(meta_test_lstm, columns=['LSTM'])"
      ],
      "metadata": {
        "id": "zZLhFmwTXxYp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta_train_lstm.to_pickle('./drive/MyDrive/save/taiwan/meta_train_lstm.pkl')\n",
        "meta_test_ltsm.to_pickle('./drive/MyDrive/save/taiwan/meta_test_ltsm.pkl')"
      ],
      "metadata": {
        "id": "ympOATPKX0eY"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}